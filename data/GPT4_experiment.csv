abstract,CS_actual,GPT4_CS
"  Predictive models allow subject-specific inference when analyzing disease
related alterations in neuroimaging data. Given a subject's data, inference can
be made at two levels: global, i.e. identifiying condition presence for the
subject, and local, i.e. detecting condition effect on each individual
measurement extracted from the subject's data. While global inference is widely
used, local inference, which can be used to form subject-specific effect maps,
is rarely used because existing models often yield noisy detections composed of
dispersed isolated islands. In this article, we propose a reconstruction
method, named RSM, to improve subject-specific detections of predictive
modeling approaches and in particular, binary classifiers. RSM specifically
aims to reduce noise due to sampling error associated with using a finite
sample of examples to train classifiers. The proposed method is a wrapper-type
algorithm that can be used with different binary classifiers in a diagnostic
manner, i.e. without information on condition presence. Reconstruction is posed
as a Maximum-A-Posteriori problem with a prior model whose parameters are
estimated from training data in a classifier-specific fashion. Experimental
evaluation is performed on synthetically generated data and data from the
Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on
synthetic data demonstrate that using RSM yields higher detection accuracy
compared to using models directly or with bootstrap averaging. Analyses on the
ADNI dataset show that RSM can also improve correlation between
subject-specific detections in cortical thickness data and non-imaging markers
of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score
and Cerebrospinal Fluid amyloid-$\beta$ levels. Further reliability studies on
the longitudinal ADNI dataset show improvement on detection reliability when
RSM is used.
",1,1
"  Rotation invariance and translation invariance have great values in image
recognition tasks. In this paper, we bring a new architecture in convolutional
neural network (CNN) named cyclic convolutional layer to achieve rotation
invariance in 2-D symbol recognition. We can also get the position and
orientation of the 2-D symbol by the network to achieve detection purpose for
multiple non-overlap target. Last but not least, this architecture can achieve
one-shot learning in some cases using those invariance.
",1,1
"  We introduce and develop the notion of spherical polyharmonics, which are a
natural generalisation of spherical harmonics. In particular we study the
theory of zonal polyharmonics, which allows us, analogously to zonal harmonics,
to construct Poisson kernels for polyharmonic functions on the union of rotated
balls. We find the representation of Poisson kernels and zonal polyharmonics in
terms of the Gegenbauer polynomials. We show the connection between the
classical Poisson kernel for harmonic functions on the ball, Poisson kernels
for polyharmonic functions on the union of rotated balls, and the Cauchy-Hua
kernel for holomorphic functions on the Lie ball.
",0,0
"  The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the
Maxwell equations (the so called stochastic MLLG system) describes the creation
of domain walls and vortices (fundamental objects for the novel nanostructured
magnetic memories). We first reformulate the stochastic LLG equation into an
equation with time-differentiable solutions. We then propose a convergent
$\theta$-linear scheme to approximate the solutions of the reformulated system.
As a consequence, we prove convergence of the approximate solutions, with no or
minor conditions on time and space steps (depending on the value of $\theta$).
Hence, we prove the existence of weak martingale solutions of the stochastic
MLLG system. Numerical results are presented to show applicability of the
method.
",0,1
"  Fourier-transform infra-red (FTIR) spectra of samples from 7 plant species
were used to explore the influence of preprocessing and feature extraction on
efficiency of machine learning algorithms. Wavelet Tensor Train (WTT) and
Discrete Wavelet Transforms (DWT) were compared as feature extraction
techniques for FTIR data of medicinal plants. Various combinations of signal
processing steps showed different behavior when applied to classification and
clustering tasks. Best results for WTT and DWT found through grid search were
similar, significantly improving quality of clustering as well as
classification accuracy for tuned logistic regression in comparison to original
spectra. Unlike DWT, WTT has only one parameter to be tuned (rank), making it a
more versatile and easier to use as a data processing tool in various signal
processing applications.
",1,1
"  Let $\Omega \subset \mathbb{R}^n$ be a bounded domain satisfying a
Hayman-type asymmetry condition, and let $ D $ be an arbitrary bounded domain
referred to as ""obstacle"". We are interested in the behaviour of the first
Dirichlet eigenvalue $ \lambda_1(\Omega \setminus (x+D)) $. First, we prove an
upper bound on $ \lambda_1(\Omega \setminus (x+D)) $ in terms of the distance
of the set $ x+D $ to the set of maximum points $ x_0 $ of the first Dirichlet
ground state $ \phi_{\lambda_1} > 0 $ of $ \Omega $. In short, a direct
corollary is that if \begin{equation} \mu_\Omega := \max_{x}\lambda_1(\Omega
\setminus (x+D)) \end{equation} is large enough in terms of $ \lambda_1(\Omega)
$, then all maximizer sets $ x+D $ of $ \mu_\Omega $ are close to each maximum
point $ x_0 $ of $ \phi_{\lambda_1} $.
Second, we discuss the distribution of $ \phi_{\lambda_1(\Omega)} $ and the
possibility to inscribe wavelength balls at a given point in $ \Omega $.
Finally, we specify our observations to convex obstacles $ D $ and show that
if $ \mu_\Omega $ is sufficiently large with respect to $ \lambda_1(\Omega) $,
then all maximizers $ x+D $ of $ \mu_\Omega $ contain all maximum points $ x_0
$ of $ \phi_{\lambda_1(\Omega)} $.
",0,0
"  We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017
U1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel
Telescope. From these observations, we derived a partial lightcurve with
peak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out
rotation periods less than 3 hr and suggests that the period is at least 5 hr.
On the assumption that the variability is due to a changing cross section, the
axial ratio is at least 3:1. We saw no evidence for a coma or tail in either
individual images or in a stacked image having an equivalent exposure time of
9000 s.
",0,0
"  The ability of metallic nanoparticles to supply heat to a liquid environment
under exposure to an external optical field has attracted growing interest for
biomedical applications. Controlling the thermal transport properties at a
solid-liquid interface then appears to be particularly relevant. In this work,
we address the thermal transport between water and a gold surface coated by a
polymer layer. Using molecular dynamics simulations, we demonstrate that
increasing the polymer density displaces the domain resisting to the heat flow,
while it doesn't affect the final amount of thermal energy released in the
liquid. This unexpected behavior results from a trade-off established by the
increasing polymer density which couples more efficiently with the solid but
initiates a counterbalancing resistance with the liquid.
",0,0
"  We model large-scale ($\approx$2000km) impacts on a Mars-like planet using a
Smoothed Particle Hydrodynamics code. The effects of material strength and of
using different Equations of State on the post-impact material and temperature
distributions are investigated. The properties of the ejected material in terms
of escaping and disc mass are analysed as well. We also study potential
numerical effects in the context of density discontinuities and rigid body
rotation. We find that in the large-scale collision regime considered here
(with impact velocities of 4km/s), the effect of material strength is
substantial for the post-impact distribution of the temperature and the
impactor material, while the influence of the Equation of State is more subtle
and present only at very high temperatures.
",0,0
"  Time varying susceptibility of host at individual level due to waning and
boosting immunity is known to induce rich long-term behavior of disease
transmission dynamics. Meanwhile, the impact of the time varying heterogeneity
of host susceptibility on the shot-term behavior of epidemics is not
well-studied, even though the large amount of the available epidemiological
data are the short-term epidemics. Here we constructed a parsimonious
mathematical model describing the short-term transmission dynamics taking into
account natural-boosting immunity by reinfection, and obtained the explicit
solution for our model. We found that our system show ""the delayed epidemic"",
the epidemic takes off after negative slope of the epidemic curve at the
initial phase of epidemic, in addition to the common classification in the
standard SIR model, i.e., ""no epidemic"" as $\mathcal{R}_{0}\leq1$ or normal
epidemic as $\mathcal{R}_{0}>1$. Employing the explicit solution we derived the
condition for each classification.
",0,0
"  We present a systematic global sensitivity analysis using the Sobol method
which can be utilized to rank the variables that affect two quantity of
interests -- pore pressure depletion and stress change -- around a
hydraulically-fractured horizontal well based on their degree of importance.
These variables include rock properties and stimulation design variables. A
fully-coupled poroelastic hydraulic fracture model is used to account for pore
pressure and stress changes due to production. To ease the computational cost
of a simulator, we also provide reduced order models (ROMs), which can be used
to replace the complex numerical model with a rather simple analytical model,
for calculating the pore pressure and stresses at different locations around
hydraulic fractures. The main findings of this research are: (i) mobility,
production pressure, and fracture half-length are the main contributors to the
changes in the quantities of interest. The percentage of the contribution of
each parameter depends on the location with respect to pre-existing hydraulic
fractures and the quantity of interest. (ii) As the time progresses, the effect
of mobility decreases and the effect of production pressure increases. (iii)
These two variables are also dominant for horizontal stresses at large
distances from hydraulic fractures. (iv) At zones close to hydraulic fracture
tips or inside the spacing area, other parameters such as fracture spacing and
half-length are the dominant factors that affect the minimum horizontal stress.
The results of this study will provide useful guidelines for the stimulation
design of legacy wells and secondary operations such as refracturing and infill
drilling.
",1,0
"  ""Three is a crowd"" is an old proverb that applies as much to social
interactions, as it does to frustrated configurations in statistical physics
models. Accordingly, social relations within a triangle deserve special
attention. With this motivation, we explore the impact of topological
frustration on the evolutionary dynamics of the snowdrift game on a triangular
lattice. This topology provides an irreconcilable frustration, which prevents
anti-coordination of competing strategies that would be needed for an optimal
outcome of the game. By using different strategy updating protocols, we observe
complex spatial patterns in dependence on payoff values that are reminiscent to
a honeycomb-like organization, which helps to minimize the negative consequence
of the topological frustration. We relate the emergence of these patterns to
the microscopic dynamics of the evolutionary process, both by means of
mean-field approximations and Monte Carlo simulations. For comparison, we also
consider the same evolutionary dynamics on the square lattice, where of course
the topological frustration is absent. However, with the deletion of diagonal
links of the triangular lattice, we can gradually bridge the gap to the square
lattice. Interestingly, in this case the level of cooperation in the system is
a direct indicator of the level of topological frustration, thus providing a
method to determine frustration levels in an arbitrary interaction network.
",0,1
"  We study the exciton magnetic polaron (EMP) formation in (Cd,Mn)Se/(Cd,Mg)Se
diluted-magnetic-semiconductor quantum wells using time-resolved
photoluminescence (PL). The magnetic field and temperature dependencies of this
dynamics allow us to separate the non-magnetic and magnetic contributions to
the exciton localization. We deduce the EMP energy of 14 meV, which is in
agreement with time-integrated measurements based on selective excitation and
the magnetic field dependence of the PL circular polarization degree. The
polaron formation time of 500 ps is significantly longer than the corresponding
values reported earlier. We propose that this behavior is related to strong
self-localization of the EMP, accompanied with a squeezing of the heavy-hole
envelope wavefunction. This conclusion is also supported by the decrease of the
exciton lifetime from 600 ps to 200 - 400 ps with increasing magnetic field and
temperature.
",0,0
"  The classical Eilenberg correspondence, based on the concept of the syntactic
monoid, relates varieties of regular languages with pseudovarieties of finite
monoids. Various modifications of this correspondence appeared, with more
general classes of regular languages on one hand and classes of more complex
algebraic structures on the other hand. For example, classes of languages need
not be closed under complementation or all preimages under homomorphisms, while
monoids can be equipped with a compatible order or they can have a
distinguished set of generators. Such generalized varieties and pseudovarieties
also have natural counterparts formed by classes of finite (ordered) automata.
In this paper the previous approaches are combined. The notion of positive
$\mathcal C$-varieties of ordered semiautomata (i.e. no initial and final
states are specified) is introduced and their correspondence with positive
$\mathcal C$-varieties of languages is proved.
",1,1
"  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
experimental evidence for spontaneous vortex phase (SVP) formation in
EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
$T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
the first-order transition from the short period domain structure, which
appears in the Meissner state, into the long period domain structure with
spontaneous vortices. It is the first experimental observation of this scenario
in the ferromagnetic superconductors. Low-temperature phase is characterized by
much larger domains in V-AV state and peculiar branched striped structures at
the surface, which are typical for uniaxial ferromagnets with perpendicular
magnetic anisotropy (PMA). The domain wall parameters at various temperatures
are estimated.
",0,0
"  The recent discovery that the exponent of matrix multiplication is determined
by the rank of the symmetrized matrix multiplication tensor has invigorated
interest in better understanding symmetrized matrix multiplication. I present
an explicit rank 18 Waring decomposition of $sM_{\langle 3\rangle}$ and
describe its symmetry group.
",0,1
"  The process that leads to the formation of the bright star forming sites
observed along prominent spiral arms remains elusive. We present results of a
multi-wavelength study of a spiral arm segment in the nearby grand-design
spiral galaxy M51 that belongs to a spiral density wave and exhibits nine gas
spurs. The combined observations of the(ionized, atomic, molecular, dusty)
interstellar medium (ISM) with star formation tracers (HII regions, young
<10Myr stellar clusters) suggest (1) no variation in giant molecular cloud
(GMC) properties between arm and gas spurs, (2) gas spurs and extinction
feathers arising from the same structure with a close spatial relation between
gas spurs and ongoing/recent star formation (despite higher gas surface
densities in the spiral arm), (3) no trend in star formation age either along
the arm or along a spur, (4) evidence for strong star formation feedback in gas
spurs: (5) tentative evidence for star formation triggered by stellar feedback
for one spur, and (6) GMC associations (GMAs) being no special entities but the
result of blending of gas arm/spur cross-sections in lower resolution
observations. We conclude that there is no evidence for a coherent star
formation onset mechanism that can be solely associated to the presence of the
spiral density wave. This suggests that other (more localized) mechanisms are
important to delay star formation such that it occurs in spurs. The evidence of
star formation proceeding over several million years within individual spurs
implies that the mechanism that leads to star formation acts or is sustained
over a longer time-scale.
",0,0
"  We describe a variant construction of the unstable Adams spectral the
sequence for a space $Y$, associated to any free simplicial resolution of
$H^*(Y;R)$ for $R=\mathbb{F}_p$ or $\mathbb{Q}$. We use this construction to
describe the differentials and filtration in the spectral sequence in terms of
appropriate systems of higher cohomology operations.
",0,0
"  When investigators seek to estimate causal effects, they often assume that
selection into treatment is based only on observed covariates. Under this
identification strategy, analysts must adjust for observed confounders. While
basic regression models have long been the dominant method of statistical
adjustment, more robust methods based on matching or weighting have become more
common. Of late, even more flexible methods based on machine learning methods
have been developed for statistical adjustment. These machine learning methods
are designed to be black box methods with little input from the researcher.
Recent research used a data competition to evaluate various methods of
statistical adjustment and found that black box methods out performed all other
methods of statistical adjustment. Matching methods with covariate
prioritization are designed for direct input from substantive investigators in
direct contrast to black methods. In this article, we use a different research
design to compare matching with covariate prioritization to black box methods.
We use black box methods to replicate results from five studies where matching
with covariate prioritization was used to customize the statistical adjustment
in direct response to substantive expertise. We find little difference across
the methods. We conclude with advice for investigators.
",0,0
"  Assigning homogeneous boundary conditions, such as acoustic impedance, to the
thermoviscous wave equations (TWE) derived by transforming the linearized
Navier-Stokes equations (LNSE) to the frequency domain yields a so-called
Helmholtz solver, whose output is a discrete set of complex eigenfunction and
eigenvalue pairs. The proposed method -- the inverse Helmholtz solver (iHS) --
reverses such procedure by returning the value of acoustic impedance at one or
more unknown impedance boundaries (IBs) of a given domain via spatial
integration of the TWE for a given real-valued frequency with assigned
conditions on other boundaries. The iHS procedure is applied to a second-order
spatial discretization of the TWEs derived on an unstructured grid with
staggered grid arrangement. The momentum equation only is extended to the
center of each IB face where pressure and velocity components are co-located
and treated as unknowns. One closure condition considered for the iHS is the
assignment of the surface gradient of pressure phase over the IBs,
corresponding to assigning the shape of the acoustic waveform at the IB. The
iHS procedure is carried out independently for each frequency in order to
return the complete broadband complex impedance distribution at the IBs in any
desired frequency range. The iHS approach is first validated against Rott's
theory for both inviscid and viscous, rectangular and circular ducts. The
impedance of a geometrically complex toy cavity is then reconstructed and
verified against companion full compressible unstructured Navier-Stokes
simulations resolving the cavity geometry and one-dimensional impedance test
tube calculations based on time-domain impedance boundary conditions (TDIBC).
The iHS methodology is also shown to capture thermoacoustic effects, with
reconstructed impedance values quantitatively in agreement with thermoacoustic
growth rates.
",0,0
"  The impact of random fluctuations on the dynamical behavior a complex
biological systems is a longstanding issue, whose understanding would shed
light on the evolutionary pressure that nature imposes on the intrinsic noise
levels and would allow rationally designing synthetic networks with controlled
noise. Using the Itō stochastic differential equation formalism, we performed
both analytic and numerical analyses of several model systems containing
different molecular species in contact with the environment and interacting
with each other through mass-action kinetics. These systems represent for
example biomolecular oligomerization processes, complex-breakage reactions,
signaling cascades or metabolic networks. For chemical reaction networks with
zero deficiency values, which admit a detailed- or complex-balanced steady
state, all molecular species are uncorrelated. The number of molecules of each
species follow a Poisson distribution and their Fano factors, which measure the
intrinsic noise, are equal to one. Systems with deficiency one have an
unbalanced non-equilibrium steady state and a non-zero S-flux, defined as the
flux flowing between the complexes multiplied by an adequate stoichiometric
coefficient. In this case, the noise on each species is reduced if the flux
flows from the species of lowest to highest complexity, and is amplified is the
flux goes in the opposite direction. These results are generalized to systems
of deficiency two, which possess two independent non-vanishing S-fluxes, and we
conjecture that a similar relation holds for higher deficiency systems.
",0,0
"  Rare regions with weak disorder (Griffiths regions) have the potential to
spoil localization. We describe a non-perturbative construction of local
integrals of motion (LIOMs) for a weakly interacting spin chain in one
dimension, under a physically reasonable assumption on the statistics of
eigenvalues. We discuss ideas about the situation in higher dimensions, where
one can no longer ensure that interactions involving the Griffiths regions are
much smaller than the typical energy-level spacing for such regions. We argue
that ergodicity is restored in dimension d > 1, although equilibration should
be extremely slow, similar to the dynamics of glasses.
",0,0
"  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB
functions for the analysis and solution of fault detection and model detection
problems. The implemented functions are based on the computational procedures
described in the Chapters 5, 6 and 7 of the book: ""A. Varga, Solving Fault
Diagnosis Problems - Linear Synthesis Techniques, Springer, 2017"". This
document is the User's Guide for the version V1.0 of FDITOOLS. First, we
present the mathematical background for solving several basic exact and
approximate synthesis problems of fault detection filters and model detection
filters. Then, we give in-depth information on the command syntax of the main
analysis and synthesis functions. Several examples illustrate the use of the
main functions of FDITOOLS.
",1,1
"  Detectability of discrete event systems (DESs) is a question whether the
current and subsequent states can be determined based on observations. Shu and
Lin designed a polynomial-time algorithm to check strong (periodic)
detectability and an exponential-time (polynomial-space) algorithm to check
weak (periodic) detectability. Zhang showed that checking weak (periodic)
detectability is PSpace-complete. This intractable complexity opens a question
whether there are structurally simpler DESs for which the problem is tractable.
In this paper, we show that it is not the case by considering DESs represented
as deterministic finite automata without non-trivial cycles, which are
structurally the simplest deadlock-free DESs. We show that even for such very
simple DESs, checking weak (periodic) detectability remains intractable. On the
contrary, we show that strong (periodic) detectability of DESs can be
efficiently verified on a parallel computer.
",1,1
"  Let $X$ be a partially ordered set with the property that each family of
order intervals of the form $[a,b],[a,\rightarrow )$ with the finite
intersection property has a nonempty intersection. We show that every directed
subset of $X$ has a supremum. Then we apply the above result to prove that if
$X$ is a topological space with a partial order $\preceq $ for which the order
intervals are compact, $\mathcal{F}$ a nonempty commutative family of monotone
maps from $X$ into $X$ and there exists $c\in X$ such that $c\preceq Tc$ for
every $T\in \mathcal{F}$, then the set of common fixed points of $\mathcal{F}$
is nonempty and has a maximal element. The result, specialized to the case of
Banach spaces gives a general fixed point theorem that drops almost all
assumptions from the recent results in this area. An application to the theory
of integral equations of Urysohn's type is also given.
",0,0
"  Efficient methods are proposed, for computing integrals appeaing in
electronic structure calculations. The methods consist of two parts: the first
part is to represent the integrals as contour integrals and the second one is
to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The
efficiency of the proposed methods is demonstrated through numerical
experiments.
",0,0
"  We present a novel sound localization algorithm for a non-line-of-sight
(NLOS) sound source in indoor environments. Our approach exploits the
diffraction properties of sound waves as they bend around a barrier or an
obstacle in the scene. We combine a ray tracing based sound propagation
algorithm with a Uniform Theory of Diffraction (UTD) model, which simulate
bending effects by placing a virtual sound source on a wedge in the
environment. We precompute the wedges of a reconstructed mesh of an indoor
scene and use them to generate diffraction acoustic rays to localize the 3D
position of the source. Our method identifies the convergence region of those
generated acoustic rays as the estimated source position based on a particle
filter. We have evaluated our algorithm in multiple scenarios consisting of a
static and dynamic NLOS sound source. In our tested cases, our approach can
localize a source position with an average accuracy error, 0.7m, measured by
the L2 distance between estimated and actual source locations in a 7m*7m*3m
room. Furthermore, we observe 37% to 130% improvement in accuracy over a
state-of-the-art localization method that does not model diffraction effects,
especially when a sound source is not visible to the robot.
",1,1
"  In this paper we introduce the notion of $\zeta$-crossbreeding in a set of
$\zeta$-factorization formulas and also the notion of complete hybrid formula
as the final result of that crossbreeding. The last formula is used as a
criterion for selection of families of $\zeta$-kindred elements in class of
real continuous functions.
Dedicated to recalling of Gregory Mendel's pea-crossbreeding.
",0,0
"  We consider the problem of estimating the $L_1$ distance between two discrete
probability measures $P$ and $Q$ from empirical data in a nonasymptotic and
large alphabet setting. When $Q$ is known and one obtains $n$ samples from $P$,
we show that for every $Q$, the minimax rate-optimal estimator with $n$ samples
achieves performance comparable to that of the maximum likelihood estimator
(MLE) with $n\ln n$ samples. When both $P$ and $Q$ are unknown, we construct
minimax rate-optimal estimators whose worst case performance is essentially
that of the known $Q$ case with $Q$ being uniform, implying that $Q$ being
uniform is essentially the most difficult case. The \emph{effective sample size
enlargement} phenomenon, identified in Jiao \emph{et al.} (2015), holds both in
the known $Q$ case for every $Q$ and the $Q$ unknown case. However, the
construction of optimal estimators for $\|P-Q\|_1$ requires new techniques and
insights beyond the approximation-based method of functional estimation in Jiao
\emph{et al.} (2015).
",0,1
"  We investigate the density large deviation function for a multidimensional
conservation law in the vanishing viscosity limit, when the probability
concentrates on weak solutions of a hyperbolic conservation law conservation
law. When the conductivity and dif-fusivity matrices are proportional, i.e. an
Einstein-like relation is satisfied, the problem has been solved in [4]. When
this proportionality does not hold, we compute explicitly the large deviation
function for a step-like density profile, and we show that the associated
optimal current has a non trivial structure. We also derive a lower bound for
the large deviation function, valid for a general weak solution, and leave the
general large deviation function upper bound as a conjecture.
",0,0
"  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
",1,1
"  In 1978 Brakke introduced the mean curvature flow in the setting of geometric
measure theory. There exist multiple variants of the original definition. Here
we prove that most of them are indeed equal. One central point is to correct
the proof of Brakke's §3.5, where he develops an estimate for the evolution
of the measure of time-dependent test functions.
",0,0
"  With recent advancements in drone technology, researchers are now considering
the possibility of deploying small cells served by base stations mounted on
flying drones. A major advantage of such drone small cells is that the
operators can quickly provide cellular services in areas of urgent demand
without having to pre-install any infrastructure. Since the base station is
attached to the drone, technically it is feasible for the base station to
dynamic reposition itself in response to the changing locations of users for
reducing the communication distance, decreasing the probability of signal
blocking, and ultimately increasing the spectral efficiency. In this paper, we
first propose distributed algorithms for autonomous control of drone movements,
and then model and analyse the spectral efficiency performance of a drone small
cell to shed new light on the fundamental benefits of dynamic repositioning. We
show that, with dynamic repositioning, the spectral efficiency of drone small
cells can be increased by nearly 100\% for realistic drone speed, height, and
user traffic model and without incurring any major increase in drone energy
consumption.
",1,1
"  Electronic health records (EHR) contain a large variety of information on the
clinical history of patients such as vital signs, demographics, diagnostic
codes and imaging data. The enormous potential for discovery in this rich
dataset is hampered by its complexity and heterogeneity.
We present the first study to assess unsupervised homogenization pipelines
designed for EHR clustering. To identify the optimal pipeline, we tested
accuracy on simulated data with varying amounts of redundancy, heterogeneity,
and missingness. We identified two optimal pipelines: 1) Multiple Imputation by
Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE,
Z-scoring, and Deep Autoencoders.
",0,1
"  Artificial Neural Network computation relies on intensive vector-matrix
multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
showed a feasibility of implementing such operations with high energy
efficiency, thus there are many works on efficiently utilizing emerging NVM
crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
characteristics restrain critical design parameters, such as the read voltage
and weight range, resulting in substantial accuracy loss. In this paper,
instead of optimizing hardware parameters to a given neural network, we propose
a methodology of reconstructing a neural network itself optimized to resistive
memory crossbar arrays. To verify the validity of the proposed method, we
simulated various neural network with MNIST and CIFAR-10 dataset using two
different specific Resistive Random Access Memory (RRAM) model. Simulation
results show that our proposed neural network produces significantly higher
inference accuracies than conventional neural network when the synapse devices
have nonlinear I-V characteristics.
",1,1
"  In this work, we establish a full single-letter characterization of the
rate-distortion region of an instance of the Gray-Wyner model with side
information at the decoders. Specifically, in this model an encoder observes a
pair of memoryless, arbitrarily correlated, sources $(S^n_1,S^n_2)$ and
communicates with two receivers over an error-free rate-limited link of
capacity $R_0$, as well as error-free rate-limited individual links of
capacities $R_1$ to the first receiver and $R_2$ to the second receiver. Both
receivers reproduce the source component $S^n_2$ losslessly; and Receiver $1$
also reproduces the source component $S^n_1$ lossily, to within some prescribed
fidelity level $D_1$. Also, Receiver $1$ and Receiver $2$ are equipped
respectively with memoryless side information sequences $Y^n_1$ and $Y^n_2$.
Important in this setup, the side information sequences are arbitrarily
correlated among them, and with the source pair $(S^n_1,S^n_2)$; and are not
assumed to exhibit any particular ordering. Furthermore, by specializing the
main result to two Heegard-Berger models with successive refinement and
scalable coding, we shed light on the roles of the common and private
descriptions that the encoder should produce and what they should carry
optimally. We develop intuitions by analyzing the developed single-letter
optimal rate-distortion regions of these models, and discuss some insightful
binary examples.
",1,1
"  This work discusses the numerical approximation of a nonlinear
reaction-advection-diffusion equation, which is a dimensionless form of the
Weertman equation. This equation models steadily-moving dislocations in
materials science. It reduces to the celebrated Peierls-Nabarro equation when
its advection term is set to zero. The approach rests on considering a
time-dependent formulation, which admits the equation under study as its
long-time limit. Introducing a Preconditioned Collocation Scheme based on
Fourier transforms, the iterative numerical method presented solves the
time-dependent problem, delivering at convergence the desired numerical
solution to the Weertman equation. Although it rests on an explicit
time-evolution scheme, the method allows for large time steps, and captures the
solution in a robust manner. Numerical results illustrate the efficiency of the
approach for several types of nonlinearities.
",0,1
"  There are many web-based visualization systems available to date, each having
its strengths and limitations. The goals these systems set out to accomplish
influence design decisions and determine how reusable and scalable they are.
Weave is a new web-based visualization platform with the broad goal of enabling
visualization of any available data by anyone for any purpose. Our open source
framework supports highly interactive linked visualizations for users of
varying skill levels. What sets Weave apart from other systems is its
consideration for real-time remote collaboration with session history. We
provide a detailed account of the various framework designs we considered with
comparisons to existing state-of-the-art systems.
",1,1
"  We present an investigation of the supernova remnant (SNR) G306.3$-$0.9 using
archival multi-wavelength data. The Suzaku spectra are well described by
two-component thermal plasma models: The soft component is in ionization
equilibrium and has a temperature $\sim$0.59 keV, while the hard component has
temperature $\sim$3.2 keV and ionization time-scale $\sim$$2.6\times10^{10}$
cm$^{-3}$ s. We clearly detected Fe K-shell line at energy of $\sim$6.5 keV
from this remnant. The overabundances of Si, S, Ar, Ca, and Fe confirm that the
X-ray emission has an ejecta origin. The centroid energy of the Fe-K line
supports that G306.3$-$0.9 is a remnant of a Type Ia supernova (SN) rather than
a core-collapse SN. The GeV gamma-ray emission from G306.3$-$0.9 and its
surrounding were analyzed using about 6 years of Fermi data. We report about
the non-detection of G306.3$-$0.9 and the detection of a new extended gamma-ray
source in the south-west of G306.3$-$0.9 with a significance of
$\sim$13$\sigma$. We discuss several scenarios for these results with the help
of data from other wavebands to understand the SNR and its neighborhood.
",0,0
"  Previous approaches to training syntax-based sentiment classification models
required phrase-level annotated corpora, which are not readily available in
many languages other than English. Thus, we propose the use of tree-structured
Long Short-Term Memory with an attention mechanism that pays attention to each
subtree of the parse tree. Experimental results indicate that our model
achieves the state-of-the-art performance in a Japanese sentiment
classification task.
",1,1
"  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale datasets. However, even when MFVB provides accurate posterior means
for certain parameters, it often mis-estimates variances and covariances.
Furthermore, prior robustness measures have remained undeveloped for MFVB. By
deriving a simple formula for the effect of infinitesimal model perturbations
on MFVB posterior means, we provide both improved covariance estimates and
local robustness measures for MFVB, thus greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
relating derivatives of posterior expectations to posterior covariances and
include the Laplace approximation as a special case. Our key condition is that
the MFVB approximation provides good estimates of a select subset of posterior
means---an assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC.
",0,1
"  In this paper, we empirically study models for pricing Italian sovereign
bonds under a reduced form framework, by assuming different dynamics for the
short-rate process. We analyze classical Cox-Ingersoll-Ross and Vasicek
multi-factor models, with a focus on optimization algorithms applied in the
calibration exercise. The Kalman filter algorithm together with a maximum
likelihood estimation method are considered to fit the Italian term-structure
over a 12-year horizon, including the global financial crisis and the euro area
sovereign debt crisis. Analytic formulas for the gradient vector and the
Hessian matrix of the likelihood function are provided.
",0,0
"  Ballistic point contact (BPC) with zigzag edges in graphene is a main
candidate of a valley filter, in which the polarization of the valley degree of
freedom can be selected by using a local gate voltage. Here, we propose to
detect the valley filtering effect by Andreev reflection. Because electrons in
the lowest conduction band and the highest valence band of the BPC possess
opposite chirality, the inter-band Andreev reflection is strongly suppressed,
after multiple scattering and interference. We draw this conclusion by both the
scattering matrix analysis and the numerical simulation. The Andreev reflection
as a function of the incident energy of electrons and the local gate voltage at
the BPC is obtained, by which the parameter region for a perfect valley filter
and the direction of valley polarization can be determined. The Andreev
reflection exhibits an oscillatory decay with the length of the BPC, indicating
a negative correlation to valley polarization.
",0,0
"  Sparse superposition (SS) codes were originally proposed as a
capacity-achieving communication scheme over the additive white Gaussian noise
channel (AWGNC) [1]. Very recently, it was discovered that these codes are
universal, in the sense that they achieve capacity over any memoryless channel
under generalized approximate message-passing (GAMP) decoding [2], although
this decoder has never been stated for SS codes. In this contribution we
introduce the GAMP decoder for SS codes, we confirm empirically the
universality of this communication scheme through its study on various channels
and we provide the main analysis tools: state evolution and potential. We also
compare the performance of GAMP with the Bayes-optimal MMSE decoder. We
empirically illustrate that despite the presence of a phase transition
preventing GAMP to reach the optimal performance, spatial coupling allows to
boost the performance that eventually tends to capacity in a proper limit. We
also prove that, in contrast with the AWGNC case, SS codes for binary input
channels have a vanishing error floor in the limit of large codewords.
Moreover, the performance of Hadamard-based encoders is assessed for practical
implementations.
",1,1
"  When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform.
",1,1
"  We propose an approach to estimate 3D human pose in real world units from a
single RGBD image and show that it exceeds performance of monocular 3D pose
estimation approaches from color as well as pose estimation exclusively from
depth. Our approach builds on robust human keypoint detectors for color images
and incorporates depth for lifting into 3D. We combine the system with our
learning from demonstration framework to instruct a service robot without the
need of markers. Experiments in real world settings demonstrate that our
approach enables a PR2 robot to imitate manipulation actions observed from a
human teacher.
",1,1
"  We extend the work of Fouvry, Kowalski and Michel on correlation between
Hecke eigenvalues of modular forms and algebraic trace functions in order to
establish an asymptotic formula for a generalized cubic moment of modular
L-functions at the central point s = 1/2 and for prime moduli q. As an
application, we exploit our recent result on the mollification of the fourth
moment of Dirichlet L-functions to derive that for any pair
$(\omega_1,\omega_2)$ of multiplicative characters modulo q, there is a
positive proportion of $\chi$ (mod q) such that $L(\chi, 1/2 ), L(\chi\omega_1,
1/2 )$ and $L(\chi\omega_2, 1/2)$ are simultaneously not too small.
",0,0
"  Nonclassical states of a quantized light are described in terms of
Glauber-Sudarshan P distribution which is not a genuine classical probability
distribution. Despite several attempts, defining a uniform measure of
nonclassicality (NC) for the single mode quantum states of light is yet an open
task. In our previous work [Phys. Rev. A 95, 012330 (2017)] we have shown that
the existing well-known measures fail to quantify the NC of single mode states
that are generated under multiple NC-inducing operations. Recently, Ivan et.
al. [Quantum. Inf. Process. 11, 853 (2012)] have defined a measure of
non-Gaussian character of quantum optical states in terms of Wehrl entropy.
Here, we adopt this concept in the context of single mode NC. In this paper, we
propose a new quantification of NC for the single mode quantum states of light
as the difference between the total Wehrl entropy of the state and the maximum
Wehrl entropy arising due to its classical characteristics. This we achieve by
subtracting from its Wehrl entropy, the maximum Wehrl entropy attainable by any
classical state that has same randomness as measured in terms of von-Neumann
entropy. We obtain analytic expressions of NC for most of the states, in
particular, all pure states and Gaussian mixed states. However, the evaluation
of NC for the non-Gaussian mixed states is subject to extensive numerical
computation that lies beyond the scope of the current work. We show that, along
with the states generated under single NC-inducing operations, also for the
broader class of states that are generated under multiple NC-inducing
operations, our quantification enumerates the NC consistently.
",1,0
"  Following the recent progress in image classification and captioning using
deep learning, we develop a novel natural language person retrieval system
based on an attention mechanism. More specifically, given the description of a
person, the goal is to localize the person in an image. To this end, we first
construct a benchmark dataset for natural language person retrieval. To do so,
we generate bounding boxes for persons in a public image dataset from the
segmentation masks, which are then annotated with descriptions and attributes
using the Amazon Mechanical Turk. We then adopt a region proposal network in
Faster R-CNN as a candidate region generator. The cropped images based on the
region proposals as well as the whole images with attention weights are fed
into Convolutional Neural Networks for visual feature extraction, while the
natural language expression and attributes are input to Bidirectional Long
Short- Term Memory (BLSTM) models for text feature extraction. The visual and
text features are integrated to score region proposals, and the one with the
highest score is retrieved as the output of our system. The experimental
results show significant improvement over the state-of-the-art method for
generic object retrieval and this line of research promises to benefit search
in surveillance video footage.
",1,1
"  Real time large scale streaming data pose major challenges to forecasting, in
particular defying the presence of human experts to perform the corresponding
analysis. We present here a class of models and methods used to develop an
automated, scalable and versatile system for large scale forecasting oriented
towards safety and security monitoring. Our system provides short and long term
forecasts and uses them to detect safety and security issues in relation with
multiple internet connected devices well in advance they might take place.
",0,1
"  Machine learning algorithms such as linear regression, SVM and neural network
have played an increasingly important role in the process of scientific
discovery. However, none of them is both interpretable and accurate on
nonlinear datasets. Here we present contextual regression, a method that joins
these two desirable properties together using a hybrid architecture of neural
network embedding and dot product layer. We demonstrate its high prediction
accuracy and sensitivity through the task of predictive feature selection on a
simulated dataset and the application of predicting open chromatin sites in the
human genome. On the simulated data, our method achieved high fidelity recovery
of feature contributions under random noise levels up to 200%. On the open
chromatin dataset, the application of our method not only outperformed the
state of the art method in terms of accuracy, but also unveiled two previously
unfound open chromatin related histone marks. Our method can fill the blank of
accurate and interpretable nonlinear modeling in scientific data mining tasks.
",1,1
"  We consider multi-time correlators for output signals from linear detectors,
continuously measuring several qubit observables at the same time. Using the
quantum Bayesian formalism, we show that for unital (symmetric) evolution in
the absence of phase backaction, an $N$-time correlator can be expressed as a
product of two-time correlators when $N$ is even. For odd $N$, there is a
similar factorization, which also includes a single-time average. Theoretical
predictions agree well with experimental results for two detectors, which
simultaneously measure non-commuting qubit observables.
",0,0
"  Constraint Handling Rules is an effective concurrent declarative programming
language and a versatile computational logic formalism. CHR programs consist of
guarded reactive rules that transform multisets of constraints. One of the main
features of CHR is its inherent concurrency. Intuitively, rules can be applied
to parts of a multiset in parallel. In this comprehensive survey, we give an
overview of concurrent and parallel as well as distributed CHR semantics,
standard and more exotic, that have been proposed over the years at various
levels of refinement. These semantics range from the abstract to the concrete.
They are related by formal soundness results. Their correctness is established
as correspondence between parallel and sequential computations. We present
common concise sample CHR programs that have been widely used in experiments
and benchmarks. We review parallel CHR implementations in software and
hardware. The experimental results obtained show a consistent parallel speedup.
Most implementations are available online. The CHR formalism can also be used
to implement and reason with models for concurrency. To this end, the Software
Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
have been faithfully encoded in CHR. Under consideration in Theory and Practice
of Logic Programming (TPLP).
",1,1
"  Many people are suffering from voice disorders, which can adversely affect
the quality of their lives. In response, some researchers have proposed
algorithms for automatic assessment of these disorders, based on voice signals.
However, these signals can be sensitive to the recording devices. Indeed, the
channel effect is a pervasive problem in machine learning for healthcare. In
this study, we propose a detection system for pathological voice, which is
robust against the channel effect. This system is based on a bidirectional LSTM
network. To increase the performance robustness against channel mismatch, we
integrate domain adversarial training (DAT) to eliminate the differences
between the devices. When we train on data recorded on a high-quality
microphone and evaluate on smartphone data without labels, our robust detection
system increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target
sample labels). To the best of our knowledge, this is the first study applying
unsupervised domain adaptation to pathological voice detection. Notably, our
system does not need target device sample labels, which allows for
generalization to many new devices.
",1,1
"  Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.
",1,1
"  Investigating the emergence of a particular cell type is a recurring theme in
models of growing cellular populations. The evolution of resistance to therapy
is a classic example. Common questions are: when does the cell type first
occur, and via which sequence of steps is it most likely to emerge? For growing
populations, these questions can be formulated in a general framework of
branching processes spreading through a graph from a root to a target vertex.
Cells have a particular fitness value on each vertex and can transition along
edges at specific rates. Vertices represents cell states, say \mic{genotypes
}or physical locations, while possible transitions are acquiring a mutation or
cell migration. We focus on the setting where cells at the root vertex have the
highest fitness and transition rates are small. Simple formulas are derived for
the time to reach the target vertex and for the probability that it is reached
along a given path in the graph. We demonstrate our results on \mic{several
scenarios relevant to the emergence of drug resistance}, including: the
orderings of resistance-conferring mutations in bacteria and the impact of
imperfect drug penetration in cancer.
",0,0
"  Stimuli-responsive materials that modify their shape in response to changes
in environmental conditions -- such as solute concentration, temperature, pH,
and stress -- are widespread in nature and technology. Applications include
micro- and nanoporous materials used in filtration and flow control. The
physiochemical mechanisms that induce internal volume modifications have been
widely studies. The coupling between induced volume changes and solute
transport through porous materials, however, is not well understood. Here, we
consider advective and diffusive transport through a small channel linking two
large reservoirs. A section of stimulus-responsive material regulates the
channel permeability, which is a function of the local solute concentration. We
derive an exact solution to the coupled transport problem and demonstrate the
existence of a flow regime in which the steady state is reached via a damped
oscillation around the equilibrium concentration value. Finally, the
feasibility of an experimental observation of the phenomena is discussed.
Please note that this version of the paper has not been formally peer reviewed,
revised or accepted by a journal.
",0,0
"  Today's landscape of robotics is dominated by vertical integration where
single vendors develop the final product leading to slow progress, expensive
products and customer lock-in. Opposite to this, an horizontal integration
would result in a rapid development of cost-effective mass-market products with
an additional consumer empowerment. The transition of an industry from vertical
integration to horizontal integration is typically catalysed by de facto
industry standards that enable a simplified and seamless integration of
products. However, in robotics there is currently no leading candidate for a
global plug-and-play standard.
This paper tackles the problem of incompatibility between robot components
that hinder the reconfigurability and flexibility demanded by the robotics
industry. Particularly, it presents a model to create plug-and-play robot
hardware components. Rather than iteratively evolving previous ontologies, our
proposed model answers the needs identified by the industry while facilitating
interoperability, measurability and comparability of robotics technology. Our
approach differs significantly with the ones presented before as it is
hardware-oriented and establishes a clear set of actions towards the
integration of this model in real environments and with real manufacturers.
",1,1
"  Machine learning models, especially based on deep architectures are used in
everyday applications ranging from self driving cars to medical diagnostics. It
has been shown that such models are dangerously susceptible to adversarial
samples, indistinguishable from real samples to human eye, adversarial samples
lead to incorrect classifications with high confidence. Impact of adversarial
samples is far-reaching and their efficient detection remains an open problem.
We propose to use direct density ratio estimation as an efficient model
agnostic measure to detect adversarial samples. Our proposed method works
equally well with single and multi-channel samples, and with different
adversarial sample generation methods. We also propose a method to use density
ratio estimates for generating adversarial samples with an added constraint of
preserving density ratio.
",1,1
"  We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.
",1,1
"  This paper studies the emotion recognition from musical tracks in the
2-dimensional valence-arousal (V-A) emotional space. We propose a method based
on convolutional (CNN) and recurrent neural networks (RNN), having
significantly fewer parameters compared with the state-of-the-art method for
the same task. We utilize one CNN layer followed by two branches of RNNs
trained separately for arousal and valence. The method was evaluated using the
'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
arousal and 0.268 for valence, which is the best result reported on this
dataset.
",1,1
"  We consider previous models of Timed, Probabilistic and Stochastic Timed
Automata, we introduce our model of Timed Automata with Polynomial Delay and we
characterize the expressiveness of these models relative to each other.
",1,1
"  We present muon spin rotation measurements on superconducting Cu intercalated
Bi$_2$Se$_3$, which was suggested as a realization of a topological
superconductor. We observe a clear evidence of the superconducting transition
below 4 K, where the width of magnetic field distribution increases as the
temperature is decreased. The measured broadening at mK temperatures suggests a
large London penetration depth in the $ab$ plane ($\lambda_{\mathrm{eff}}\sim
1.6$ $\mathrm{\mu}$m). We show that the temperature dependence of this
broadening follows the BCS prediction, but could be consistent with several gap
symmetries.
",0,0
"  Here we reveal details of the interaction between human lysozyme proteins,
both native and fibrils, and their water environment by intense terahertz time
domain spectroscopy. With the aid of a rigorous dielectric model, we determine
the amplitude and phase of the oscillating dipole induced by the THz field in
the volume containing the protein and its hydration water. At low
concentrations, the amplitude of this induced dipolar response decreases with
increasing concentration. Beyond a certain threshold, marking the onset of the
interactions between the extended hydration shells, the amplitude remains fixed
but the phase of the induced dipolar response, which is initially in phase with
the applied THz field, begins to change. The changes observed in the THz
response reveal protein-protein interactions me-diated by extended hydration
layers, which may control fibril formation and may have an important role in
chemical recognition phenomena.
",0,0
"  We report on experimentally measured light shifts of superconducting flux
qubits deep-strongly coupled to LC oscillators, where the coupling constants
are comparable to the qubit and oscillator resonance frequencies. By using
two-tone spectroscopy, the energies of the six lowest levels of each circuit
are determined. We find huge Lamb shifts that exceed 90% of the bare qubit
frequencies and inversions of the qubits' ground and excited states when there
are a finite number of photons in the oscillator. Our experimental results
agree with theoretical predictions based on the quantum Rabi model.
",0,0
"  We describe a novel weakly supervised deep learning framework that combines
both the discriminative and generative models to learn meaningful
representation in the multiple instance learning (MIL) setting. MIL is a weakly
supervised learning problem where labels are associated with groups of
instances (referred as bags) instead of individual instances. To address the
essential challenge in MIL problems raised from the uncertainty of positive
instances label, we use a discriminative model regularized by variational
autoencoders (VAEs) to maximize the differences between latent representations
of all instances and negative instances. As a result, the hidden layer of the
variational autoencoder learns meaningful representation. This representation
can effectively be used for MIL problems as illustrated by better performance
on the standard benchmark datasets comparing to the state-of-the-art
approaches. More importantly, unlike most related studies, the proposed
framework can be easily scaled to large dataset problems, as illustrated by the
audio event detection and segmentation task. Visualization also confirms the
effectiveness of the latent representation in discriminating positive and
negative classes.
",0,1
"  We establish the C^{1,1} regularity of quasi-psh envelopes in a Kahler class,
confirming a conjecture of Berman.
",0,0
"  Let $M$ be a complex manifold of dimension $n$ with smooth connected boundary
$X$. Assume that $\overline M$ admits a holomorphic $S^1$-action preserving the
boundary $X$ and the $S^1$-action is transversal and CR on $X$. We show that
the $\overline\partial$-Neumann Laplacian on $M$ is transversally elliptic and
as a consequence, the $m$-th Fourier component of the $q$-th Dolbeault
cohomology group $H^q_m(\overline M)$ is finite dimensional, for every
$m\in\mathbb Z$ and every $q=0,1,\ldots,n$. This enables us to define
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ the $m$-th Fourier
component of the Euler characteristic on $M$ and to study large $m$-behavior of
$H^q_m(\overline M)$. In this paper, we establish an index formula for
$\sum^{n}_{j=0}(-1)^j{\rm dim\,}H^q_m(\overline M)$ and Morse inequalities for
$H^q_m(\overline M)$.
",0,0
"  Reinforcement learning methods require careful design involving a reward
function to obtain the desired action policy for a given task. In the absence
of hand-crafted reward functions, prior work on the topic has proposed several
methods for reward estimation by using expert state trajectories and action
pairs. However, there are cases where complete or good action information
cannot be obtained from expert demonstrations. We propose a novel reinforcement
learning method in which the agent learns an internal model of observation on
the basis of expert-demonstrated state trajectories to estimate rewards without
completely learning the dynamics of the external environment from state-action
pairs. The internal model is obtained in the form of a predictive model for the
given expert state distribution. During reinforcement learning, the agent
predicts the reward as a function of the difference between the actual state
and the state predicted by the internal model. We conducted multiple
experiments in environments of varying complexity, including the Super Mario
Bros and Flappy Bird games. We show our method successfully trains good
policies directly from expert game-play videos.
",1,1
"  In this paper we are interested in the class of n-ary operations on an
arbitrary chain that are quasitrivial, symmetric, nondecreasing, and
associative. We first provide a description of these operations. We then prove
that associativity can be replaced with bisymmetry in the definition of this
class. Finally we investigate the special situation where the chain is finite.
",0,0
"  We propose a new multivariate dependency measure. It is obtained by
considering a Gaussian kernel based distance between the copula transform of
the given d-dimensional distribution and the uniform copula and then
appropriately normalizing it. The resulting measure is shown to satisfy a
number of desirable properties. A nonparametric estimate is proposed for this
dependency measure and its properties (finite sample as well as asymptotic) are
derived. Some comparative studies of the proposed dependency measure estimate
with some widely used dependency measure estimates on artificial datasets are
included. A non-parametric test of independence between two or more random
variables based on this measure is proposed. A comparison of the proposed test
with some existing nonparametric multivariate test for independence is
presented.
",0,0
"  The pyrochlore metal Cd2Re2O7 has been recently investigated by
second-harmonic generation (SHG) reflectivity. In this paper, we develop a
general formalism that allows for the identification of the relevant tensor
components of the SHG from azimuthal scans. We demonstrate that the secondary
order parameter identified by SHG at the structural phase transition is the
x2-y2 component of the axial toroidal quadrupole. This differs from the 3z2-r2
symmetry of the atomic displacements associated with the I-4m2 crystal
structure that was previously thought to be its origin. Within the same
formalism, we suggest that the primary order parameter detected in the SHG
experiment is the 3z2-r2 component of the magnetic quadrupole. We discuss the
general mechanism driving the phase transition in our proposed framework, and
suggest experiments, particularly resonant X-ray scattering ones, that could
clarify this issue.
",0,0
"  In evolutionary biology, the speciation history of living organisms is
represented graphically by a phylogeny, that is, a rooted tree whose leaves
correspond to current species and branchings indicate past speciation events.
Phylogenies are commonly estimated from molecular sequences, such as DNA
sequences, collected from the species of interest. At a high level, the idea
behind this inference is simple: the further apart in the Tree of Life are two
species, the greater is the number of mutations to have accumulated in their
genomes since their most recent common ancestor. In order to obtain accurate
estimates in phylogenetic analyses, it is standard practice to employ
statistical approaches based on stochastic models of sequence evolution on a
tree. For tractability, such models necessarily make simplifying assumptions
about the evolutionary mechanisms involved. In particular, commonly omitted are
insertions and deletions of nucleotides -- also known as indels.
Properly accounting for indels in statistical phylogenetic analyses remains a
major challenge in computational evolutionary biology. Here we consider the
problem of reconstructing ancestral sequences on a known phylogeny in a model
of sequence evolution incorporating nucleotide substitutions, insertions and
deletions, specifically the classical TKF91 process. We focus on the case of
dense phylogenies of bounded height, which we refer to as the taxon-rich
setting, where statistical consistency is achievable. We give the first
polynomial-time ancestral reconstruction algorithm with provable guarantees
under constant rates of mutation. Our algorithm succeeds when the phylogeny
satisfies the ""big bang"" condition, a necessary and sufficient condition for
statistical consistency in this context.
",1,1
"  Subject of research is complex networks and network systems. The network
system is defined as a complex network in which flows are moved. Classification
of flows in the network is carried out on the basis of ordering and continuity.
It is shown that complex networks with different types of flows generate
various network systems. Flow analogues of the basic concepts of the theory of
complex networks are introduced and the main problems of this theory in terms
of flow characteristics are formulated. Local and global flow characteristics
of networks bring closer the theory of complex networks to the systems theory
and systems analysis. Concept of flow core of network system is introduced and
defined how it simplifies the process of its investigation. Concepts of kernel
and flow core of multiplex are determined. Features of operation of multiplex
type systems are analyzed.
",1,1
"  We study the effect of domain growth on the orientation of striped phases in
a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter
dependence that allows stripe formation in a half plane, and suppresses
patterns in the complement, while the boundary of the pattern-forming region is
propagating with fixed normal velocity. We construct front solutions that leave
behind stripes in the pattern-forming region that are parallel to or at a small
oblique angle to the boundary.
Technically, the construction of stripe formation parallel to the boundary
relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at
a small oblique angle are constructed using a functional-analytic, perturbative
approach. Here, the main difficulties are the presence of continuous spectrum
and the fact that small oblique angles appear as a singular perturbation in a
traveling-wave problem. We resolve the former difficulty using a farfield-core
decomposition and Fredholm theory in weighted spaces. The singular perturbation
problem is resolved using preconditioners and boot-strapping.
",0,0
"  This paper discusses minimum distance estimation method in the linear
regression model with dependent errors which are strongly mixing. The
regression parameters are estimated through the minimum distance estimation
method, and asymptotic distributional properties of the estimators are
discussed. A simulation study compares the performance of the minimum distance
estimator with other well celebrated estimator. This simulation study shows the
superiority of the minimum distance estimator over another estimator. KoulMde
(R package) which was used for the simulation study is available online. See
section 4 for the detail.
",0,0
"  Mobile edge clouds (MECs) bring the benefits of the cloud closer to the user,
by installing small cloud infrastructures at the network edge. This enables a
new breed of real-time applications, such as instantaneous object recognition
and safety assistance in intelligent transportation systems, that require very
low latency. One key issue that comes with proximity is how to ensure that
users always receive good performance as they move across different locations.
Migrating services between MECs is seen as the means to achieve this. This
article presents a layered framework for migrating active service applications
that are encapsulated either in virtual machines (VMs) or containers. This
layering approach allows a substantial reduction in service downtime. The
framework is easy to implement using readily available technologies, and one of
its key advantages is that it supports containers, which is a promising
emerging technology that offers tangible benefits over VMs. The migration
performance of various real applications is evaluated by experiments under the
presented framework. Insights drawn from the experimentation results are
discussed.
",1,1
"  Analog black/white hole pairs, consisting of a region of supersonic flow,
have been achieved in a recent experiment by J. Steinhauer using an elongated
Bose-Einstein condensate. A growing standing density wave, and a checkerboard
feature in the density-density correlation function, were observed in the
supersonic region. We model the density-density correlation function, taking
into account both quantum fluctuations and the shot-to-shot variation of atom
number normally present in ultracold-atom experiments. We find that quantum
fluctuations alone produce some, but not all, of the features of the
correlation function, whereas atom-number fluctuation alone can produce all the
observed features, and agreement is best when both are included. In both cases,
the density-density correlation is not intrinsic to the fluctuations, but
rather is induced by modulation of the standing wave caused by the
fluctuations.
",0,0
"  Let $K$ be a function field over a finite field $k$ of characteristic $p$ and
let $K_{\infty}/K$ be a geometric extension with Galois group $\mathbb{Z}_p$.
Let $K_n$ be the corresponding subextension with Galois group
$\mathbb{Z}/p^n\mathbb{Z}$ and genus $g_n$. In this paper, we give a simple
explicit formula $g_n$ in terms of an explicit Witt vector construction of the
$\mathbb{Z}_p$-tower. This formula leads to a tight lower bound on $g_n$ which
is quadratic in $p^n$. Furthermore, we determine all $\mathbb{Z}_p$-towers for
which the genus sequence is stable, in the sense that there are $a,b,c \in
\mathbb{Q}$ such that $g_n=a p^{2n}+b p^n +c$ for $n$ large enough. Such genus
stable towers are expected to have strong stable arithmetic properties for
their zeta functions. A key technical contribution of this work is a new
simplified formula for the Schmid-Witt symbol coming from local class field
theory.
",0,0
"  We study the evolution of spin-orbital correlations in an inhomogeneous
quantum system with an impurity replacing a doublon by a holon orbital degree
of freedom. Spin-orbital entanglement is large when spin correlations are
antiferromagnetic, while for a ferromagnetic host we obtain a pure orbital
description. In this regime the orbital model can be mapped on spinless
fermions and we uncover topological phases with zero energy modes at the edge
or at the domain between magnetically inequivalent regions.
",0,0
"  For autonomous agents to successfully operate in the real world, anticipation
of future events and states of their environment is a key competence. This
problem has been formalized as a sequence extrapolation problem, where a number
of observations are used to predict the sequence into the future. Real-world
scenarios demand a model of uncertainty of such predictions, as predictions
become increasingly uncertain -- in particular on long time horizons. While
impressive results have been shown on point estimates, scenarios that induce
multi-modal distributions over future sequences remain challenging. Our work
addresses these challenges in a Gaussian Latent Variable model for sequence
prediction. Our core contribution is a ""Best of Many"" sample objective that
leads to more accurate and more diverse predictions that better capture the
true variations in real-world sequence data. Beyond our analysis of improved
model fit, our models also empirically outperform prior work on three diverse
tasks ranging from traffic scenes to weather data.
",0,1
"  End-to-end approaches have drawn much attention recently for significantly
simplifying the construction of an automatic speech recognition (ASR) system.
RNN transducer (RNN-T) is one of the popular end-to-end methods. Previous
studies have shown that RNN-T is difficult to train and a very complex training
process is needed for a reasonable performance. In this paper, we explore RNN-T
for a Chinese large vocabulary continuous speech recognition (LVCSR) task and
aim to simplify the training process while maintaining performance. First, a
new strategy of learning rate decay is proposed to accelerate the model
convergence. Second, we find that adding convolutional layers at the beginning
of the network and using ordered data can discard the pre-training process of
the encoder without loss of performance. Besides, we design experiments to find
a balance among the usage of GPU memory, training circle and model performance.
Finally, we achieve 16.9% character error rate (CER) on our test set which is
2% absolute improvement from a strong BLSTM CE system with language model
trained on the same text corpus.
",1,1
"  Elasticity is a cloud property that enables applications and its execution
systems to dynamically acquire and release shared computational resources on
demand. Moreover, it unfolds the advantage of economies of scale in the cloud
through a drop in the average costs of these shared resources. However, it is
still an open challenge to achieve a perfect match between resource demand and
provision in autonomous elasticity management. Resource adaptation decisions
essentially involve a trade-off between economics and performance, which
produces a gap between the ideal and actual resource provisioning. This gap, if
not properly managed, can negatively impact the aggregate utility of a cloud
customer in the long run. To address this limitation, we propose a technical
debt-aware learning approach for autonomous elasticity management based on a
reinforcement learning of elasticity debts in resource provisioning; the
adaptation pursues strategic decisions that trades off economics against
performance. We extend CloudSim and Burlap to evaluate our approach. The
evaluation shows that a reinforcement learning of technical debts in elasticity
obtains a higher utility for a cloud customer, while conforming expected levels
of performance.
",1,1
"  This is an exposition of homotopical results on the geometric realization of
semi-simplicial spaces. We then use these to derive basic foundational results
about classifying spaces of topological categories, possibly without units. The
topics considered include: fibrancy conditions on topological categories; the
effect on classifying spaces of freely adjoining units; approximate notions of
units; Quillen's Theorems A and B for non-unital topological categories; the
effect on classifying spaces of changing the topology on the space of objects;
the Group-Completion Theorem.
",0,0
"  Answer Set Programming (ASP) is a well-established declarative paradigm. One
of the successes of ASP is the availability of efficient systems.
State-of-the-art systems are based on the ground+solve approach. In some
applications this approach is infeasible because the grounding of one or few
constraints is expensive. In this paper, we systematically compare alternative
strategies to avoid the instantiation of problematic constraints, that are
based on custom extensions of the solver. Results on real and synthetic
benchmarks highlight some strengths and weaknesses of the different strategies.
(Under consideration for acceptance in TPLP, ICLP 2017 Special Issue.)
",1,1
"  The advances in geometric approaches to optical devices due to transformation
optics has led to the development of cloaks, concentrators, and other devices.
It has also been shown that transformation optics can be used to gravitational
fields from general relativity. However, the technique is currently constrained
to linear devices, as a consistent approach to nonlinearity (including both the
case of a nonlinear background medium and a nonlinear transformation) remains
an open question. Here we show that nonlinearity can be incorporated into
transformation optics in a consistent way. We use this to illustrate a number
of novel effects, including cloaking an optical soliton, modeling nonlinear
solutions to Einstein's field equations, controlling transport in a Debye
solid, and developing a set of constitutive to relations for relativistic
cloaks in arbitrary nonlinear backgrounds.
",0,0
"  We investigate crack propagation in a simple two-dimensional visco-elastic
model and find a scaling regime in the relation between the propagation
velocity and energy release rate or fracture energy, together with lower and
upper bounds of the scaling regime. On the basis of our result, the existence
of the lower and upper bounds is expected to be universal or model-independent:
the present simple simulation model provides generic insight into the physics
of crack propagation, and the model will be a first step towards the
development of a more refined coarse-grained model. Relatively abrupt changes
of velocity are predicted near the lower and upper bounds for the scaling
regime and the positions of the bounds could be good markers for the
development of tough polymers, for which we provide simple views that could be
useful as guiding principles for toughening polymer-based materials.
",0,0
"  The fundamental group $\pi$ of a Kodaira fibration is, by definition, the
extension of a surface group $\Pi_b$ by another surface group $\Pi_g$, i.e. \[
1 \rightarrow \Pi_g \rightarrow \pi \rightarrow \Pi_b \rightarrow 1. \]
Conversely, we can inquire about what conditions need to be satisfied by a
group of that sort in order to be the fundamental group of a Kodaira fibration.
In this short note we collect some restriction on the image of the classifying
map $m \colon \Pi_b \to \Gamma_g$ in terms of the coinvariant homology of
$\Pi_g$. In particular, we observe that if $\pi$ is the fundamental group of a
Kodaira fibration with relative irregularity $g-s$, then $g \leq 1+ 6s$, and we
show that this effectively constrains the possible choices for $\pi$, namely
that there are group extensions as above that fail to satisfy this bound, hence
cannot be the fundamental group of a Kodaira fibration. In particular this
provides examples of symplectic $4$--manifolds that fail to admit a Kähler
structure for reasons that eschew the usual obstructions.
",0,0
"  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel
material are used in a variety of electronics applications. However, a
competitive CNT-based technology requires the precise placement of CNTs at
predefined locations of a substrate. One promising placement approach is to use
chemical recognition to bind CNTs from solution at the desired locations on a
surface. Producing the chemical pattern on the substrate is challenging. Here
we describe a one-step patterning approach based on a highly photosensitive
surface monolayer. The monolayer contains chromophopric group as light
sensitive body with heteroatoms as high quantum yield photolysis center. As
deposited, the layer will bind CNTs from solution. However, when exposed to
ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for
conventional photoresists, the monolayer cleaves and no longer binds CNTs.
These features allow standard, wafer-scale UV lithography processes to be used
to form a patterned chemical monolayer without the need for complex substrate
patterning or monolayer stamping.
",0,0
"  This paper derives two new optimization-driven Monte Carlo algorithms
inspired from variable splitting and data augmentation. In particular, the
formulation of one of the proposed approaches is closely related to the
alternating direction method of multipliers (ADMM) main steps. The proposed
framework enables to derive faster and more efficient sampling schemes than the
current state-of-the-art methods and can embed the latter. By sampling
efficiently the parameter to infer as well as the hyperparameters of the
problem, the generated samples can be used to approximate Bayesian estimators
of the parameters to infer. Additionally, the proposed approach brings
confidence intervals at a low cost contrary to optimization methods.
Simulations on two often-studied signal processing problems illustrate the
performance of the two proposed samplers. All results are compared to those
obtained by recent state-of-the-art optimization and MCMC algorithms used to
solve these problems.
",0,1
"  Yes, but only for a parameter value that makes it almost coincide with the
standard model. We reconsider the cosmological dynamics of a generalized
Chaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a
dark energy (DE) component with constant equation of state. This model, which
implies a specific interaction between CDM and DE, has a $\Lambda$CDM limit and
provides the basis for studying deviations from the latter. Including matter
and radiation, we use the (modified) CLASS code \cite{class} to construct the
CMB and matter power spectra in order to search for a gCg-based concordance
model that is in agreement with the SNIa data from the JLA sample and with
recent Planck data. The results reveal that the gCg parameter $\alpha$ is
restricted to $|\alpha|\lesssim 0.05$, i.e., to values very close to the
$\Lambda$CDM limit $\alpha =0$. This excludes, in particular, models in which
DE decays linearly with the Hubble rate.
",0,0
"  The interest in the extracellular vesicles (EVs) is rapidly growing as they
became reliable biomarkers for many diseases. For this reason, fast and
accurate techniques of EVs size characterization are the matter of utmost
importance. One increasingly popular technique is the Nanoparticle Tracking
Analysis (NTA), in which the diameters of EVs are calculated from their
diffusion constants. The crucial assumption here is that the diffusion in NTA
follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement
(MSD) of a particle grows linearly in time (MSD $\propto t$). However, we show
that NTA violates this assumption in both artificial and biological samples,
i.e. a large population of particles show a strongly sub-diffusive behaviour
(MSD $\propto t^\alpha$, $0<\alpha<1$). To support this observation we present
a range of experimental results for both polystyrene beads and EVs. This is
also related to another problem: for the same samples there exists a huge
discrepancy (by the factor of 2-4) between the sizes measured with NTA and with
the direct imaging methods, such as AFM. This can be remedied by e.g. the
Finite Track Length Adjustment (FTLA) method in NTA, but its applicability is
limited in the biological and poly-disperse samples. On the other hand, the
models of sub-diffusion rarely provide the direct relation between the size of
a particle and the generalized diffusion constant. However, we solve this last
problem by introducing the logarithmic model of sub-diffusion, aimed at
retrieving the size data. In result, we propose a novel protocol of NTA data
analysis. The accuracy of our method is on par with FTLA for small
($\simeq$200nm) particles. We apply our method to study the EVs samples and
corroborate the results with AFM.
",0,0
"  The processes of the averaged regression quantiles and of their modifications
provide useful tools in the regression models when the covariates are not fully
under our control. As an application we mention the probabilistic risk
assessment in the situation when the return depends on some exogenous
variables. The processes enable to evaluate the expected $\alpha$-shortfall
($0\leq\alpha\leq 1$) and other measures of the risk, recently generally
accepted in the financial literature, but also help to measure the risk in
environment analysis and elsewhere.
",0,0
"  We study primordial perturbations from hyperinflation, proposed recently and
based on a hyperbolic field-space. In the previous work, it was shown that the
field-space angular momentum supported by the negative curvature modifies the
background dynamics and enhances fluctuations of the scalar fields
qualitatively, assuming that the inflationary background is almost de Sitter.
In this work, we confirm and extend the analysis based on the standard approach
of cosmological perturbation in multi-field inflation. At the background level,
to quantify the deviation from de Sitter, we introduce the slow-varying
parameters and show that steep potentials, which usually can not drive
inflation, can drive inflation. At the linear perturbation level, we obtain the
power spectrum of primordial curvature perturbation and express the spectral
tilt and running in terms of the slow-varying parameters. We show that
hyperinflation with power-law type potentials has already been excluded by the
recent Planck observations, while exponential-type potential with the exponent
of order unity can be made consistent with observations as far as the power
spectrum is concerned. We also argue that, in the context of a simple $D$-brane
inflation, the hyperinflation requires exponentially large hyperbolic extra
dimensions but that masses of Kaluza-Klein gravitons can be kept relatively
heavy.
",0,0
"  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,
exhibits interesting semiconductor to metal transition in the temperature range
of 530-560 K. The metallic behavior originates because of the reduction of V2O5
through oxygen vacancies. In the present report, V2O5 nanorods in the
orthorhombic phase with crystal orientation of (001) are grown using vapor
transport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal
formula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the
formation of metallic phase above the transition temperature is established
from the temperature-dependent Raman spectroscopic studies. The origin of the
metallic behavior of V2O5 is also understood due to the breakdown of pdpi bond
between OI and nearest V atom instigated by the formation of vanadyl OI
vacancy, confirmed from the downward shift of the bottom most split-off
conduction bands in the material with increasing temperature.
",0,0
"  In this paper, we presented a novel convolutional neural network framework
for graph modeling, with the introduction of two new modules specially designed
for graph-structured data: the $k$-th order convolution operator and the
adaptive filtering module. Importantly, our framework of High-order and
Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed
architecture that fits various applications on both node and graph centrics, as
well as graph generative models. We conducted extensive experiments on
demonstrating the advantages of our framework. Particularly, our HA-GCN
outperforms the state-of-the-art models on node classification and molecule
property prediction tasks. It also generates 32% more real molecules on the
molecule generation task, both of which will significantly benefit real-world
applications such as material design and drug screening.
",1,1
"  A variety of representation learning approaches have been investigated for
reinforcement learning; much less attention, however, has been given to
investigating the utility of sparse coding. Outside of reinforcement learning,
sparse coding representations have been widely used, with non-convex objectives
that result in discriminative representations. In this work, we develop a
supervised sparse coding objective for policy evaluation. Despite the
non-convexity of this objective, we prove that all local minima are global
minima, making the approach amenable to simple optimization strategies. We
empirically show that it is key to use a supervised objective, rather than the
more straightforward unsupervised sparse coding approach. We compare the
learned representations to a canonical fixed sparse representation, called
tile-coding, demonstrating that the sparse coding representation outperforms a
wide variety of tilecoding representations.
",1,1
"  Motivated by Perelman's Pseudo Locality Theorem for the Ricci flow, we prove
that if a Riemannian manifold has Ricci curvature bounded below in a metric
ball which moreover has almost maximal volume, then in a smaller ball (in a
quantified sense) it holds an almost-euclidean isoperimetric inequality. The
result is actually established in the more general framework of non-smooth
spaces satisfying local Ricci curvature lower bounds in a synthetic sense via
optimal transportation.
",0,0
"  We bound an exponential sum that appears in the study of irregularities of
distribution (the low-frequency Fourier energy of the sum of several Dirac
measures) by geometric quantities: a special case is that for all $\left\{ x_1,
\dots, x_N\right\} \subset \mathbb{T}^2$, $X \geq 1$ and a universal $c>0$ $$
\sum_{i,j=1}^{N}{ \frac{X^2}{1 + X^4 \|x_i -x_j\|^4}} \lesssim \sum_{k \in
\mathbb{Z}^2 \atop \|k\| \leq X}{ \left| \sum_{n=1}^{N}{ e^{2 \pi i
\left\langle k, x_n \right\rangle}}\right|^2} \lesssim \sum_{i,j=1}^{N}{ X^2
e^{-c X^2\|x_i -x_j\|^2}}.$$ Since this exponential sum is intimately tied to
rather subtle distribution properties of the points, we obtain nonlocal
structural statements for near-minimizers of the Riesz-type energy. In the
regime $X \gtrsim N^{1/2}$ both upper and lower bound match for
maximally-separated point sets satisfying $\|x_i -x_j\| \gtrsim N^{-1/2}$.
",0,0
"  We investigate the effect of dimensional crossover in the ground state of the
antiferromagnetic spin-$1$ Heisenberg model on the anisotropic triangular
lattice that interpolates between the regime of weakly coupled Haldane chains
($J^{\prime}\! \!\ll\!\! J$) and the isotropic triangular lattice
($J^{\prime}\!\!=\!\!J$). We use the density-matrix renormalization group
(DMRG) and Schwinger boson theory performed at the Gaussian correction level
above the saddle-point solution. Our DMRG results show an abrupt transition
between decoupled spin chains and the spirally ordered regime at
$(J^{\prime}/J)_c\sim 0.42$, signaled by the sudden closing of the spin gap.
Coming from the magnetically ordered side, the computation of the spin
stiffness within Schwinger boson theory predicts the instability of the spiral
magnetic order toward a magnetically disordered phase with one-dimensional
features at $(J^{\prime}/J)_c \sim 0.43$. The agreement of these complementary
methods, along with the strong difference found between the intra- and the
interchain DMRG short spin-spin correlations; for sufficiently large values of
the interchain coupling, suggests that the interplay between the quantum
fluctuations and the dimensional crossover effects gives rise to the
one-dimensionalization phenomenon in this frustrated spin-$1$ Hamiltonian.
",0,0
"  Humans can learn in a continuous manner. Old rarely utilized knowledge can be
overwritten by new incoming information while important, frequently used
knowledge is prevented from being erased. In artificial learning systems,
lifelong learning so far has focused mainly on accumulating knowledge over
tasks and overcoming catastrophic forgetting. In this paper, we argue that,
given the limited model capacity and the unlimited new information to be
learned, knowledge has to be preserved or erased selectively. Inspired by
neuroplasticity, we propose a novel approach for lifelong learning, coined
Memory Aware Synapses (MAS). It computes the importance of the parameters of a
neural network in an unsupervised and online manner. Given a new sample which
is fed to the network, MAS accumulates an importance measure for each parameter
of the network, based on how sensitive the predicted output function is to a
change in this parameter. When learning a new task, changes to important
parameters can then be penalized, effectively preventing important knowledge
related to previous tasks from being overwritten. Further, we show an
interesting connection between a local version of our method and Hebb's
rule,which is a model for the learning process in the brain. We test our method
on a sequence of object recognition tasks and on the challenging problem of
learning an embedding for predicting $<$subject, predicate, object$>$ triplets.
We show state-of-the-art performance and, for the first time, the ability to
adapt the importance of the parameters based on unlabeled data towards what the
network needs (not) to forget, which may vary depending on test conditions.
",1,1
"  In this paper, we study the generalized polynomial chaos (gPC) based
stochastic Galerkin method for the linear semiconductor Boltzmann equation
under diffusive scaling and with random inputs from an anisotropic collision
kernel and the random initial condition. While the numerical scheme and the
proof of uniform-in-Knudsen-number regularity of the distribution function in
the random space has been introduced in [Jin-Liu-16'], the main goal of this
paper is to first obtain a sharper estimate on the regularity of the
solution-an exponential decay towards its local equilibrium, which then lead to
the uniform spectral convergence of the stochastic Galerkin method for the
problem under study.
",0,1
"  Over the last decade, wireless networks have experienced an impressive growth
and now play a main role in many telecommunications systems. As a consequence,
scarce radio resources, such as frequencies, became congested and the need for
effective and efficient assignment methods arose. In this work, we present a
Genetic Algorithm for solving large instances of the Power, Frequency and
Modulation Assignment Problem, arising in the design of wireless networks. To
our best knowledge, this is the first Genetic Algorithm that is proposed for
such problem. Compared to previous works, our approach allows a wider
exploration of the set of power solutions, while eliminating sources of
numerical problems. The performance of the algorithm is assessed by tests over
a set of large realistic instances of a Fixed WiMAX Network.
",1,1
"  We report on a combined study of the de Haas-van Alphen effect and angle
resolved photoemission spectroscopy on single crystals of the metallic
delafossite PdRhO$_2$ rounded off by \textit{ab initio} band structure
calculations. A high sensitivity torque magnetometry setup with SQUID readout
and synchrotron-based photoemission with a light spot size of
$~50\,\mu\mathrm{m}$ enabled high resolution data to be obtained from samples
as small as $150\times100\times20\,(\mu\mathrm{m})^3$. The Fermi surface shape
is nearly cylindrical with a rounded hexagonal cross section enclosing a
Luttinger volume of 1.00(1) electrons per formula unit.
",0,0
"  Atar, Chowdhary and Dupuis have recently exhibited a variational formula for
exponential integrals of bounded measurable functions in terms of Rényi
divergences. We develop a variational characterization of the Rényi
divergences between two probability distributions on a measurable sace in terms
of relative entropies. When combined with the elementary variational formula
for exponential integrals of bounded measurable functions in terms of relative
entropy, this yields the variational formula of Atar, Chowdhary and Dupuis as a
corollary. We also develop an analogous variational characterization of the
Rényi divergence rates between two stationary finite state Markov chains in
terms of relative entropy rates. When combined with Varadhan's variational
characterization of the spectral radius of square matrices with nonnegative
entries in terms of relative entropy, this yields an analog of the variational
formula of Atar, Chowdary and Dupuis in the framework of finite state Markov
chains.
",1,1
"  Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2
have attracted much attention recently, particularly because of their type II
band alignments and the formation of interlayer exciton as the lowest-energy
excitonic state. In this work, we calculate the electronic and optical
properties of such heterostructures with the first-principles GW+Bethe-Salpeter
Equation (BSE) method and reveal the important role of interlayer coupling in
deciding the excited-state properties, including the band alignment and
excitonic properties. Our calculation shows that due to the interlayer
coupling, the low energy excitons can be widely tunable by a vertical gate
field. In particular, the dipole oscillator strength and radiative lifetime of
the lowest energy exciton in these bilayer heterostructures is varied by over
an order of magnitude within a practical external gate field. We also build a
simple model that captures the essential physics behind this tunability and
allows the extension of the ab initio results to a large range of electric
fields. Our work clarifies the physical picture of interlayer excitons in
bilayer vdW heterostructures and predicts a wide range of gate-tunable
excited-state properties of 2D optoelectronic devices.
",0,0
"  We construct the algebraic cobordism theory of bundles and divisors on
varieties. It has a simple basis (over Q) from projective spaces and its rank
is equal to the number of Chern numbers. An application of this algebraic
cobordism theory is the enumeration of singular subvarieties with give tangent
conditions with a fixed smooth divisor, where the subvariety is the zero locus
of a section of a vector bundle. We prove that the generating series of numbers
of such subvarieties gives a homomorphism from the algebraic cobordism group to
the power series ring. This implies that the enumeration of singular
subvarieties with tangency conditions is governed by universal polynomials of
Chern numbers, when the vector bundle is sufficiently ample. This result
combines and generalizes the Caporaso-Harris recursive formula, Gottsche's
conjecture, classical De Jonquiere's Formula and node polynomials from tropical
geometry.
",0,0
"  People with profound motor deficits could perform useful physical tasks for
themselves by controlling robots that are comparable to the human body. Whether
this is possible without invasive interfaces has been unclear, due to the
robot's complexity and the person's limitations. We developed a novel,
augmented reality interface and conducted two studies to evaluate the extent to
which it enabled people with profound motor deficits to control robotic body
surrogates. 15 novice users achieved meaningful improvements on a clinical
manipulation assessment when controlling the robot in Atlanta from locations
across the United States. Also, one expert user performed 59 distinct tasks in
his own home over seven days, including self-care tasks such as feeding. Our
results demonstrate that people with profound motor deficits can effectively
control robotic body surrogates without invasive interfaces.
",1,1
"  Object detection in wide area motion imagery (WAMI) has drawn the attention
of the computer vision research community for a number of years. WAMI proposes
a number of unique challenges including extremely small object sizes, both
sparse and densely-packed objects, and extremely large search spaces (large
video frames). Nearly all state-of-the-art methods in WAMI object detection
report that appearance-based classifiers fail in this challenging data and
instead rely almost entirely on motion information in the form of background
subtraction or frame-differencing. In this work, we experimentally verify the
failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a
heatmap-based fully convolutional neural network (CNN), and propose a novel
two-stage spatio-temporal CNN which effectively and efficiently combines both
appearance and motion information to significantly surpass the state-of-the-art
in WAMI object detection. To reduce the large search space, the first stage
(ClusterNet) takes in a set of extremely large video frames, combines the
motion and appearance information within the convolutional architecture, and
proposes regions of objects of interest (ROOBI). These ROOBI can contain from
one to clusters of several hundred objects due to the large video frame size
and varying object density in WAMI. The second stage (FoveaNet) then estimates
the centroid location of all objects in that given ROOBI simultaneously via
heatmap estimation. The proposed method exceeds state-of-the-art results on the
WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped
objects, as well as being the first proposed method in wide area motion imagery
to detect completely stationary objects.
",1,1
"  Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
intelligence (e.g., the game of Go), is a well-known strategy for constructing
approximate solutions to sequential decision problems. Its primary innovation
is the use of a heuristic, known as a default policy, to obtain Monte Carlo
estimates of downstream values for states in a decision tree. This information
is used to iteratively expand the tree towards regions of states and actions
that an optimal policy might visit. However, to guarantee convergence to the
optimal action, MCTS requires the entire tree to be expanded asymptotically. In
this paper, we propose a new technique called Primal-Dual MCTS that utilizes
sampled information relaxation upper bounds on potential actions, creating the
possibility of ""ignoring"" parts of the tree that stem from highly suboptimal
choices. This allows us to prove that despite converging to a partial decision
tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
new approach shows significant promise when used to optimize the behavior of a
single driver navigating a graph while operating on a ride-sharing platform.
Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
trees and exhibits a reduced sensitivity to the size of the action space.
",1,1
"  We study the Fermi-edge singularity, describing the response of a degenerate
electron system to optical excitation, in the framework of the functional
renormalization group (fRG). Results for the (interband) particle-hole
susceptibility from various implementations of fRG (one- and two-
particle-irreducible, multi-channel Hubbard-Stratonovich, flowing
susceptibility) are compared to the summation of all leading logarithmic (log)
diagrams, achieved by a (first-order) solution of the parquet equations. For
the (zero-dimensional) special case of the X-ray-edge singularity, we show that
the leading log formula can be analytically reproduced in a consistent way from
a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic
structure, we show that this derivation relies on fortuitous partial
cancellations special to the form of and accuracy applied to the X-ray-edge
singularity and does not generalize.
",0,0
"  Retrosynthesis is a technique to plan the chemical synthesis of organic
molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a
search tree is built by analysing molecules recursively and dissecting them
into simpler molecular building blocks until one obtains a set of known
building blocks. The search space is intractably large, and it is difficult to
determine the value of retrosynthetic positions. Here, we propose to model
retrosynthesis as a Markov Decision Process. In combination with a Deep Neural
Network policy learned from essentially the complete published knowledge of
chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In
exploratory studies, we demonstrate that MCTS with neural network policies
outperforms the traditionally used best-first search with hand-coded
heuristics.
",1,1
"  The class of stochastically self-similar sets contains many famous examples
of random sets, e.g. Mandelbrot percolation and general fractal percolation.
Under the assumption of the uniform open set condition and some mild
assumptions on the iterated function systems used, we show that the
quasi-Assouad dimension of self-similar random recursive sets is almost surely
equal to the almost sure Hausdorff dimension of the set. We further comment on
random homogeneous and $V$-variable sets and the removal of overlap conditions.
",0,0
"  We report on the influence of spin-orbit coupling (SOC) in the Fe-based
superconductors (FeSCs) via application of circularly-polarized spin and
angle-resolved photoemission spectroscopy. We combine this technique in
representative members of both the Fe-pnictides and Fe-chalcogenides with ab
initio density functional theory and tight-binding calculations to establish an
ubiquitous modification of the electronic structure in these materials imbued
by SOC. The influence of SOC is found to be concentrated on the hole pockets
where the superconducting gap is generally found to be largest. This result
contests descriptions of superconductivity in these materials in terms of pure
spin-singlet eigenstates, raising questions regarding the possible pairing
mechanisms and role of SOC therein.
",0,0
"  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
",1,1
"  Gene regulatory networks are powerful abstractions of biological systems.
Since the advent of high-throughput measurement technologies in biology in the
late 90s, reconstructing the structure of such networks has been a central
computational problem in systems biology. While the problem is certainly not
solved in its entirety, considerable progress has been made in the last two
decades, with mature tools now available. This chapter aims to provide an
introduction to the basic concepts underpinning network inference tools,
attempting a categorisation which highlights commonalities and relative
strengths. While the chapter is meant to be self-contained, the material
presented should provide a useful background to the later, more specialised
chapters of this book.
",0,1
"  Glaucoma is the second leading cause of blindness all over the world, with
approximately 60 million cases reported worldwide in 2010. If undiagnosed in
time, glaucoma causes irreversible damage to the optic nerve leading to
blindness. The optic nerve head examination, which involves measurement of
cup-to-disc ratio, is considered one of the most valuable methods of structural
diagnosis of the disease. Estimation of cup-to-disc ratio requires segmentation
of optic disc and optic cup on eye fundus images and can be performed by modern
computer vision algorithms. This work presents universal approach for automatic
optic disc and cup segmentation, which is based on deep learning, namely,
modification of U-Net convolutional neural network. Our experiments include
comparison with the best known methods on publicly available databases
DRIONS-DB, RIM-ONE v.3, DRISHTI-GS. For both optic disc and cup segmentation,
our method achieves quality comparable to current state-of-the-art methods,
outperforming them in terms of the prediction time.
",1,1
"  The life of the modern world essentially depends on the work of the large
artificial homogeneous networks, such as wired and wireless communication
systems, networks of roads and pipelines. The support of their effective
continuous functioning requires automatic screening and permanent optimization
with processing of the huge amount of data by high-performance distributed
systems. We propose new meta-algorithm of large homogeneous network analysis,
its decomposition into alternative sets of loosely connected subnets, and
parallel optimization of the most independent elements. This algorithm is based
on a network-specific correlation function, Simulated Annealing technique, and
is adapted to work in the computer cluster. On the example of large wireless
network, we show that proposed algorithm essentially increases speed of
parallel optimization. The elaborated general approach can be used for analysis
and optimization of the wide range of networks, including such specific types
as artificial neural networks or organized in networks physiological systems of
living organisms.
",1,1
"  This paper considers the actor-critic contextual bandit for the mobile health
(mHealth) intervention. The state-of-the-art decision-making methods in mHealth
generally assume that the noise in the dynamic system follows the Gaussian
distribution. Those methods use the least-square-based algorithm to estimate
the expected reward, which is prone to the existence of outliers. To deal with
the issue of outliers, we propose a novel robust actor-critic contextual bandit
method for the mHealth intervention. In the critic updating, the
capped-$\ell_{2}$ norm is used to measure the approximation error, which
prevents outliers from dominating our objective. A set of weights could be
achieved from the critic updating. Considering them gives a weighted objective
for the actor updating. It provides the badly noised sample in the critic
updating with zero weights for the actor updating. As a result, the robustness
of both actor-critic updating is enhanced. There is a key parameter in the
capped-$\ell_{2}$ norm. We provide a reliable method to properly set it by
making use of one of the most fundamental definitions of outliers in
statistics. Extensive experiment results demonstrate that our method can
achieve almost identical results compared with the state-of-the-art methods on
the dataset without outliers and dramatically outperform them on the datasets
noised by outliers.
",1,1
"  In 1933 Kolmogorov constructed a general theory that defines the modern
concept of conditional expectation. In 1955 Renyi fomulated a new axiomatic
theory for probability motivated by the need to include unbounded measures. We
introduce a general concept of conditional expectation in Renyi spaces. In this
theory improper priors are allowed, and the resulting posterior can also be
improper.
In 1965 Lindley published his classic text on Bayesian statistics using the
theory of Renyi, but retracted this idea in 1973 due to the appearance of
marginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes
are investigated, and the seemingly conflicting results are explained. The
theory of Renyi can hence be used as an axiomatic basis for statistics that
allows use of unbounded priors.
Keywords: Haldane's prior; Poisson intensity; Marginalization paradox;
Measure theory; conditional probability space; axioms for statistics;
conditioning on a sigma field; improper prior
",0,0
"  Recently a new fault tolerant and simple mechanism was designed for solving
commit consensus problem. It is based on replicated validation of messages sent
between transaction participants and a special dispatcher validator manager
node. This paper presents a correctness, safety proofs and performance analysis
of this algorithm.
",1,1
"  This work presents a new method to quantify connectivity in transportation
networks. Inspired by the field of topological data analysis, we propose a
novel approach to explore the robustness of road network connectivity in the
presence of congestion on the roadway. The robustness of the pattern is
summarized in a congestion barcode, which can be constructed directly from
traffic datasets commonly used for navigation. As an initial demonstration, we
illustrate the main technique on a publicly available traffic dataset in a
neighborhood in New York City.
",1,0
"  The first transiting planetesimal orbiting a white dwarf was recently
detected in K2 data of WD1145+017 and has been followed up intensively. The
multiple, long, and variable transits suggest the transiting objects are dust
clouds, probably produced by a disintegrating asteroid. In addition, the system
contains circumstellar gas, evident by broad absorption lines, mostly in the
u'-band, and a dust disc, indicated by an infrared excess. Here we present the
first detection of a change in colour of WD1145+017 during transits, using
simultaneous multi-band fast-photometry ULTRACAM measurements over the
u'g'r'i'-bands. The observations reveal what appears to be 'bluing' during
transits; transits are deeper in the redder bands, with a u'-r' colour
difference of up to ~-0.05 mag. We explore various possible explanations for
the bluing. 'Spectral' photometry obtained by integrating over bandpasses in
the spectroscopic data in- and out-of-transit, compared to the photometric
data, shows that the observed colour difference is most likely the result of
reduced circumstellar absorption in the spectrum during transits. This
indicates that the transiting objects and the gas share the same line-of-sight,
and that the gas covers the white dwarf only partially, as would be expected if
the gas, the transiting debris, and the dust emitting the infrared excess, are
part of the same general disc structure (although possibly at different radii).
In addition, we present the results of a week-long monitoring campaign of the
system.
",0,0
"  In this review article, we discuss recent studies on drops and bubbles in
Hele-Shaw cells, focusing on how scaling laws exhibit crossovers from the
three-dimensional counterparts and focusing on topics in which viscosity plays
an important role. By virtue of progresses in analytical theory and high-speed
imaging, dynamics of drops and bubbles have actively been studied with the aid
of scaling arguments. However, compared with three dimensional problems,
studies on the corresponding problems in Hele-Shaw cells are still limited.
This review demonstrates that the effect of confinement in the Hele-Shaw cell
introduces new physics allowing different scaling regimes to appear. For this
purpose, we discuss various examples that are potentially important for
industrial applications handling drops and bubbles in confined spaces by
showing agreement between experiments and scaling theories. As a result, this
review provides a collection of problems in hydrodynamics that may be
analytically solved or that may be worth studying numerically in the near
future.
",0,0
"  Stacking-based deep neural network (S-DNN), in general, denotes a deep neural
network (DNN) resemblance in terms of its very deep, feedforward network
architecture. The typical S-DNN aggregates a variable number of individually
learnable modules in series to assemble a DNN-alike alternative to the targeted
object recognition tasks. This work likewise devises an S-DNN instantiation,
dubbed deep analytic network (DAN), on top of the spectral histogram (SH)
features. The DAN learning principle relies on ridge regression, and some key
DNN constituents, specifically, rectified linear unit, fine-tuning, and
normalization. The DAN aptitude is scrutinized on three repositories of varying
domains, including FERET (faces), MNIST (handwritten digits), and CIFAR10
(natural objects). The empirical results unveil that DAN escalates the SH
baseline performance over a sufficiently deep layer.
",1,1
"  In spite of Anderson's theorem, disorder is known to affect superconductivity
in conventional s-wave superconductors. In most superconductors, the degree of
disorder is fixed during sample preparation. Here we report measurements of the
superconducting properties of the two-dimensional gas that forms at the
interface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal
orientation, a system that permits \emph{in situ} tuning of carrier density and
disorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO
interface, superconductivity at the (111) LAO/STO interface can be tuned by
$V_g$. In contrast to the (001) interface, superconductivity in these (111)
samples is anisotropic, being different along different interface crystal
directions, consistent with the strong anisotropy already observed other
transport properties at the (111) LAO/STO interface. In addition, we find that
the (111) interface samples ""remember"" the backgate voltage $V_F$ at which they
are cooled at temperatures near the superconducting transition temperature
$T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low
energy scale and other characteristics of this memory effect ($<1$ K)
distinguish it from charge-trapping effects previously observed in (001)
interface samples.
",0,0
"  We investigate beam loading and emittance preservation for a high-charge
electron beam being accelerated in quasi-linear plasma wakefields driven by a
short proton beam. The structure of the studied wakefields are similar to those
of a long, modulated proton beam, such as the AWAKE proton driver. We show that
by properly choosing the electron beam parameters and exploiting two well known
effects, beam loading of the wakefield and full blow out of plasma electrons by
the accelerated beam, the electron beam can gain large amounts of energy with a
narrow final energy spread (%-level) and without significant emittance growth.
",0,0
"  In this paper, we propose a practical receiver for multicarrier signals
subjected to a strong memoryless nonlinearity. The receiver design is based on
a generalized approximate message passing (GAMP) framework, and this allows
real-time algorithm implementation in software or hardware with moderate
complexity. We demonstrate that the proposed receiver can provide more than a
2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range
$10^{-4}\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to
clipping nonlinearity and the crest-factor of the clipped waveform is only
1.9dB. Simulation results also demonstrate that the proposed receiver provides
significant performance gain in frequency-selective multipath channels
",1,1
"  According to astrophysical observations value of recession velocity in a
certain point is proportional to a distance to this point. The proportionality
coefficient is the Hubble constant measured with 5% accuracy. It is used in
many cosmological theories describing dark energy, dark matter, baryons, and
their relation with the cosmological constant introduced by Einstein.
In the present work we have determined a limit value of the global Hubble
constant (in a big distance from a point of observations) theoretically without
using any empirical constants on the base of our own fractal model used for the
description a relation between distance to an observed galaxy and coordinate of
its center. The distance has been defined as a nonlinear fractal measure with
scale of measurement corresponding to a deviation of the measure from its fixed
value (zero-gravity radius). We have suggested a model of specific anisotropic
fractal for simulation a radial Universe expansion. Our theoretical results
have shown existence of an inverse proportionality between accuracy of
determination the Hubble constant and accuracy of calculation a coordinates of
galaxies leading to ambiguity results obtained at cosmological observations.
",0,0
"  Dynamic languages often employ reflection primitives to turn dynamically
generated text into executable code at run-time. These features make standard
static analysis extremely hard if not impossible because its essential data
structures, i.e., the control-flow graph and the system of recursive equations
associated with the program to analyse, are themselves dynamically mutating
objects. We introduce SEA, an abstract interpreter for automatic sound string
executability analysis of dynamic languages employing bounded (i.e, finitely
nested) reflection and dynamic code generation. Strings are statically
approximated in an abstract domain of finite state automata with basic
operations implemented as symbolic transducers. SEA combines standard program
analysis together with string executability analysis. The analysis of a call to
reflection determines a call to the same abstract interpreter over a code which
is synthesised directly from the result of the static string executability
analysis at that program point. The use of regular languages for approximating
dynamically generated code structures allows SEA to soundly approximate safety
properties of self modifying programs yet maintaining efficiency. Soundness
here means that the semantics of the code synthesised by the analyser to
resolve reflection over-approximates the semantics of the code dynamically
built at run-rime by the program at that point.
",1,1
"  Reductions for transition systems have been recently introduced as a uniform
and principled method for comparing the expressiveness of system models with
respect to a range of properties, especially bisimulations. In this paper we
study the expressiveness (w.r.t. bisimulations) of models for quantitative
computations such as weighted labelled transition systems (WLTSs), uniform
labelled transition systems (ULTraSs), and state-to-function transition systems
(FuTSs). We prove that there is a trade-off between labels and weights: at one
extreme lays the class of (unlabelled) weighted transition systems where
information is presented using weights only; at the other lays the class of
labelled transition systems (LTSs) where information is shifted on labels.
These categories of systems cannot be further reduced in any significant way
and subsume all the aforementioned models.
",1,1
"  Poynting's theorem is used to obtain an expression for the turbulent
power-spectral density as function of frequency and wavenumber in low-frequency
magnetic turbulence. No reference is made to Elsasser variables as is usually
done in magnetohydrodynamic turbulence mixing mechanical and electromagnetic
turbulence. We rather stay with an implicit form of the mechanical part of
turbulence as suggested by electromagnetic theory in arbitrary media. All of
mechanics and flows is included into a turbulent response function which by
appropriate observations can be determined from knowledge of the turbulent
fluctuation spectra. This approach is not guided by the wish of developing a
complete theory of turbulence. It aims on the identification of the response
function from observations as input into a theory which afterwards attempts its
interpretation. Combination of both the magnetic and electric power spectral
densities leads to a representation of the turbulent response function, i.e.
the turbulent conductivity spectrum $\sigma_{\omega k}$ as function of
frequency $\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to
electric power spectral densities in frequency space. This knowledge allows for
formally writing down a turbulent dispersion relation. Power law inertial range
spectra result in a power law turbulent conductivity spectrum. These can be
compared with observations in the solar wind. Keywords: MHD turbulence,
turbulent dispersion relation, turbulent response function, solar wind
turbulence
",0,0
"  Let M be a compact Riemannian manifold and let $\mu$,d be the associated
measure and distance on M. Robert McCann obtained, generalizing results for the
Euclidean case by Yann Brenier, the polar factorization of Borel maps S : M ->
M pushing forward $\mu$ to a measure $\nu$: each S factors uniquely a.e. into
the composition S = T \circ U, where U : M -> M is volume preserving and T : M
-> M is the optimal map transporting $\mu$ to $\nu$ with respect to the cost
function d^2/2.
In this article we study the polar factorization of conformal and projective
maps of the sphere S^n. For conformal maps, which may be identified with
elements of the identity component of O(1,n+1), we prove that the polar
factorization in the sense of optimal mass transport coincides with the
algebraic polar factorization (Cartan decomposition) of this Lie group. For the
projective case, where the group GL_+(n+1) is involved, we find necessary and
sufficient conditions for these two factorizations to agree.
",0,0
"  We examine the representation of numbers as the sum of two squares in
$\mathbb{Z}_n$ for a general positive integer $n$. Using this information we
make some comments about the density of positive integers which can be
represented as the sum of two squares and powers of $2$ in $\mathbb{N}$.
",0,0
"  Regression for spatially dependent outcomes poses many challenges, for
inference and for computation. Non-spatial models and traditional spatial
mixed-effects models each have their advantages and disadvantages, making it
difficult for practitioners to determine how to carry out a spatial regression
analysis. We discuss the data-generating mechanisms implicitly assumed by
various popular spatial regression models, and discuss the implications of
these assumptions. We propose Bayesian spatial filtering as an approximate
middle way between non-spatial models and traditional spatial mixed models. We
show by simulation that our Bayesian spatial filtering model has several
desirable properties and hence may be a useful addition to a spatial
statistician's toolkit.
",0,0
"  One of the most important parameters in ionospheric plasma research also
having a wide practical application in wireless satellite telecommunications is
the total electron content (TEC) representing the columnal electron number
density. The F region with high electron density provides the biggest
contribution to TEC while the relatively weakly ionized plasma of the D region
(60 km - 90 km above Earths surface) is often considered as a negligible cause
of satellite signal disturbances. However, sudden intensive ionization
processes like those induced by solar X ray flares can cause relative increases
of electron density that are significantly larger in the D-region than in
regions at higher altitudes. Therefore, one cannot exclude a priori the D
region from investigations of ionospheric influences on propagation of
electromagnetic signals emitted by satellites. We discuss here this problem
which has not been sufficiently treated in literature so far. The obtained
results are based on data collected from the D region monitoring by very low
frequency radio waves and on vertical TEC calculations from the Global
Navigation Satellite System (GNSS) signal analyses, and they show noticeable
variations in the D region electron content (TECD) during activity of a solar X
ray flare (it rises by a factor of 136 in the considered case) when TECD
contribution to TEC can reach several percent and which cannot be neglected in
practical applications like global positioning procedures by satellites.
",0,0
"  For the particles undergoing the anomalous diffusion with different waiting
time distributions for different internal states, we derive the Fokker-Planck
and Feymann-Kac equations, respectively, describing positions of the particles
and functional distributions of the trajectories of particles; in particular,
the equations governing the functional distribution of internal states are also
obtained. The dynamics of the stochastic processes are analyzed and the
applications, calculating the distribution of the first passage time and the
distribution of the fraction of the occupation time, of the equations are
given.
",0,0
"  Stabilizing the magnetic signal of single adatoms is a crucial step towards
their successful usage in widespread technological applications such as
high-density magnetic data storage devices. The quantum mechanical nature of
these tiny objects, however, introduces intrinsic zero-point spin-fluctuations
that tend to destabilize the local magnetic moment of interest by dwindling the
magnetic anisotropy potential barrier even at absolute zero temperature. Here,
we elucidate the origins and quantify the effect of the fundamental ingredients
determining the magnitude of the fluctuations, namely the ($i$) local magnetic
moment, ($ii$) spin-orbit coupling and ($iii$) electron-hole Stoner
excitations. Based on a systematic first-principles study of 3d and 4d adatoms,
we demonstrate that the transverse contribution of the fluctuations is
comparable in size to the magnetic moment itself, leading to a remarkable
$\gtrsim$50$\%$ reduction of the magnetic anisotropy energy. Our analysis gives
rise to a comprehensible diagram relating the fluctuation magnitude to
characteristic features of adatoms, providing practical guidelines for
designing magnetically stable nanomagnets with minimal quantum fluctuations.
",0,0
"  We study a minimal model for the growth of a phenotypically heterogeneous
population of cells subject to a fluctuating environment in which they can
replicate (by exploiting available resources) and modify their phenotype within
a given landscape (thereby exploring novel configurations). The model displays
an exploration-exploitation trade-off whose specifics depend on the statistics
of the environment. Most notably, the phenotypic distribution corresponding to
maximum population fitness (i.e. growth rate) requires a non-zero exploration
rate when the magnitude of environmental fluctuations changes randomly over
time, while a purely exploitative strategy turns out to be optimal in two-state
environments, independently of the statistics of switching times. We obtain
analytical insight into the limiting cases of very fast and very slow
exploration rates by directly linking population growth to the features of the
environment.
",0,0
"  Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.
",1,1
"  Mission critical data dissemination in massive Internet of things (IoT)
networks imposes constraints on the message transfer delay between devices. Due
to low power and communication range of IoT devices, data is foreseen to be
relayed over multiple device-to-device (D2D) links before reaching the
destination. The coexistence of a massive number of IoT devices poses a
challenge in maximizing the successful transmission capacity of the overall
network alongside reducing the multi-hop transmission delay in order to support
mission critical applications. There is a delicate interplay between the
carrier sensing threshold of the contention based medium access protocol and
the choice of packet forwarding strategy selected at each hop by the devices.
The fundamental problem in optimizing the performance of such networks is to
balance the tradeoff between conflicting performance objectives such as the
spatial frequency reuse, transmission quality, and packet progress towards the
destination. In this paper, we use a stochastic geometry approach to quantify
the performance of multi-hop massive IoT networks in terms of the spatial
frequency reuse and the transmission quality under different packet forwarding
schemes. We also develop a comprehensive performance metric that can be used to
optimize the system to achieve the best performance. The results can be used to
select the best forwarding scheme and tune the carrier sensing threshold to
optimize the performance of the network according to the delay constraints and
transmission quality requirements.
",1,1
"  We develope a two-species exclusion process with a distinct pair of entry and
exit sites for each species of rigid rods. The relatively slower forward
stepping of the rods in an extended bottleneck region, located in between the
two entry sites, controls the extent of interference of the co-directional flow
of the two species of rods. The relative positions of the sites of entry of the
two species of rods with respect to the location of the bottleneck are
motivated by a biological phenomenon. However, the primary focus of the study
here is to explore the effects of the interference of the flow of the two
species of rods on their spatio-temporal organization and the regulations of
this interference by the extended bottleneck. By a combination of mean-field
theory and computer simulation we calculate the flux of both species of rods
and their density profiles as well as the composite phase diagrams of the
system. If the bottleneck is sufficiently stringent some of the phases become
practically unrealizable although not ruled out on the basis of any fundamental
physical principle. Moreover the extent of suppression of flow of the
downstream entrants by the flow of the upstream entrants can also be regulated
by the strength of the bottleneck. We speculate on the possible implications of
the results in the context of the biological phenomenon that motivated the
formulation of the theoretical model.
",0,0
"  We introduce a large class of random Young diagrams which can be regarded as
a natural one-parameter deformation of some classical Young diagram ensembles;
a deformation which is related to Jack polynomials and Jack characters. We show
that each such a random Young diagram converges asymptotically to some limit
shape and that the fluctuations around the limit are asymptotically Gaussian.
",0,0
"  We explicitly compute the critical exponents associated with logarithmic
corrections (the so-called hatted exponents) starting from the renormalization
group equations and the mean field behavior for a wide class of models at the
upper critical behavior (for short and long range $\phi^n$-theories) and below
it. This allows us to check the scaling relations among these critical
exponents obtained by analysing the complex singularities (Lee-Yang and Fisher
zeroes) of these models. Moreover, we have obtained an explicit method to
compute the $\hat{\coppa}$ exponent [defined by $\xi\sim L (\log
L)^{\hat{\coppa}}$] and, finally, we have found a new derivation of the scaling
law associated with it.
",0,0
"  We obtain a Bernstein-type inequality for sums of Banach-valued random
variables satisfying a weak dependence assumption of general type and under
certain smoothness assumptions of the underlying Banach norm. We use this
inequality in order to investigate in the asymptotical regime the error upper
bounds for the broad family of spectral regularization methods for reproducing
kernel decision rules, when trained on a sample coming from a $\tau-$mixing
process.
",0,1
"  The temperature-dependent evolution of the Kondo lattice is a long-standing
topic of theoretical and experimental investigation and yet it lacks a truly
microscopic description of the relation of the basic $f$-$d$ hybridization
processes to the fundamental temperature scales of Kondo screening and
Fermi-liquid lattice coherence. Here, the temperature-dependence of $f$-$d$
hybridized band dispersions and Fermi-energy $f$ spectral weight in the Kondo
lattice system CeCoIn$_5$ is investigated using $f$-resonant angle-resolved
photoemission (ARPES) with sufficient detail to allow direct comparison to
first principles dynamical mean field theory (DMFT) calculations containing
full realism of crystalline electric field states. The ARPES results, for two
orthogonal (001) and (100) cleaved surfaces and three different $f$-$d$
hybridization scenarios, with additional microscopic insight provided by DMFT,
reveal $f$ participation in the Fermi surface at temperatures much higher than
the lattice coherence temperature, $T^*\approx$ 45 K, commonly believed to be
the onset for such behavior. The identification of a $T$-dependent crystalline
electric field degeneracy crossover in the DMFT theory $below$ $T^*$ is
specifically highlighted.
",0,0
"  Hegarty conjectured for $n\neq 2, 3, 5, 7$ that $\mathbb{Z}/n\mathbb{Z}$ has
a permutation which destroys all arithmetic progressions mod $n$. For $n\ge
n_0$, Hegarty and Martinsson demonstrated that $\mathbb{Z}/n\mathbb{Z}$ has an
arithmetic-progression destroying permutation. However $n_0\approx 1.4\times
10^{14}$ and thus resolving the conjecture in full remained out of reach of any
computational techniques. However, this paper using constructions modeled after
those used by Elkies and Swaminathan for the case of $\mathbb{Z}/p\mathbb{Z}$
with $p$ being prime, establish the conjecture in full. Furthermore our results
do not rely on the fact that it suffices to study when $n<n_0$ and thus our
results completely independent of the proof given by Hegarty and Martinsson.
",0,0
"  An immersion $f : {\mathcal D} \rightarrow \mathcal C$ between cell complexes
is a local homeomorphism onto its image that commutes with the characteristic
maps of the cell complexes. We study immersions between finite-dimensional
connected $\Delta$-complexes by replacing the fundamental group of the base
space by an appropriate inverse monoid. We show how conjugacy classes of the
closed inverse submonoids of this inverse monoid may be used to classify
connected immersions into the complex. This extends earlier results of Margolis
and Meakin for immersions between graphs and of Meakin and Szakács on
immersions into $2$-dimensional $CW$-complexes.
",0,0
"  Resolving the relationship between biodiversity and ecosystem functioning has
been one of the central goals of modern ecology. Early debates about the
relationship were finally resolved with the advent of a statistical
partitioning scheme that decomposed the biodiversity effect into a ""selection""
effect and a ""complementarity"" effect. We prove that both the biodiversity
effect and its statistical decomposition into selection and complementarity are
fundamentally flawed because these methods use a naïve null expectation based
on neutrality, likely leading to an overestimate of the net biodiversity
effect, and they fail to account for the nonlinear abundance-ecosystem
functioning relationships observed in nature. Furthermore, under such
nonlinearity no statistical scheme can be devised to partition the biodiversity
effects. We also present an alternative metric providing a more reasonable
estimate of biodiversity effect. Our results suggest that all studies conducted
since the early 1990s likely overestimated the positive effects of biodiversity
on ecosystem functioning.
",0,0
"  The principle of democracy is that the people govern through elected
representatives. Therefore, a democracy is healthy as long as the elected
politicians do represent the people. We have analyzed data from the Brazilian
electoral court (Tribunal Superior Eleitoral, TSE) concerning money donations
for the electoral campaigns and the election results. Our work points to two
disturbing conclusions: money is a determining factor on whether a candidate is
elected or not (as opposed to representativeness); secondly, the use of
Benford's Law to analyze the declared donations received by the parties and
electoral campaigns shows evidence of fraud in the declarations. A better term
to define Brazil's government system is what we define as chrimatocracy (govern
by money).
",1,0
"  With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society.
",1,1
"  We rework and generalize equivariant infinite loop space theory, which shows
how to construct G-spectra from G-spaces with suitable structure. There is a
naive version which gives naive G-spectra for any topological group G, but our
focus is on the construction of genuine G-spectra when G is finite.
We give new information about the Segal and operadic equivariant infinite
loop space machines, supplying many details that are missing from the
literature, and we prove by direct comparison that the two machines give
equivalent output when fed equivalent input. The proof of the corresponding
nonequivariant uniqueness theorem, due to May and Thomason, works for naive
G-spectra for general G but fails hopelessly for genuine G-spectra when G is
finite. Even in the nonequivariant case, our comparison theorem is considerably
more precise, giving a direct point-set level comparison.
We have taken the opportunity to update this general area, equivariant and
nonequivariant, giving many new proofs, filling in some gaps, and giving some
corrections to results in the literature.
",0,0
"  We prove that any open subset $U$ of a semi-simple simply connected
quasi-split linear algebraic group $G$ with ${codim} (G\setminus U, G)\geq 2$
over a number field satisfies strong approximation by establishing a fibration
of $G$ over a toric variety. We also prove a similar result of strong
approximation with Brauer-Manin obstruction for a partial equivariant smooth
compactification of a homogeneous space where all invertible functions are
constant and the semi-simple part of the linear algebraic group is quasi-split.
Some semi-abelian varieties of any given dimension where the complements of a
rational point do not satisfy strong approximation with Brauer-Manin
obstruction are given.
",0,0
"  We show that nonlocal minimal cones which are non-singular subgraphs outside
the origin are necessarily halfspaces.
The proof is based on classical ideas of~\cite{DG1} and on the computation of
the linearized nonlocal mean curvature operator, which is proved to satisfy a
suitable maximum principle.
With this, we obtain new, and somehow simpler, proofs of the Bernstein-type
results for nonlocal minimal surfaces which have been recently established
in~\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type
result which classifies Lipschitz nonlocal minimal subgraphs outside a ball.
",0,0
"  Let $f_1,\ldots,f_k : \mathbb{N} \rightarrow \mathbb{C}$ be multiplicative
functions taking values in the closed unit disc. Using an analytic approach in
the spirit of Halász' mean value theorem, we compute multidimensional
averages of the shape $$x^{-l} \sum_{\mathbf{n} \in [x]^l} \prod_{1 \leq j \leq
k} f_j(L_j(\mathbf{n}))$$ as $x \rightarrow \infty$, where $[x] := [1,x]$ and
$L_1,\ldots, L_k$ are affine linear forms that satisfy some natural conditions.
Our approach gives a new proof of a result of Frantzikinakis and Host that is
distinct from theirs, with \emph{explicit} main and error terms. \\ As an
application of our formulae, we establish a \emph{local-to-global} principle
for Gowers norms of multiplicative functions. We also compute the asymptotic
densities of the sets of integers $n$ such that a given multiplicative function
$f: \mathbb{N} \rightarrow \{-1, 1\}$ yields a fixed sign pattern of length 3
or 4 on almost all 3- and 4-term arithmetic progressions, respectively, with
first term $n$.
",0,0
"  The apparent gas permeability of the porous medium is an important parameter
in the prediction of unconventional gas production, which was first
investigated systematically by Klinkenberg in 1941 and found to increase with
the reciprocal mean gas pressure (or equivalently, the Knudsen number).
Although the underlying rarefaction effects are well-known, the reason that the
correction factor in Klinkenberg's famous equation decreases when the Knudsen
number increases has not been fully understood. Most of the studies idealize
the porous medium as a bundle of straight cylindrical tubes, however, according
to the gas kinetic theory, this only results in an increase of the correction
factor with the Knudsen number, which clearly contradicts Klinkenberg's
experimental observations. Here, by solving the Bhatnagar-Gross-Krook equation
in simplified (but not simple) porous media, we identify, for the first time,
two key factors that can explain Klinkenberg's experimental results: the
tortuous flow path and the non-unitary tangential momentum accommodation
coefficient for the gas-surface interaction. Moreover, we find that
Klinkenberg's results can only be observed when the ratio between the apparent
and intrinsic permeabilities is $\lesssim30$; at large ratios (or Knudsen
numbers) the correction factor increases with the Knudsen number. Our numerical
results could also serve as benchmarking cases to assess the accuracy of
macroscopic models and/or numerical schemes for the modeling/simulation of
rarefied gas flows in complex geometries over a wide range of gas rarefaction.
",0,0
"  In previous papers, threshold probabilities for the properties of a random
distance graph to contain strictly balanced graphs were found. We extend this
result to arbitrary graphs and prove that the number of copies of a strictly
balanced graph has asymptotically Poisson distribution at the threshold.
",0,1
"  Runtime enforcement can be effectively used to improve the reliability of
software applications. However, it often requires the definition of ad hoc
policies and enforcement strategies, which might be expensive to identify and
implement. This paper discusses how to exploit lifecycle events to obtain
useful enforcement strategies that can be easily reused across applications,
thus reducing the cost of adoption of the runtime enforcement technology. The
paper finally sketches how this idea can be used to define libraries that can
automatically overcome problems related to applications misusing them.
",1,1
"  The atomic norm provides a generalization of the $\ell_1$-norm to continuous
parameter spaces. When applied as a sparse regularizer for line spectral
estimation the solution can be obtained by solving a convex optimization
problem. This problem is known as atomic norm soft thresholding (AST). It can
be cast as a semidefinite program and solved by standard methods. In the
semidefinite formulation there are $O(N^2)$ dual variables and a standard
primal-dual interior point method requires at least $O(N^6)$ flops per
iteration. That has lead researcher to consider alternating direction method of
multipliers (ADMM) for the solution of AST, but this method is still somewhat
slow for large problem sizes. To obtain a faster algorithm we reformulate AST
as a non-symmetric conic program. That has two properties of key importance to
its numerical solution: the conic formulation has only $O(N)$ dual variables
and the Toeplitz structure inherent to AST is preserved. Based on it we derive
FastAST which is a primal-dual interior point method for solving AST. Two
variants are considered with the fastest one requiring only $O(N^2)$ flops per
iteration. Extensive numerical experiments demonstrate that FastAST solves AST
significantly faster than a state-of-the-art solver based on ADMM.
",1,1
"  We study the problem of causal structure learning over a set of random
variables when the experimenter is allowed to perform at most $M$ experiments
in a non-adaptive manner. We consider the optimal learning strategy in terms of
minimizing the portions of the structure that remains unknown given the limited
number of experiments in both Bayesian and minimax setting. We characterize the
theoretical optimal solution and propose an algorithm, which designs the
experiments efficiently in terms of time complexity. We show that for bounded
degree graphs, in the minimax case and in the Bayesian case with uniform
priors, our proposed algorithm is a $\rho$-approximation algorithm, where
$\rho$ is independent of the order of the underlying graph. Simulations on both
synthetic and real data show that the performance of our algorithm is very
close to the optimal solution.
",1,1
"  We present a novel data-driven nested optimization framework that addresses
the problem of coupling between plant and controller optimization. This
optimization strategy is tailored towards instances where a closed-form
expression for the system dynamic response is unobtainable and simulations or
experiments are necessary. Specifically, Bayesian Optimization, which is a
data-driven technique for finding the optimum of an unknown and
expensive-to-evaluate objective function, is employed to solve a nested
optimization problem. The underlying objective function is modeled by a
Gaussian Process (GP); then, Bayesian Optimization utilizes the predictive
uncertainty information from the GP to determine the best subsequent control or
plant parameters. The proposed framework differs from the majority of co-design
literature where there exists a closed-form model of the system dynamics.
Furthermore, we utilize the idea of Batch Bayesian Optimization at the plant
optimization level to generate a set of plant designs at each iteration of the
overall optimization process, recognizing that there will exist economies of
scale in running multiple experiments in each iteration of the plant design
process. We validate the proposed framework for a Buoyant Airborne Turbine
(BAT). We choose the horizontal stabilizer area, longitudinal center of mass
relative to center of buoyancy (plant parameters), and the pitch angle
set-point (controller parameter) as our decision variables. Our results
demonstrate that these plant and control parameters converge to their
respective optimal values within only a few iterations.
",1,1
"  We explore the topological properties of quantum spin-1/2 chains with two
Ising symmetries. This class of models does not possess any of the symmetries
that are required to protect the Haldane phase. Nevertheless, we show that
there are 4 symmetry-protected topological phases, in addition to 6 phases that
spontaneously break one or both Ising symmetries. By mapping the model to
one-dimensional interacting fermions with particle-hole and time-reversal
symmetry, we obtain integrable parent Hamiltonians for the conventional and
topological phases of the spin model. We use these Hamiltonians to characterize
the physical properties of all 10 phases, identify their local and nonlocal
order parameters, and understand the effects of weak perturbations that respect
the Ising symmetries. Our study provides the first explicit example of a class
of spin chains with several topologically non-trivial phases, and binds
together the topological classifications of interacting bosons and fermions.
",0,0
"  Most of the codes that have an algebraic decoding algorithm are derived from
the Reed Solomon codes. They are obtained by taking equivalent codes, for
example the generalized Reed Solomon codes, or by using the so-called subfield
subcode method, which leads to Alternant codes and Goppa codes over the
underlying prime field, or over some intermediate subfield. The main advantages
of these constructions is to preserve both the minimum distance and the
decoding algorithm of the underlying Reed Solomon code. In this paper, we
propose a generalization of the subfield subcode construction by introducing
the notion of subspace subcodes and a generalization of the equivalence of
codes which leads to the notion of generalized subspace subcodes. When the
dimension of the selected subspaces is equal to one, we show that our approach
gives exactly the family of the codes obtained by equivalence and subfield
subcode technique. However, our approach highlights the links between the
subfield subcode of a code defined over an extension field and the operation of
puncturing the $q$-ary image of this code. When the dimension of the subspaces
is greater than one, we obtain codes whose alphabet is no longer a finite
field, but a set of r-uples. We explain why these codes are practically as
efficient for applications as the codes defined on an extension of degree r. In
addition, they make it possible to obtain decodable codes over a large alphabet
having parameters previously inaccessible. As an application, we give some
examples that can be used in public key cryptosystems such as McEliece.
",1,1
"  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of
Gelfand-Cetlin systems over complex partial flag manifolds, we provide a
complete description of the topology of Gelfand-Cetlin fibers. We prove that
all fibers are \emph{smooth} isotropic submanifolds and give a complete
description of the fiber to be Lagrangian in terms of combinatorics of
Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian
fibers. After a few combinatorial and numercal tests for the displaceability,
using the bulk-deformation of Floer cohomology by Schubert cycles, we prove
that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone
Kirillov-Kostant-Souriau symplectic form carries a continuum of
non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the
Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$
is non-displaceable the question of which was raised by Nohara-Ueda who
computed its Floer cohomology to be vanishing.
",0,0
"  Ensemble data assimilation methods such as the Ensemble Kalman Filter (EnKF)
are a key component of probabilistic weather forecasting. They represent the
uncertainty in the initial conditions by an ensemble which incorporates
information coming from the physical model with the latest observations.
High-resolution numerical weather prediction models ran at operational centers
are able to resolve non-linear and non-Gaussian physical phenomena such as
convection. There is therefore a growing need to develop ensemble assimilation
algorithms able to deal with non-Gaussianity while staying computationally
feasible. In the present paper we address some of these needs by proposing a
new hybrid algorithm based on the Ensemble Kalman Particle Filter. It is fully
formulated in ensemble space and uses a deterministic scheme such that it has
the ensemble transform Kalman filter (ETKF) instead of the stochastic EnKF as a
limiting case. A new criterion for choosing the proportion of particle filter
and ETKF update is also proposed. The new algorithm is implemented in the COSMO
framework and numerical experiments in a quasi-operational convective-scale
setup are conducted. The results show the feasibility of the new algorithm in
practice and indicate a strong potential for such local hybrid methods, in
particular for forecasting non-Gaussian variables such as wind and hourly
precipitation.
",0,1
"  In this paper, we consider the Tensor Robust Principal Component Analysis
(TRPCA) problem, which aims to exactly recover the low-rank and sparse
components from their sum. Our model is based on the recently proposed
tensor-tensor product (or t-product) [13]. Induced by the t-product, we first
rigorously deduce the tensor spectral norm, tensor nuclear norm, and tensor
average rank, and show that the tensor nuclear norm is the convex envelope of
the tensor average rank within the unit ball of the tensor spectral norm. These
definitions, their relationships and properties are consistent with matrix
cases. Equipped with the new tensor nuclear norm, we then solve the TRPCA
problem by solving a convex program and provide the theoretical guarantee for
the exact recovery. Our TRPCA model and recovery guarantee include matrix RPCA
as a special case. Numerical experiments verify our results, and the
applications to image recovery and background modeling problems demonstrate the
effectiveness of our method.
",0,1
"  Galaxies in the local Universe are known to follow bimodal distributions in
the global stellar populations properties. We analyze the distribution of the
local average stellar-population ages of 654,053 sub-galactic regions resolved
on ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the
CALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.
We find a bimodal local-age distribution, with an old and a young peak
primarily due to regions in early-type galaxies and star-forming regions of
spirals, respectively. Within spiral galaxies, the older ages of bulges and
inter-arm regions relative to spiral arms support an internal age bimodality.
Although regions of higher stellar-mass surface-density, mu*, are typically
older, mu* alone does not determine the stellar population age and a bimodal
distribution is found at any fixed mu*. We identify an ""old ridge"" of regions
of age ~9 Gyr, independent of mu*, and a ""young sequence"" of regions with age
increasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as
regions containing only old stars, and the latter as regions where the relative
contamination of old stellar populations by young stars decreases as mu*
increases. The reason why this bimodal age distribution is not inconsistent
with the unimodal shape of the cosmic-averaged star-formation history is that
i) the dominating contribution by young stars biases the age low with respect
to the average epoch of star formation, and ii) the use of a single average age
per region is unable to represent the full time-extent of the star-formation
history of ""young-sequence"" regions.
",0,0
"  We introduce a minimal model for the evolution of functional
protein-interaction networks using a sequence-based mutational algorithm, and
apply the model to study neutral drift in networks that yield oscillatory
dynamics. Starting with a functional core module, random evolutionary drift
increases network complexity even in the absence of specific selective
pressures. Surprisingly, we uncover a hidden order in sequence space that gives
rise to long-term evolutionary memory, implying strong constraints on network
evolution due to the topology of accessible sequence space.
",0,0
"  The handwritten string recognition is still a challengeable task, though the
powerful deep learning tools were introduced. In this paper, based on TAO-FCN,
we proposed an end-to-end system for handwritten string recognition. Compared
with the conventional methods, there is no preprocess nor manually designed
rules employed. With enough labelled data, it is easy to apply the proposed
method to different applications. Although the performance of the proposed
method may not be comparable with the state-of-the-art approaches, it's
usability and robustness are more meaningful for practical applications.
",1,1
"  We note that the necessary and sufficient conditions established by Marcel
Riesz for the inclusion of regular Nörlund summation methods are in fact
applicable quite generally.
",0,0
"  These lectures notes were written for a summer school on Mathematics for
post-quantum cryptography in Thiès, Senegal. They try to provide a guide for
Masters' students to get through the vast literature on elliptic curves,
without getting lost on their way to learning isogeny based cryptography. They
are by no means a reference text on the theory of elliptic curves, nor on
cryptography; students are encouraged to complement these notes with some of
the books recommended in the bibliography.
The presentation is divided in three parts, roughly corresponding to the
three lectures given. In an effort to keep the reader interested, each part
alternates between the fundamental theory of elliptic curves, and applications
in cryptography. We often prefer to have the main ideas flow smoothly, rather
than having a rigorous presentation as one would have in a more classical book.
The reader will excuse us for the inaccuracies and the omissions.
",1,0
"  It has been shown recently that changing the fluidic properties of a drug can
improve its efficacy in ablating solid tumors. We develop a modeling framework
for tumor ablation, and present the simplest possible model for drug diffusion
in a spherical tumor with leaky boundaries and assuming cell death eventually
leads to ablation of that cell effectively making the two quantities
numerically equivalent. The death of a cell after a given exposure time depends
on both the concentration of the drug and the amount of oxygen available to the
cell. Higher oxygen availability leads to cell death at lower drug
concentrations. It can be assumed that a minimum concentration is required for
a cell to die, effectively connecting diffusion with efficacy. The
concentration threshold decreases as exposure time increases, which allows us
to compute dose-response curves. Furthermore, these curves can be plotted at
much finer time intervals compared to that of experiments, which is used to
produce a dose-threshold-response surface giving an observer a complete picture
of the drug's efficacy for an individual. In addition, since the diffusion,
leak coefficients, and the availability of oxygen is different for different
individuals and tumors, we produce artificial replication data through
bootstrapping to simulate error. While the usual data-driven model with
Sigmoidal curves use 12 free parameters, our mechanistic model only has two
free parameters, allowing it to be open to scrutiny rather than forcing
agreement with data. Even so, the simplest model in our framework, derived
here, shows close agreement with the bootstrapped curves, and reproduces well
established relations, such as Haber's rule.
",0,0
"  To identify the estimand in missing data problems and observational studies,
it is common to base the statistical estimation on the ""missing at random"" and
""no unmeasured confounder"" assumptions. However, these assumptions are
unverifiable using empirical data and pose serious threats to the validity of
the qualitative conclusions of the statistical inference. A sensitivity
analysis asks how the conclusions may change if the unverifiable assumptions
are violated to a certain degree. In this paper we consider a marginal
sensitivity model which is a natural extension of Rosenbaum's sensitivity model
that is widely used for matched observational studies. We aim to construct
confidence intervals based on inverse probability weighting estimators, such
that asymptotically the intervals have at least nominal coverage of the
estimand whenever the data generating distribution is in the collection of
marginal sensitivity models. We use a percentile bootstrap and a generalized
minimax/maximin inequality to transform this intractable problem to a linear
fractional programming problem, which can be solved very efficiently. We
illustrate our method using a real dataset to estimate the causal effect of
fish consumption on blood mercury level.
",0,0
"  In this paper, we provide an analysis of self-organized network management,
with an end-to-end perspective of the network. Self-organization as applied to
cellular networks is usually referred to Self-organizing Networks (SONs), and
it is a key driver for improving Operations, Administration, and Maintenance
(OAM) activities. SON aims at reducing the cost of installation and management
of 4G and future 5G networks, by simplifying operational tasks through the
capability to configure, optimize and heal itself. To satisfy 5G network
management requirements, this autonomous management vision has to be extended
to the end to end network. In literature and also in some instances of products
available in the market, Machine Learning (ML) has been identified as the key
tool to implement autonomous adaptability and take advantage of experience when
making decisions. In this paper, we survey how network management can
significantly benefit from ML solutions. We review and provide the basic
concepts and taxonomy for SON, network management and ML. We analyse the
available state of the art in the literature, standardization, and in the
market. We pay special attention to 3rd Generation Partnership Project (3GPP)
evolution in the area of network management and to the data that can be
extracted from 3GPP networks, in order to gain knowledge and experience in how
the network is working, and improve network performance in a proactive way.
Finally, we go through the main challenges associated with this line of
research, in both 4G and in what 5G is getting designed, while identifying new
directions for research.
",1,1
"  Understanding smart grid cyber attacks is key for developing appropriate
protection and recovery measures. Advanced attacks pursue maximized impact at
minimized costs and detectability. This paper conducts risk analysis of
combined data integrity and availability attacks against the power system state
estimation. We compare the combined attacks with pure integrity attacks - false
data injection (FDI) attacks. A security index for vulnerability assessment to
these two kinds of attacks is proposed and formulated as a mixed integer linear
programming problem. We show that such combined attacks can succeed with fewer
resources than FDI attacks. The combined attacks with limited knowledge of the
system model also expose advantages in keeping stealth against the bad data
detection. Finally, the risk of combined attacks to reliable system operation
is evaluated using the results from vulnerability assessment and attack impact
analysis. The findings in this paper are validated and supported by a detailed
case study.
",1,1
"  We propose a family of near-metrics based on local graph diffusion to capture
similarity for a wide class of data sets. These quasi-metametrics, as their
names suggest, dispense with one or two standard axioms of metric spaces,
specifically distinguishability and symmetry, so that similarity between data
points of arbitrary type and form could be measured broadly and effectively.
The proposed near-metric family includes the forward k-step diffusion and its
reverse, typically on the graph consisting of data objects and their features.
By construction, this family of near-metrics is particularly appropriate for
categorical data, continuous data, and vector representations of images and
text extracted via deep learning approaches. We conduct extensive experiments
to evaluate the performance of this family of similarity measures and compare
and contrast with traditional measures of similarity used for each specific
application and with the ground truth when available. We show that for
structured data including categorical and continuous data, the near-metrics
corresponding to normalized forward k-step diffusion (k small) work as one of
the best performing similarity measures; for vector representations of text and
images including those extracted from deep learning, the near-metrics derived
from normalized and reverse k-step graph diffusion (k very small) exhibit
outstanding ability to distinguish data points from different classes.
",1,1
"  Recommender system is an important component of many web services to help
users locate items that match their interests. Several studies showed that
recommender systems are vulnerable to poisoning attacks, in which an attacker
injects fake data to a given system such that the system makes recommendations
as the attacker desires. However, these poisoning attacks are either agnostic
to recommendation algorithms or optimized to recommender systems that are not
graph-based. Like association-rule-based and matrix-factorization-based
recommender systems, graph-based recommender system is also deployed in
practice, e.g., eBay, Huawei App Store. However, how to design optimized
poisoning attacks for graph-based recommender systems is still an open problem.
In this work, we perform a systematic study on poisoning attacks to graph-based
recommender systems. Due to limited resources and to avoid detection, we assume
the number of fake users that can be injected into the system is bounded. The
key challenge is how to assign rating scores to the fake users such that the
target item is recommended to as many normal users as possible. To address the
challenge, we formulate the poisoning attacks as an optimization problem,
solving which determines the rating scores for the fake users. We also propose
techniques to solve the optimization problem. We evaluate our attacks and
compare them with existing attacks under white-box (recommendation algorithm
and its parameters are known), gray-box (recommendation algorithm is known but
its parameters are unknown), and black-box (recommendation algorithm is
unknown) settings using two real-world datasets. Our results show that our
attack is effective and outperforms existing attacks for graph-based
recommender systems. For instance, when 1% fake users are injected, our attack
can make a target item recommended to 580 times more normal users in certain
scenarios.
",0,1
"  This paper describes the Stockholm University/University of Groningen
(SU-RUG) system for the SIGMORPHON 2017 shared task on morphological
inflection. Our system is based on an attentional sequence-to-sequence neural
network model using Long Short-Term Memory (LSTM) cells, with joint training of
morphological inflection and the inverse transformation, i.e. lemmatization and
morphological analysis. Our system outperforms the baseline with a large
margin, and our submission ranks as the 4th best team for the track we
participate in (task 1, high-resource).
",1,1
"  Neuroscientists classify neurons into different types that perform similar
computations at different locations in the visual field. Traditional methods
for neural system identification do not capitalize on this separation of 'what'
and 'where'. Learning deep convolutional feature spaces that are shared among
many neurons provides an exciting path forward, but the architectural design
needs to account for data limitations: While new experimental techniques enable
recordings from thousands of neurons, experimental time is limited so that one
can sample only a small fraction of each neuron's response space. Here, we show
that a major bottleneck for fitting convolutional neural networks (CNNs) to
neural data is the estimation of the individual receptive field locations, a
problem that has been scratched only at the surface thus far. We propose a CNN
architecture with a sparse readout layer factorizing the spatial (where) and
feature (what) dimensions. Our network scales well to thousands of neurons and
short recordings and can be trained end-to-end. We evaluate this architecture
on ground-truth data to explore the challenges and limitations of CNN-based
system identification. Moreover, we show that our network model outperforms
current state-of-the art system identification models of mouse primary visual
cortex.
",1,1
"  The extremely low efficiency is regarded as the bottleneck of Wireless Power
Transfer (WPT) technology. To tackle this problem, either enlarging the
transfer power or changing the infrastructure of WPT system could be an
intuitively proposed way. However, the drastically important issue on the user
exposure of electromagnetic radiation is rarely considered while we try to
improve the efficiency of WPT. In this paper, a Distributed Antenna Power
Beacon (DA-PB) based WPT system where these antennas are uniformly distributed
on a circle is analyzed and optimized with the safety electromagnetic radiation
level (SERL) requirement. In this model, three key questions are intended to be
answered: 1) With the SERL, what is the performance of the harvested power at
the users ? 2) How do we configure the parameters to maximize the efficiency of
WPT? 3) Under the same constraints, does the DA-PB still have performance gain
than the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of
DA-PB is derived to make the radio frequency (RF) electromagnetic radiation
power density at any location of the charging cell lower than the SERL
published by the Federal Communications Commission (FCC). Second, the
closed-form expressions of average harvested Direct Current (DC) power per user
in the charging cell for pass-loss exponent 2 and 4 are also provided. In order
to maximize the average efficiency of WPT, the optimal radii for distributed
antennas elements (DAEs) are derived when the pass-loss exponent takes the
typical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a
benchmark. Simulation results verify our derived theoretical results. And it is
shown that the proposed DA-PB indeed achieves larger average harvested DC power
than CA-PB and can improve the efficiency of WPT.
",1,1
"  A numerical method for particle-laden fluids interacting with a deformable
solid domain and mobile rigid parts is proposed and implemented in a full
engineering system. The fluid domain is modeled with a lattice Boltzmann
representation, the particles and rigid parts are modeled with a discrete
element representation, and the deformable solid domain is modeled using a
Lagrangian mesh. The main issue of this work, since separately each of these
methods is a mature tool, is to develop coupling and model-reduction approaches
in order to efficiently simulate coupled problems of this nature, as occur in
various geological and engineering applications. The lattice Boltzmann method
incorporates a large-eddy simulation technique using the Smagorinsky turbulence
model. The discrete element method incorporates spherical and polyhedral
particles for stiff contact interactions. A neo-Hookean hyperelastic model is
used for the deformable solid. We provide a detailed description of how to
couple the three solvers within a unified algorithm. The technique we propose
for rubber modeling/coupling exploits a simplification that prevents having to
solve a finite-element problem each time step. We also develop a technique to
reduce the domain size of the full system by replacing certain zones with
quasi-analytic solutions, which act as effective boundary conditions for the
lattice Boltzmann method. The major ingredients of the routine are are
separately validated. To demonstrate the coupled method in full, we simulate
slurry flows in two kinds of piston-valve geometries. The dynamics of the valve
and slurry are studied and reported over a large range of input parameters.
",1,0
"  We construct a Schwinger-Keldysh effective field theory for relativistic
hydrodynamics for charged matter in a thermal background using a superspace
formalism. Superspace allows us to efficiently impose the symmetries of the
problem and to obtain a simple expression for the effective action. We show
that the theory we obtain is compatible with the Kubo-Martin-Schwinger
condition, which in turn implies that Green's functions obey the
fluctuation-dissipation theorem. Our approach complements and extends existing
formulations found in the literature.
",0,0
"  Observables have a dual nature in both classical and quantum kinematics: they
are at the same time \emph{quantities}, allowing to separate states by means of
their numerical values, and \emph{generators of transformations}, establishing
relations between different states. In this work, we show how this two-fold
role of observables constitutes a key feature in the conceptual analysis of
classical and quantum kinematics, shedding a new light on the distinguishing
feature of the quantum at the kinematical level. We first take a look at the
algebraic description of both classical and quantum observables in terms of
Jordan-Lie algebras and show how the two algebraic structures are the precise
mathematical manifestation of the two-fold role of observables. Then, we turn
to the geometric reformulation of quantum kinematics in terms of Kähler
manifolds. A key achievement of this reformulation is to show that the two-fold
role of observables is the constitutive ingredient defining what an observable
is. Moreover, it points to the fact that, from the restricted point of view of
the transformational role of observables, classical and quantum kinematics
behave in exactly the same way. Finally, we present Landsman's general
framework of Poisson spaces with transition probability, which highlights with
unmatched clarity that the crucial difference between the two kinematics lies
in the way the two roles of observables are related to each other.
",0,0
"  Let $(M,g)$ be a smooth compact Riemannian manifold of dimension $n$ with
smooth boundary $\partial M$. Suppose that $(M,g)$ admits a scalar-flat
conformal metric. We prove that the supremum of the isoperimetric quotient over
the scalar-flat conformal class is strictly larger than the best constant of
the isoperimetric inequality in the Euclidean space, and consequently is
achieved, if either (i) $n\ge 12$ and $\partial M$ has a nonumbilic point; or
(ii) $n\ge 10$, $\partial M$ is umbilic and the Weyl tensor does not vanish at
some boundary point.
",0,0
"  Random feature maps are ubiquitous in modern statistical machine learning,
where they generalize random projections by means of powerful, yet often
difficult to analyze nonlinear operators. In this paper, we leverage the
""concentration"" phenomenon induced by random matrix theory to perform a
spectral analysis on the Gram matrix of these random feature maps, here for
Gaussian mixture models of simultaneously large dimension and size. Our results
are instrumental to a deeper understanding on the interplay of the nonlinearity
and the statistics of the data, thereby allowing for a better tuning of random
feature-based techniques.
",0,1
"  The calculation of minimum energy paths for transitions such as atomic and/or
spin re-arrangements is an important task in many contexts and can often be
used to determine the mechanism and rate of transitions. An important challenge
is to reduce the computational effort in such calculations, especially when ab
initio or electron density functional calculations are used to evaluate the
energy since they can require large computational effort. Gaussian process
regression is used here to reduce significantly the number of energy
evaluations needed to find minimum energy paths of atomic rearrangements. By
using results of previous calculations to construct an approximate energy
surface and then converge to the minimum energy path on that surface in each
Gaussian process iteration, the number of energy evaluations is reduced
significantly as compared with regular nudged elastic band calculations. For a
test problem involving rearrangements of a heptamer island on a crystal
surface, the number of energy evaluations is reduced to less than a fifth. The
scaling of the computational effort with the number of degrees of freedom as
well as various possible further improvements to this approach are discussed.
",0,1
"  Social media has changed the ways of communication, where everyone is
equipped with the power to express their opinions to others in online
discussion platforms. Previously, a number of stud- ies have been presented to
identify opinion leaders in online discussion networks. Feng (""Are you
connected? Evaluating information cascade in online discussion about the
#RaceTogether campaign"", Computers in Human Behavior, 2016) identified five
types of central users and their communication patterns in an online
communication network of a limited time span. However, to trace the change in
communication pattern, a long-term analysis is required. In this study, we
critically analyzed framework presented by Feng based on five types of central
users in online communication network and their communication pattern in a
long-term manner. We take another case study presented by Udnor et al.
(""Determining social media impact on the politics of developing countries using
social network analytics"", Program, 2016) to further understand the dynamics as
well as to perform validation . Results indicate that there may not exist all
of these central users in an online communication network in a long-term
manner. Furthermore, we discuss the changing positions of opinion leaders and
their power to keep isolates interested in an online discussion network.
",1,1
"  Let $E_n(f)_{\alpha,\beta,\gamma}$ denote the error of best approximation by
polynomials of degree at most $n$ in the space
$L^2(\varpi_{\alpha,\beta,\gamma})$ on the triangle $\{(x,y): x, y \ge 0, x+y
\le 1\}$, where $\varpi_{\alpha,\beta,\gamma}(x,y) := x^\alpha y ^\beta
(1-x-y)^\gamma$ for $\alpha,\beta,\gamma > -1$. Our main result gives a sharp
estimate of $E_n(f)_{\alpha,\beta,\gamma}$ in terms of the error of best
approximation for higher order derivatives of $f$ in appropriate Sobolev
spaces. The result also leads to a characterization of
$E_n(f)_{\alpha,\beta,\gamma}$ by a weighted $K$-functional.
",0,0
"  Due to the increasing dependency of critical infrastructure on synchronized
clocks, network time synchronization protocols have become an attractive target
for attackers. We identify data origin authentication as the key security
objective and suggest to employ recently proposed high-performance digital
signature schemes (Ed25519 and MQQ-SIG)) as foundation of a novel set of
security measures to secure multicast time synchronization. We conduct
experiments to verify the computational and communication efficiency for using
these signatures in the standard time synchronization protocols NTP and PTP. We
propose additional security measures to prevent replay attacks and to mitigate
delay attacks. Our proposed solutions cover 1-step mode for NTP and PTP and we
extend our security measures specifically to 2-step mode (PTP) and show that
they have no impact on time synchronization's precision.
",1,1
"  We implement an efficient numerical method to calculate response functions of
complex impurities based on the Density Matrix Renormalization Group (DMRG) and
use it as the impurity-solver of the Dynamical Mean Field Theory (DMFT). This
method uses the correction vector to obtain precise Green's functions on the
real frequency axis at zero temperature. By using a self-consistent bath
configuration with very low entanglement, we take full advantage of the DMRG to
calculate dynamical response functions paving the way to treat large effective
impurities such as those corresponding to multi-orbital interacting models and
multi-site or multi-momenta clusters. This method leads to reliable
calculations of non-local self energies at arbitrary dopings and interactions
and at any energy scale.
",0,0
"  Bulk and surface electronic structures, calculated using density functional
theory and a tight-binding model Hamiltonian, reveal the existence of two
topologically invariant (TI) surface states in the family of cubic Bi
perovskites (ABiO$_3$; A = Na, K, Rb, Cs, Mg, Ca, Sr and Ba). The two TI
states, one lying in the valence band (TI-V) and other lying in the conduction
band (TI-C) are formed out of bonding and antibonding states of the
Bi-$\{$s,p$\}$ - O-$\{$p$\}$ coordinated covalent interaction. Below a certain
critical thickness of the film, which varies with A, TI states of top and
bottom surfaces couple to destroy the Dirac type linear dispersion and
consequently to open surface energy gaps. The origin of s-p band inversion,
necessary to form a TI state, classifies the family of ABiO$_3$ into two. For
class-I (A = Na, K, Rb, Cs and Mg) the band inversion, leading to TI-C state,
is induced by spin-orbit coupling of the Bi-p states and for class-II (A = Ca,
Sr and Ba) the band inversion is induced through weak but sensitive second
neighbor Bi-Bi interactions.
",0,0
"  It is often recommended that identifiers for ontology terms should be
semantics-free or meaningless. In practice, ontology developers tend to use
numeric identifiers, starting at 1 and working upwards. In this paper we
present a critique of current ontology semantics-free identifiers;
monotonically increasing numbers have a number of significant usability flaws
which make them unsuitable as a default option, and we present a series of
alternatives. We have provide an implementation of these alternatives which can
be freely combined.
",1,1
"  Deep learning methods have achieved high performance in sound recognition
tasks. Deciding how to feed the training data is important for further
performance improvement. We propose a novel learning method for deep sound
recognition: Between-Class learning (BC learning). Our strategy is to learn a
discriminative feature space by recognizing the between-class sounds as
between-class sounds. We generate between-class sounds by mixing two sounds
belonging to different classes with a random ratio. We then input the mixed
sound to the model and train the model to output the mixing ratio. The
advantages of BC learning are not limited only to the increase in variation of
the training data; BC learning leads to an enlargement of Fisher's criterion in
the feature space and a regularization of the positional relationship among the
feature distributions of the classes. The experimental results show that BC
learning improves the performance on various sound recognition networks,
datasets, and data augmentation schemes, in which BC learning proves to be
always beneficial. Furthermore, we construct a new deep sound recognition
network (EnvNet-v2) and train it with BC learning. As a result, we achieved a
performance surpasses the human level.
",1,1
"  We propose a linear-time, single-pass, top-down algorithm for multiple
testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and
edges specify a partial ordering in which hypotheses must be tested. The
procedure is guaranteed to reject a sub-DAG with bounded false discovery rate
(FDR) while satisfying the logical constraint that a rejected node's parents
must also be rejected. It is designed for sequential testing settings, when the
DAG structure is known a priori, but the $p$-values are obtained selectively
(such as in a sequence of experiments), but the algorithm is also applicable in
non-sequential settings when all $p$-values can be calculated in advance (such
as variable/model selection). Our DAGGER algorithm, shorthand for Greedily
Evolving Rejections on DAGs, provably controls the false discovery rate under
independence, positive dependence or arbitrary dependence of the $p$-values.
The DAGGER procedure specializes to known algorithms in the special cases of
trees and line graphs, and simplifies to the classical Benjamini-Hochberg
procedure when the DAG has no edges. We explore the empirical performance of
DAGGER using simulations, as well as a real dataset corresponding to a gene
ontology, showing favorable performance in terms of time and power.
",0,1
"  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\""
odinger equation (NLS) and an ordinary differential equation (ODE). This system
is a simplified model of the NLS around soliton solutions. Following Nakanishi
\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.
The proof is based on Nakanishi's framework and Fermi Golden Rule estimates on
$L^4$ in time norms.
",0,0
"  We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols.
",0,1
"  Time-varying network topologies can deeply influence dynamical processes
mediated by them. Memory effects in the pattern of interactions among
individuals are also known to affect how diffusive and spreading phenomena take
place. In this paper we analyze the combined effect of these two ingredients on
epidemic dynamics on networks. We study the susceptible-infected-susceptible
(SIS) and the susceptible-infected-removed (SIR) models on the recently
introduced activity-driven networks with memory. By means of an activity-based
mean-field approach we derive, in the long time limit, analytical predictions
for the epidemic threshold as a function of the parameters describing the
distribution of activities and the strength of the memory effects. Our results
show that memory reduces the threshold, which is the same for SIS and SIR
dynamics, therefore favouring epidemic spreading. The theoretical approach
perfectly agrees with numerical simulations in the long time asymptotic regime.
Strong aging effects are present in the preasymptotic regime and the epidemic
threshold is deeply affected by the starting time of the epidemics. We discuss
in detail the origin of the model-dependent preasymptotic corrections, whose
understanding could potentially allow for epidemic control on correlated
temporal networks.
",1,0
"  A long-standing obstacle to progress in deep learning is the problem of
vanishing and exploding gradients. Although, the problem has largely been
overcome via carefully constructed initializations and batch normalization,
architectures incorporating skip-connections such as highway and resnets
perform much better than standard feedforward architectures despite well-chosen
initialization and batch normalization. In this paper, we identify the
shattered gradients problem. Specifically, we show that the correlation between
gradients in standard feedforward networks decays exponentially with depth
resulting in gradients that resemble white noise whereas, in contrast, the
gradients in architectures with skip-connections are far more resistant to
shattering, decaying sublinearly. Detailed empirical evidence is presented in
support of the analysis, on both fully-connected networks and convnets.
Finally, we present a new ""looks linear"" (LL) initialization that prevents
shattering, with preliminary experiments showing the new initialization allows
to train very deep networks without the addition of skip-connections.
",1,1
"  We study the band structure topology and engineering from the interplay
between local moments and itinerant electrons in the context of pyrochlore
iridates. For the metallic iridate Pr$_2$Ir$_2$O$_7$, the Ir $5d$ conduction
electrons interact with the Pr $4f$ local moments via the $f$-$d$ exchange.
While the Ir electrons form a Luttinger semimetal, the Pr moments can be tuned
into an ordered spin ice with a finite ordering wavevector, dubbed
""Melko-Hertog-Gingras"" state, by varying Ir and O contents. We point out that
the ordered spin ice of the Pr local moments generates an internal magnetic
field that reconstructs the band structure of the Luttinger semimetal. Besides
the broad existence of Weyl nodes, we predict that the magnetic translation of
the ""Melko-Hertog-Gingras"" state for the Pr moments protects the Dirac band
touching at certain time reversal invariant momenta for the Ir conduction
electrons. We propose the magnetic fields to control the Pr magnetic structure
and thereby indirectly influence the topological and other properties of the Ir
electrons. Our prediction may be immediately tested in the ordered
Pr$_2$Ir$_2$O$_7$ samples. We expect our work to stimulate a detailed
examination of the band structure, magneto-transport, and other properties of
Pr$_2$Ir$_2$O$_7$.
",0,0
"  Boundary value problems for Sturm-Liouville operators with potentials from
the class $W_2^{-1}$ on a star-shaped graph are considered. We assume that the
potentials are known on all the edges of the graph except two, and show that
the potentials on the remaining edges can be constructed by fractional parts of
two spectra. A uniqueness theorem is proved, and an algorithm for the
constructive solution of the partial inverse problem is provided. The main
ingredient of the proofs is the Riesz-basis property of specially constructed
systems of functions.
",0,0
