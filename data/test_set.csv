ID,TITLE,ABSTRACT,Computer Science,Physics,Mathematics,Statistics,Quantitative Biology,Quantitative Finance
475,Selecting optimal minimum spanning trees that share a topological correspondence with phylogenetic trees,"  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method
called CLGrouping, for constructing tree-structured probabilistic graphical
models, a statistical framework that is commonly used for inferring
phylogenetic trees. While CLGrouping works correctly if there is a unique MST,
we observe an indeterminacy in the method in the case that there are multiple
MSTs. In this work we remove this indeterminacy by introducing so-called
vertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely
related to the number of leaves in the MST. This motivates the problem of
finding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We
provide a polynomial time algorithm for the MLVRMST problem, and prove its
correctness for graphs whose edges are weighted with tree-additive distances.
",1,0,1,0,0,0
11242,A Markov Chain Model for the Cure Rate of Non-Performing Loans,"  A Markov-chain model is developed for the purpose estimation of the cure rate
of non-performing loans. The technique is performed collectively, on portfolios
and it can be applicable in the process of calculation of credit impairment. It
is efficient in terms of data manipulation costs which makes it accessible even
to smaller financial institutions. In addition, several other applications to
portfolio optimization are suggested.
",0,0,0,1,0,1
649,On Popov's formula involving the Von Mangoldt function,"  We offer a generalization of a formula of Popov involving the Von Mangoldt
function. Some commentary on its relation to other results in analytic number
theory is mentioned as well as an analogue involving the m$\ddot{o}$bius
function.
",0,0,1,0,0,0
15325,Retirement spending and biological age,"  We solve a lifecycle model in which the consumer's chronological age does not
move in lockstep with calendar time. Instead, biological age increases at a
stochastic non-linear rate in time like a broken clock that might occasionally
move backwards. In other words, biological age could actually decline. Our
paper is inspired by the growing body of medical literature that has identified
biomarkers which indicate how people age at different rates. This offers better
estimates of expected remaining lifetime and future mortality rates. It isn't
farfetched to argue that in the not-too-distant future personal age will be
more closely associated with biological vs. calendar age. Thus, after
introducing our stochastic mortality model we derive optimal consumption rates
in a classic Yaari (1965) framework adjusted to our proper clock time. In
addition to the normative implications of having access to biological age, our
positive objective is to partially explain the cross-sectional heterogeneity in
retirement spending rates at any given chronological age. In sum, we argue that
neither biological nor chronological age alone is a sufficient statistic for
making economic decisions. Rather, both ages are required to behave rationally.
",0,0,0,0,0,1
1529,Social Network based Short-Term Stock Trading System,"  This paper proposes a novel adaptive algorithm for the automated short-term
trading of financial instrument. The algorithm adopts a semantic sentiment
analysis technique to inspect the Twitter posts and to use them to predict the
behaviour of the stock market. Indeed, the algorithm is specifically developed
to take advantage of both the sentiment and the past values of a certain
financial instrument in order to choose the best investment decision. This
allows the algorithm to ensure the maximization of the obtainable profits by
trading on the stock market. We have conducted an investment simulation and
compared the performance of our proposed with a well-known benchmark (DJTATO
index) and the optimal results, in which an investor knows in advance the
future price of a product. The result shows that our approach outperforms the
benchmark and achieves the performance score close to the optimal result.
",1,0,0,0,0,1
288,The anti-spherical category,"  We study a diagrammatic categorification (the ""anti-spherical category"") of
the anti-spherical module for any Coxeter group. We deduce that Deodhar's
(sign) parabolic Kazhdan-Lusztig polynomials have non-negative coefficients,
and that a monotonicity conjecture of Brenti's holds. The main technical
observation is a localisation procedure for the anti-spherical category, from
which we construct a ""light leaves"" basis of morphisms. Our techniques may be
used to calculate many new elements of the $p$-canonical basis in the
anti-spherical module. The results use generators and relations for Soergel
bimodules (""Soergel calculus"") in a crucial way.
",0,0,1,0,0,0
659,Observation of surface plasmon polaritons in 2D electron gas of surface electron accumulation in InN nanostructures,"  Recently, heavily doped semiconductors are emerging as an alternate for low
loss plasmonic materials. InN, belonging to the group III nitrides, possesses
the unique property of surface electron accumulation (SEA) which provides two
dimensional electron gas (2DEG) system. In this report, we demonstrated the
surface plasmon properties of InN nanoparticles originating from SEA using the
real space mapping of the surface plasmon fields for the first time. The SEA is
confirmed by Raman studies which are further corroborated by photoluminescence
and photoemission spectroscopic studies. The frequency of 2DEG corresponding to
SEA is found to be in the THz region. The periodic fringes are observed in the
near-field scanning optical microscopic images of InN nanostructures. The
observed fringes are attributed to the interference of propagated and back
reflected surface plasmon polaritons (SPPs). The observation of SPPs is solely
attributed to the 2DEG corresponding to the SEA of InN. In addition, resonance
kind of behavior with the enhancement of the near-field intensity is observed
in the near-field images of InN nanostructures. Observation of SPPs indicates
that InN with SEA can be a promising THz plasmonic material for the light
confinement.
",0,1,0,0,0,0
2877,Limits to Arbitrage in Markets with Stochastic Settlement Latency,"  Distributed ledger technologies rely on consensus protocols confronting
traders with random waiting times until the transfer of ownership is
accomplished. This time-consuming settlement process exposes arbitrageurs to
price risk and imposes limits to arbitrage. We derive theoretical arbitrage
boundaries under general assumptions and show that they increase with expected
latency, latency uncertainty, spot volatility, and risk aversion. Using
high-frequency data from the Bitcoin network, we estimate arbitrage boundaries
due to settlement latency of on average 124 basis points, covering 88 percent
of the observed cross-exchange price differences. Settlement through
decentralized systems thus induces non-trivial frictions affecting market
efficiency and price formation.
",0,0,0,0,0,1
801,Tangle-tree duality in abstract separation systems,"  We prove a general width duality theorem for combinatorial structures with
well-defined notions of cohesion and separation. These might be graphs and
matroids, but can be much more general or quite different. The theorem asserts
a duality between the existence of high cohesiveness somewhere local and a
global overall tree structure.
We describe cohesive substructures in a unified way in the format of tangles:
as orientations of low-order separations satisfying certain consistency axioms.
These axioms can be expressed without reference to the underlying structure,
such as a graph or matroid, but just in terms of the poset of the separations
themselves. This makes it possible to identify tangles, and apply our
tangle-tree duality theorem, in very diverse settings.
Our result implies all the classical duality theorems for width parameters in
graph minor theory, such as path-width, tree-width, branch-width or rank-width.
It yields new, tangle-type, duality theorems for tree-width and path-width. It
implies the existence of width parameters dual to cohesive substructures such
as $k$-blocks, edge-tangles, or given subsets of tangles, for which no width
duality theorems were previously known.
Abstract separation systems can be found also in structures quite unlike
graphs and matroids. For example, our theorem can be applied to image analysis
by capturing the regions of an image as tangles of separations defined as
natural partitions of its set of pixels. It can be applied in big data contexts
by capturing clusters as tangles. It can be applied in the social sciences,
e.g. by capturing as tangles the few typical mindsets of individuals found by a
survey. It could also be applied in pure mathematics, e.g. to separations of
compact manifolds.
",0,0,1,0,0,0
6844,Micro-sized cold atmospheric plasma source for brain and breast cancer treatment,"  Micro-sized cold atmospheric plasma (uCAP) has been developed to expand the
applications of CAP in cancer therapy. In this paper, uCAP devices with
different nozzle lengths were applied to investigate effects on both brain
(glioblastoma U87) and breast (MDA-MB-231) cancer cells. Various diagnostic
techniques were employed to evaluate the parameters of uCAP devices with
different lengths such as potential distribution, electron density, and optical
emission spectroscopy. The generation of short- and long-lived species (such as
hydroxyl radical (.OH), superoxide (O2-), hydrogen peroxide (H2O2), nitrite
(NO2-), et al) were studied. These data revealed that uCAP treatment with a 20
mm length tube has a stronger effect than that of the 60 mm tube due to the
synergetic effects of reactive species and free radicals. Reactive species
generated by uCAP enhanced tumor cell death in a dose-dependent fashion and was
not specific with regards to tumor cell type.
",0,0,0,0,1,0
1003,A reproducible effect size is more useful than an irreproducible hypothesis test to analyze high throughput sequencing datasets,"  Motivation: P values derived from the null hypothesis significance testing
framework are strongly affected by sample size, and are known to be
irreproducible in underpowered studies, yet no suitable replacement has been
proposed. Results: Here we present implementations of non-parametric
standardized median effect size estimates, dNEF, for high-throughput sequencing
datasets. Case studies are shown for transcriptome and tag-sequencing datasets.
The dNEF measure is shown to be more repro- ducible and robust than P values
and requires sample sizes as small as 3 to reproducibly identify differentially
abundant features. Availability: Source code and binaries freely available at:
this https URL, omicplotR, and
this https URL.
",0,0,0,0,1,0
646,Data-Driven Sparse Structure Selection for Deep Neural Networks,"  Deep convolutional neural networks have liberated its extraordinary power on
various tasks. However, it is still very challenging to deploy state-of-the-art
models into real-world applications due to their high computational complexity.
How can we design a compact and effective network without massive experiments
and expert knowledge? In this paper, we propose a simple and effective
framework to learn and prune deep models in an end-to-end manner. In our
framework, a new type of parameter -- scaling factor is first introduced to
scale the outputs of specific structures, such as neurons, groups or residual
blocks. Then we add sparsity regularizations on these factors, and solve this
optimization problem by a modified stochastic Accelerated Proximal Gradient
(APG) method. By forcing some of the factors to zero, we can safely remove the
corresponding structures, thus prune the unimportant parts of a CNN. Comparing
with other structure selection methods that may need thousands of trials or
iterative fine-tuning, our method is trained fully end-to-end in one training
pass without bells and whistles. We evaluate our method, Sparse Structure
Selection with several state-of-the-art CNNs, and demonstrate very promising
results with adaptive depth and width selection.
",1,0,0,0,0,0
2068,The Broad Consequences of Narrow Banking,"  We investigate the macroeconomic consequences of narrow banking in the
context of stock-flow consistent models. We begin with an extension of the
Goodwin-Keen model incorporating time deposits, government bills, cash, and
central bank reserves to the base model with loans and demand deposits and use
it to describe a fractional reserve banking system. We then characterize narrow
banking by a full reserve requirement on demand deposits and describe the
resulting separation between the payment system and lending functions of the
resulting banking sector. By way of numerical examples, we explore the
properties of fractional and full reserve versions of the model and compare
their asymptotic properties. We find that narrow banking does not lead to any
loss in economic growth when the models converge to a finite equilibrium, while
allowing for more direct monitoring and prevention of financial breakdowns in
the case of explosive asymptotic behaviour.
",0,0,0,0,0,1
943,Graph complexity and Mahler measure,"  The (torsion) complexity of a finite edge-weighted graph is defined to be the
order of the torsion subgroup of the abelian group presented by its Laplacian
matrix. When G is d-periodic (i.e., G has a free action of the rank-d free
abelian group by graph automorphisms, with finite quotient) the Mahler measure
of its Laplacian determinant polynomial is the growth rate of the complexity of
finite quotients of G. Lehmer's question, an open question about the roots of
monic integral polynomials, is equivalent to a question about the complexity
growth of edge-weighted 1-periodic graphs.
",0,0,1,0,0,0
300,Spectral analysis of jet turbulence,"  Informed by LES data and resolvent analysis of the mean flow, we examine the
structure of turbulence in jets in the subsonic, transonic, and supersonic
regimes. Spectral (frequency-space) proper orthogonal decomposition is used to
extract energy spectra and decompose the flow into energy-ranked coherent
structures. The educed structures are generally well predicted by the resolvent
analysis. Over a range of low frequencies and the first few azimuthal mode
numbers, these jets exhibit a low-rank response characterized by
Kelvin-Helmholtz (KH) type wavepackets associated with the annular shear layer
up to the end of the potential core and that are excited by forcing in the
very-near-nozzle shear layer. These modes too the have been experimentally
observed before and predicted by quasi-parallel stability theory and other
approximations--they comprise a considerable portion of the total turbulent
energy. At still lower frequencies, particularly for the axisymmetric mode, and
again at high frequencies for all azimuthal wavenumbers, the response is not
low rank, but consists of a family of similarly amplified modes. These modes,
which are primarily active downstream of the potential core, are associated
with the Orr mechanism. They occur also as sub-dominant modes in the range of
frequencies dominated by the KH response. Our global analysis helps tie
together previous observations based on local spatial stability theory, and
explains why quasi-parallel predictions were successful at some frequencies and
azimuthal wavenumbers, but failed at others.
",0,1,0,0,0,0
12313,Assessing the effect of advertising expenditures upon sales: a Bayesian structural time series model,"  We propose a robust implementation of the Nerlove--Arrow model using a
Bayesian structural time series model to explain the relationship between
advertising expenditures of a country-wide fast-food franchise network with its
weekly sales. Thanks to the flexibility and modularity of the model, it is well
suited to generalization to other markets or situations. Its Bayesian nature
facilitates incorporating \emph{a priori} information (the manager's views),
which can be updated with relevant data. This aspect of the model will be used
to present a strategy of budget scheduling across time and channels.
",0,0,0,1,0,1
220,GENFIRE: A generalized Fourier iterative reconstruction algorithm for high-resolution 3D imaging,"  Tomography has made a radical impact on diverse fields ranging from the study
of 3D atomic arrangements in matter to the study of human health in medicine.
Despite its very diverse applications, the core of tomography remains the same,
that is, a mathematical method must be implemented to reconstruct the 3D
structure of an object from a number of 2D projections. In many scientific
applications, however, the number of projections that can be measured is
limited due to geometric constraints, tolerable radiation dose and/or
acquisition speed. Thus it becomes an important problem to obtain the
best-possible reconstruction from a limited number of projections. Here, we
present the mathematical implementation of a tomographic algorithm, termed
GENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between
real and reciprocal space, GENFIRE searches for a global solution that is
concurrently consistent with the measured data and general physical
constraints. The algorithm requires minimal human intervention and also
incorporates angular refinement to reduce the tilt angle error. We demonstrate
that GENFIRE can produce superior results relative to several other popular
tomographic reconstruction techniques by numerical simulations, and by
experimentally by reconstructing the 3D structure of a porous material and a
frozen-hydrated marine cyanobacterium. Equipped with a graphical user
interface, GENFIRE is freely available from our website and is expected to find
broad applications across different disciplines.
",0,1,0,0,0,0
875,Computing Simple Multiple Zeros of Polynomial Systems,"  Given a polynomial system f associated with a simple multiple zero x of
multiplicity {\mu}, we give a computable lower bound on the minimal distance
between the simple multiple zero x and other zeros of f. If x is only given
with limited accuracy, we propose a numerical criterion that f is certified to
have {\mu} zeros (counting multiplicities) in a small ball around x.
Furthermore, for simple double zeros and simple triple zeros whose Jacobian is
of normalized form, we define modified Newton iterations and prove the
quantified quadratic convergence when the starting point is close to the exact
simple multiple zero. For simple multiple zeros of arbitrary multiplicity whose
Jacobian matrix may not have a normalized form, we perform unitary
transformations and modified Newton iterations, and prove its non-quantified
quadratic convergence and its quantified convergence for simple triple zeros.
",0,0,1,0,0,0
419,Stellar streams as gravitational experiments I. The case of Sagittarius,"  Tidal streams of disrupting dwarf galaxies orbiting around their host galaxy
offer a unique way to constrain the shape of galactic gravitational potentials.
Such streams can be used as leaning tower gravitational experiments on galactic
scales. The most well motivated modification of gravity proposed as an
alternative to dark matter on galactic scales is Milgromian dynamics (MOND),
and we present here the first ever N-body simulations of the dynamical
evolution of the disrupting Sagittarius dwarf galaxy in this framework. Using a
realistic baryonic mass model for the Milky Way, we attempt to reproduce the
present-day spatial and kinematic structure of the Sagittarius dwarf and its
immense tidal stream that wraps around the Milky Way. With very little freedom
on the original structure of the progenitor, constrained by the total
luminosity of the Sagittarius structure and by the observed stellar mass-size
relation for isolated dwarf galaxies, we find reasonable agreement between our
simulations and observations of this system. The observed stellar velocities in
the leading arm can be reproduced if we include a massive hot gas corona around
the Milky Way that is flattened in the direction of the principal plane of its
satellites. This is the first time that tidal dissolution in MOND has been
tested rigorously at these mass and acceleration scales.
",0,1,0,0,0,0
796,Neutral Carbon Emission in luminous infrared galaxies The \CI\ Lines as Total Molecular Gas Tracers,"  We present a statistical study on the [C I]($^{3} \rm P_{1} \rightarrow {\rm
^3 P}_{0}$), [C I] ($^{3} \rm P_{2} \rightarrow {\rm ^3 P}_{1}$) lines
(hereafter [C I] (1$-$0) and [C I] (2$-$1), respectively) and the CO (1$-$0)
line for a sample of (ultra)luminous infrared galaxies [(U)LIRGs]. We explore
the correlations between the luminosities of CO (1$-$0) and [C I] lines, and
find that $L'_\mathrm{CO(1-0)}$ correlates almost linearly with both $L'_
\mathrm{[CI](1-0)}$ and $L'_\mathrm{[CI](2-1)}$, suggesting that [C I] lines
can trace total molecular gas mass at least for (U)LIRGs. We also investigate
the dependence of $L'_\mathrm{[CI](1-0)}$/$L'_\mathrm{CO(1-0)}$,
$L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{CO(1-0)}$ and
$L'_\mathrm{[CI](2-1)}$/$L'_\mathrm{[CI](1-0)}$ on the far-infrared color of
60-to-100 $\mu$m, and find non-correlation, a weak correlation and a modest
correlation, respectively. Under the assumption that these two carbon
transitions are optically thin, we further calculate the [C I] line excitation
temperatures, atomic carbon masses, and the mean [C I] line flux-to-H$_2$ mass
conversion factors for our sample. The resulting $\mathrm{H_2}$ masses using
these [C I]-based conversion factors roughly agree with those derived from
$L'_\mathrm{CO(1-0)}$ and CO-to-H$_2$ conversion factor.
",0,1,0,0,0,0
1133,Exact partial information decompositions for Gaussian systems based on dependency constraints,"  The Partial Information Decomposition (PID) [arXiv:1004.2515] provides a
theoretical framework to characterize and quantify the structure of
multivariate information sharing. A new method (Idep) has recently been
proposed for computing a two-predictor PID over discrete spaces.
[arXiv:1709.06653] A lattice of maximum entropy probability models is
constructed based on marginal dependency constraints, and the unique
information that a particular predictor has about the target is defined as the
minimum increase in joint predictor-target mutual information when that
particular predictor-target marginal dependency is constrained. Here, we apply
the Idep approach to Gaussian systems, for which the marginally constrained
maximum entropy models are Gaussian graphical models. Closed form solutions for
the Idep PID are derived for both univariate and multivariate Gaussian systems.
Numerical and graphical illustrations are provided, together with practical and
theoretical comparisons of the Idep PID with the minimum mutual information PID
(Immi). [arXiv:1411.2832] In particular, it is proved that the Immi method
generally produces larger estimates of redundancy and synergy than does the
Idep method. In discussion of the practical examples, the PIDs are complemented
by the use of deviance tests for the comparison of Gaussian graphical models.
",0,0,0,1,1,0
237,Tuning across the BCS-BEC crossover in the multiband superconductor Fe$_{1+y}$Se$_x$Te$_{1-x}$ : An angle-resolved photoemission study,"  The crossover from Bardeen-Cooper-Schrieffer (BCS) superconductivity to
Bose-Einstein condensation (BEC) is difficult to realize in quantum materials
because, unlike in ultracold atoms, one cannot tune the pairing interaction. We
realize the BCS-BEC crossover in a nearly compensated semimetal
Fe$_{1+y}$Se$_x$Te$_{1-x}$ by tuning the Fermi energy, $\epsilon_F$, via
chemical doping, which permits us to systematically change $\Delta /
\epsilon_F$ from 0.16 to 0.5 were $\Delta$ is the superconducting (SC) gap. We
use angle-resolved photoemission spectroscopy to measure the Fermi energy, the
SC gap and characteristic changes in the SC state electronic dispersion as the
system evolves from a BCS to a BEC regime. Our results raise important
questions about the crossover in multiband superconductors which go beyond
those addressed in the context of cold atoms.
",0,1,0,0,0,0
897,Bounded solutions for a class of Hamiltonian systems,"  We obtain bounded for all $t$ solutions of ordinary differential equations as
limits of the solutions of the corresponding Dirichlet problems on $(-L,L)$,
with $L \rightarrow \infty$. We derive a priori estimates for the Dirichlet
problems, allowing passage to the limit, via a diagonal sequence. This approach
carries over to the PDE case.
",0,0,1,0,0,0
9524,Dynamic Advisor-Based Ensemble (dynABE): Case Study in Stock Trend Prediction of Critical Metal Companies,"  The demand for metals by modern technology has been shifting from common base
metals to a variety of minor metals, such as cobalt or indium. The industrial
importance and limited geological availability of some minor metals have led to
them being considered more ""critical,"" and there is a growing investment
interest in such critical metals and their producing companies. In this
research, we create a novel framework, Dynamic Advisor-Based Ensemble (dynABE),
for stock prediction and use critical metal companies as case study. dynABE
uses domain knowledge to diversify the feature set by dividing them into
different ""advisors."" creates high-level ensembles with complex base models for
each advisor, and combines the advisors together dynamically during validation
with a novel and effective online update strategy. We test dynABE on three
cobalt-related companies, and it achieves the best-case misclassification error
of 31.12% and excess return of 477% compared to the stock itself in a year and
a half. In addition to presenting an effective stock prediction model with
decent profitabilities, this research further analyzes dynABE to visualize how
it works in practice, which also yields discoveries of its interesting
behaviors when processing time-series data.
",0,0,0,0,0,1
405,Learning body-affordances to simplify action spaces,"  Controlling embodied agents with many actuated degrees of freedom is a
challenging task. We propose a method that can discover and interpolate between
context dependent high-level actions or body-affordances. These provide an
abstract, low-dimensional interface indexing high-dimensional and time-
extended action policies. Our method is related to recent ap- proaches in the
machine learning literature but is conceptually simpler and easier to
implement. More specifically our method requires the choice of a n-dimensional
target sensor space that is endowed with a distance metric. The method then
learns an also n-dimensional embedding of possibly reactive body-affordances
that spread as far as possible throughout the target sensor space.
",1,0,0,0,0,0
6263,The CCI30 Index,"  We describe the design of the CCI30 cryptocurrency index.
",0,0,0,0,0,1
607,Boosting the Actor with Dual Critic,"  This paper proposes a new actor-critic-style algorithm called Dual
Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian
dual form of the Bellman optimality equation, which can be viewed as a
two-player game between the actor and a critic-like function, which is named as
dual critic. Compared to its actor-critic relatives, Dual-AC has the desired
property that the actor and dual critic are updated cooperatively to optimize
the same objective function, providing a more transparent way for learning the
critic that is directly related to the objective function of the actor. We then
provide a concrete algorithm that can effectively solve the minimax
optimization problem, using techniques of multi-step bootstrapping, path
regularization, and stochastic dual ascent algorithm. We demonstrate that the
proposed algorithm achieves the state-of-the-art performances across several
benchmarks.
",1,0,0,0,0,0
5819,"Health Care Expenditures, Financial Stability, and Participation in the Supplemental Nutrition Assistance Program (SNAP)","  This paper examines the association between household healthcare expenses and
participation in the Supplemental Nutrition Assistance Program (SNAP) when
moderated by factors associated with financial stability of households. Using a
large longitudinal panel encompassing eight years, this study finds that an
inter-temporal increase in out-of-pocket medical expenses increased the
likelihood of household SNAP participation in the current period. Financially
stable households with precautionary financial assets to cover at least 6
months worth of household expenses were significantly less likely to
participate in SNAP. The low income households who recently experienced an
increase in out of pocket medical expenses but had adequate precautionary
savings were less likely than similar households who did not have precautionary
savings to participate in SNAP. Implications for economists, policy makers, and
household finance professionals are discussed.
",0,0,0,0,0,1
447,Rotation of a synchronous viscoelastic shell,"  Several natural satellites of the giant planets have shown evidence of a
global internal ocean, coated by a thin, icy crust. This crust is probably
viscoelastic, which would alter its rotational response. This response would
translate into several rotational quantities, i.e. the obliquity, and the
librations at different frequencies, for which the crustal elasticity reacts
differently. This study aims at modelling the global response of the
viscoelastic crust. For that, I derive the time-dependency of the tensor of
inertia, which I combine with the time evolution of the rotational quantities,
thanks to an iterative algorithm. This algorithm combines numerical simulations
of the rotation with a digital filtering of the resulting tensor of inertia.
The algorithm works very well in the elastic case, provided the problem is not
resonant. However, considering tidal dissipation adds different phase lags to
the oscillating contributions, which challenge the convergence of the
algorithm.
",0,1,0,0,0,0
805,Homogenization of nonlinear elliptic systems in nonreflexive Musielak-Orlicz spaces,"  We study the homogenization process for families of strongly nonlinear
elliptic systems with the homogeneous Dirichlet boundary conditions. The growth
and the coercivity of the elliptic operator is assumed to be indicated by a
general inhomogeneous anisotropic $N-$function, which may be possibly also
dependent on the spatial variable, i.e., the homogenization process will change
the characteristic function spaces at each step. Such a problem is well known
and there exists many positive results for the function satisfying $\Delta_2$
and $\nabla_2$ conditions an being in addition Hölder continuous with
respect to the spatial variable. We shall show that cases these conditions can
be neglected and will deal with a rather general problem in general function
space setting.
",0,0,1,0,0,0
269,An Expanded Local Variance Gamma model,"  The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
",0,0,0,0,0,1
10660,PROOF OF VALUE ALIENATION (PoVA) - a concept of a cryptocurrency issuance protocol,"  In this paper, we will describe a concept of a cryptocurrency issuance
protocol which supports digital currencies in a Proof-of-Work (< PoW >) like
manner. However, the methods assume alternative utilization of assets used for
cryptocurrency creation (rather than purchasing electricity necessary for <
mining >).
",0,0,0,0,0,1
398,Complexity and capacity bounds for quantum channels,"  We generalise some well-known graph parameters to operator systems by
considering their underlying quantum channels. In particular, we introduce the
quantum complexity as the dimension of the smallest co-domain Hilbert space a
quantum channel requires to realise a given operator system as its
non-commutative confusability graph. We describe quantum complexity as a
generalised minimum semidefinite rank and, in the case of a graph operator
system, as a quantum intersection number. The quantum complexity and a closely
related quantum version of orthogonal rank turn out to be upper bounds for the
Shannon zero-error capacity of a quantum channel, and we construct examples for
which these bounds beat the best previously known general upper bound for the
capacity of quantum channels, given by the quantum Lovász theta number.
",0,0,1,0,0,0
493,Psychological model of the investor and manager behavior in risk,"  All people have to make risky decisions in everyday life. And we do not know
how true they are. But is it possible to mathematically assess the correctness
of our choice? This article discusses the model of decision making under risk
on the example of project management. This is a game with two players, one of
which is Investor, and the other is the Project Manager. Each player makes a
risky decision for himself, based on his past experience. With the help of a
mathematical model, the players form a level of confidence, depending on who
the player accepts the strategy or does not accept. The project manager
assesses the costs and compares them with the level of confidence. An investor
evaluates past results. Also visit the case where the strategy of the player
accepts the part.
",0,0,0,0,0,1
242,On asymptotically minimax nonparametric detection of signal in Gaussian white noise,"  For the problem of nonparametric detection of signal in Gaussian white noise
we point out strong asymptotically minimax tests. The sets of alternatives are
a ball in Besov space $B^r_{2\infty}$ with ""small"" balls in $L_2$ removed.
",0,0,1,1,0,0
9099,The sum of log-normal variates in geometric Brownian motion,"  Geometric Brownian motion (GBM) is a key model for representing
self-reproducing entities. Self-reproduction may be considered the definition
of life [5], and the dynamics it induces are of interest to those concerned
with living systems from biology to economics. Trajectories of GBM are
distributed according to the well-known log-normal density, broadening with
time. However, in many applications, what's of interest is not a single
trajectory but the sum, or average, of several trajectories. The distribution
of these objects is more complicated. Here we show two different ways of
finding their typical trajectories. We make use of an intriguing connection to
spin glasses: the expected free energy of the random energy model is an average
of log-normal variates. We make the mapping to GBM explicit and find that the
free energy result gives qualitatively correct behavior for GBM trajectories.
We then also compute the typical sum of lognormal variates using Ito calculus.
This alternative route is in close quantitative agreement with numerical work.
",0,0,0,0,0,1
6271,Quantifying the Model Risk Inherent in the Calibration and Recalibration of Option Pricing Models,"  We focus on two particular aspects of model risk: the inability of a chosen
model to fit observed market prices at a given point in time (calibration
error) and the model risk due to recalibration of model parameters (in
contradiction to the model assumptions). In this context, we follow the
approach of Glasserman and Xu (2014) and use relative entropy as a pre-metric
in order to quantify these two sources of model risk in a common framework, and
consider the trade-offs between them when choosing a model and the frequency
with which to recalibrate to the market. We illustrate this approach applied to
the models of Black and Scholes (1973) and Heston (1993), using option data for
Apple (AAPL) and Google (GOOG). We find that recalibrating a model more
frequently simply shifts model risk from one type to another, without any
substantial reduction of aggregate model risk. Furthermore, moving to a more
complicated stochastic model is seen to be counterproductive if one requires a
high degree of robustness, for example as quantified by a 99 percent quantile
of aggregate model risk.
",0,0,0,0,0,1
634,A simple introduction to Karmarkar's Algorithm for Linear Programming,"  An extremely simple, description of Karmarkar's algorithm with very few
technical terms is given.
",1,0,0,0,0,0
13777,Time consistency for scalar multivariate risk measures,"  In this paper we present results on dynamic multivariate scalar risk
measures, which arise in markets with transaction costs and systemic risk. Dual
representations of such risk measures are presented. These are then used to
obtain the main results of this paper on time consistency; namely, an
equivalent recursive formulation of multivariate scalar risk measures to
multiportfolio time consistency. We are motivated to study time consistency of
multivariate scalar risk measures as the superhedging risk measure in markets
with transaction costs (with a single eligible asset) (Jouini and Kallal
(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy
the usual scalar concept of time consistency. In fact, as demonstrated in
(Feinstein and Rudloff (2018)), scalar risk measures with the same
scalarization weight at all times would not be time consistent in general. The
deduced recursive relation for the scalarizations of multiportfolio time
consistent set-valued risk measures provided in this paper requires
consideration of the entire family of scalarizations. In this way we develop a
direct notion of a ""moving scalarization"" for scalar time consistency that
corroborates recent research on scalarizations of dynamic multi-objective
problems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2018)).
",0,0,0,0,0,1
557,Adaptive local surface refinement based on LR NURBS and its application to contact,"  A novel adaptive local surface refinement technique based on Locally Refined
Non-Uniform Rational B-Splines (LR NURBS) is presented. LR NURBS can model
complex geometries exactly and are the rational extension of LR B-splines. The
local representation of the parameter space overcomes the drawback of
non-existent local refinement in standard NURBS-based isogeometric analysis.
For a convenient embedding into general finite element code, the Bézier
extraction operator for LR NURBS is formulated. An automatic remeshing
technique is presented that allows adaptive local refinement and coarsening of
LR NURBS. In this work, LR NURBS are applied to contact computations of 3D
solids and membranes. For solids, LR NURBS-enriched finite elements are used to
discretize the contact surfaces with LR NURBS finite elements, while the rest
of the body is discretized by linear Lagrange finite elements. For membranes,
the entire surface is discretized by LR NURBS. Various numerical examples are
shown, and they demonstrate the benefit of using LR NURBS: Compared to uniform
refinement, LR NURBS can achieve high accuracy at lower computational cost.
",1,0,0,0,0,0
987,Statistical foundations for assessing the difference between the classical and weighted-Gini betas,"  The `beta' is one of the key quantities in the capital asset pricing model
(CAPM). In statistical language, the beta can be viewed as the slope of the
regression line fitted to financial returns on the market against the returns
on the asset under consideration. The insurance counterpart of CAPM, called the
weighted insurance pricing model (WIPM), gives rise to the so-called
weighted-Gini beta. The aforementioned two betas may or may not coincide,
depending on the form of the underlying regression function, and this has
profound implications when designing portfolios and allocating risk capital. To
facilitate these tasks, in this paper we develop large-sample statistical
inference results that, in a straightforward fashion, imply confidence
intervals for, and hypothesis tests about, the equality of the two betas.
",0,0,1,1,0,0
346,The Distance Standard Deviation,"  The distance standard deviation, which arises in distance correlation
analysis of multivariate data, is studied as a measure of spread. New
representations for the distance standard deviation are obtained in terms of
Gini's mean difference and in terms of the moments of spacings of order
statistics. Inequalities for the distance variance are derived, proving that
the distance standard deviation is bounded above by the classical standard
deviation and by Gini's mean difference. Further, it is shown that the distance
standard deviation satisfies the axiomatic properties of a measure of spread.
Explicit closed-form expressions for the distance variance are obtained for a
broad class of parametric distributions. The asymptotic distribution of the
sample distance variance is derived.
",0,0,1,1,0,0
556,Deformation conditions for pseudorepresentations,"  Given a property of representations satisfying a basic stability condition,
Ramakrishna developed a variant of Mazur's Galois deformation theory for
representations with that property. We introduce an axiomatic definition of
pseudorepresentations with such a property. Among other things, we show that
pseudorepresentations with a property enjoy a good deformation theory,
generalizing Ramakrishna's theory to pseudorepresentations.
",0,0,1,0,0,0
1966,Trail-Mediated Self-Interaction,"  A number of microorganisms leave persistent trails while moving along
surfaces. For single-cell organisms, the trail-mediated self-interaction will
influence its dynamics. It has been discussed recently [Kranz \textit{et al.}
Phys. Rev. Lett. \textbf{117}, 8101 (2016)] that the self-interaction may
localize the organism above a critical coupling $\chi_c$ to the trail. Here we
will derive a generalized active particle model capturing the key features of
the self-interaction and analyze its behavior for smaller couplings $\chi <
\chi_c$. We find that fluctuations in propulsion speed shift the localization
transition to stronger couplings.
",0,0,0,0,1,0
1181,How Many Subpopulations is Too Many? Exponential Lower Bounds for Inferring Population Histories,"  Reconstruction of population histories is a central problem in population
genetics. Existing coalescent-based methods, like the seminal work of Li and
Durbin (Nature, 2011), attempt to solve this problem using sequence data but
have no rigorous guarantees. Determining the amount of data needed to correctly
reconstruct population histories is a major challenge. Using a variety of tools
from information theory, the theory of extremal polynomials, and approximation
theory, we prove new sharp information-theoretic lower bounds on the problem of
reconstructing population structure -- the history of multiple subpopulations
that merge, split and change sizes over time. Our lower bounds are exponential
in the number of subpopulations, even when reconstructing recent histories. We
demonstrate the sharpness of our lower bounds by providing algorithms for
distinguishing and learning population histories with matching dependence on
the number of subpopulations.
",0,0,0,0,1,0
5404,Unravelling Airbnb Predicting Price for New Listing,"  This paper analyzes Airbnb listings in the city of San Francisco to better
understand how different attributes such as bedrooms, location, house type
amongst others can be used to accurately predict the price of a new listing
that optimal in terms of the host's profitability yet affordable to their
guests. This model is intended to be helpful to the internal pricing tools that
Airbnb provides to its hosts. Furthermore, additional analysis is performed to
ascertain the likelihood of a listings availability for potential guests to
consider while making a booking. The analysis begins with exploring and
examining the data to make necessary transformations that can be conducive for
a better understanding of the problem at large while helping us make
hypothesis. Moving further, machine learning models are built that are
intuitive to use to validate the hypothesis on pricing and availability and run
experiments in that context to arrive at a viable solution. The paper then
concludes with a discussion on the business implications, associated risks and
future scope.
",0,0,0,0,0,1
534,A Belief Propagation Algorithm for Multipath-Based SLAM,"  We present a simultaneous localization and mapping (SLAM) algorithm that is
based on radio signals and the association of specular multipath components
(MPCs) with geometric features. Especially in indoor scenarios, robust
localization from radio signals is challenging due to diffuse multipath
propagation, unknown MPC-feature association, and limited visibility of
features. In our approach, specular reflections at flat surfaces are described
in terms of virtual anchors (VAs) that are mirror images of the physical
anchors (PAs). The positions of these VAs and possibly also of the PAs are
unknown. We develop a Bayesian model of the SLAM problem including the unknown
MPC-VA/PA association. We represent this model by a factor graph, which enables
the use of the belief propagation (BP) scheme for efficient marginalization of
the joint posterior distribution. The resulting BP-based SLAM algorithm detects
the VAs associated with the PAs and estimates jointly the time-varying position
of the mobile agent and the positions of the VAs and possibly also of the PAs,
thereby leveraging the MPCs in the radio signal for improved accuracy and
robustness of agent localization. A core aspect of the algorithm is BP-based
probabilistic MPC-VA/PA association. Moreover, for improved initialization of
new VA positions, the states of unobserved potential VAs are modeled as a
random finite set and propagated in time by means of a ""zero-measurement""
probability hypothesis density filter. The proposed BP-based SLAM algorithm has
a low computational complexity and scales well in all relevant system
parameters. Experimental results using both synthetically generated
measurements and real ultra-wideband radio signals demonstrate the excellent
performance of the algorithm in challenging indoor environments.
",1,0,0,0,0,0
791,Amortized Inference Regularization,"  The variational autoencoder (VAE) is a popular model for density estimation
and representation learning. Canonically, the variational principle suggests to
prefer an expressive inference model so that the variational approximation is
accurate. However, it is often overlooked that an overly-expressive inference
model can be detrimental to the test set performance of both the amortized
posterior approximator and, more importantly, the generative density estimator.
In this paper, we leverage the fact that VAEs rely on amortized inference and
propose techniques for amortized inference regularization (AIR) that control
the smoothness of the inference model. We demonstrate that, by applying AIR, it
is possible to improve VAE generalization on both inference and generative
performance. Our paper challenges the belief that amortized inference is simply
a mechanism for approximating maximum likelihood training and illustrates that
regularization of the amortization family provides a new direction for
understanding and improving generalization in VAEs.
",0,0,0,1,0,0
782,On bifibrations of model categories,"  In this article, we develop a notion of Quillen bifibration which combines
the two notions of Grothendieck bifibration and of Quillen model structure. In
particular, given a bifibration $p:\mathcal E\to\mathcal B$, we describe when a
family of model structures on the fibers $\mathcal E_A$ and on the basis
category $\mathcal B$ combines into a model structure on the total category
$\mathcal E$, such that the functor $p$ preserves cofibrations, fibrations and
weak equivalences. Using this Grothendieck construction for model structures,
we revisit the traditional definition of Reedy model structures, and possible
generalizations, and exhibit their bifibrational nature.
",1,0,1,0,0,0
319,Retrospective Higher-Order Markov Processes for User Trails,"  Users form information trails as they browse the web, checkin with a
geolocation, rate items, or consume media. A common problem is to predict what
a user might do next for the purposes of guidance, recommendation, or
prefetching. First-order and higher-order Markov chains have been widely used
methods to study such sequences of data. First-order Markov chains are easy to
estimate, but lack accuracy when history matters. Higher-order Markov chains,
in contrast, have too many parameters and suffer from overfitting the training
data. Fitting these parameters with regularization and smoothing only offers
mild improvements. In this paper we propose the retrospective higher-order
Markov process (RHOMP) as a low-parameter model for such sequences. This model
is a special case of a higher-order Markov chain where the transitions depend
retrospectively on a single history state instead of an arbitrary combination
of history states. There are two immediate computational advantages: the number
of parameters is linear in the order of the Markov chain and the model can be
fit to large state spaces. Furthermore, by providing a specific structure to
the higher-order chain, RHOMPs improve the model accuracy by efficiently
utilizing history states without risks of overfitting the data. We demonstrate
how to estimate a RHOMP from data and we demonstrate the effectiveness of our
method on various real application datasets spanning geolocation data, review
sequences, and business locations. The RHOMP model uniformly outperforms
higher-order Markov chains, Kneser-Ney regularization, and tensor
factorizations in terms of prediction accuracy.
",1,0,0,1,0,0
260,On Convergence Rate of a Continuous-Time Distributed Self-Appraisal Model with Time-Varying Relative Interaction Matrices,"  This paper studies a recently proposed continuous-time distributed
self-appraisal model with time-varying interactions among a network of $n$
individuals which are characterized by a sequence of time-varying relative
interaction matrices. The model describes the evolution of the
social-confidence levels of the individuals via a reflected appraisal mechanism
in real time. We first show by example that when the relative interaction
matrices are stochastic (not doubly stochastic), the social-confidence levels
of the individuals may not converge to a steady state. We then show that when
the relative interaction matrices are doubly stochastic, the $n$ individuals'
self-confidence levels will all converge to $1/n$, which indicates a democratic
state, exponentially fast under appropriate assumptions, and provide an
explicit expression of the convergence rate.
",0,0,1,0,0,0
264,Continuously tempered Hamiltonian Monte Carlo,"  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)
method for performing approximate inference in complex probabilistic models of
continuous variables. In common with many MCMC methods, however, the standard
HMC approach performs poorly in distributions with multiple isolated modes. We
present a method for augmenting the Hamiltonian system with an extra continuous
temperature control variable which allows the dynamic to bridge between
sampling a complex target distribution and a simpler unimodal base
distribution. This augmentation both helps improve mixing in multimodal targets
and allows the normalisation constant of the target distribution to be
estimated. The method is simple to implement within existing HMC code,
requiring only a standard leapfrog integrator. We demonstrate experimentally
that the method is competitive with annealed importance sampling and simulating
tempering methods at sampling from challenging multimodal distributions and
estimating their normalising constants.
",0,0,0,1,0,0
5256,Computational modeling approaches in gonadotropin signaling,"  Follicle-stimulating hormone (FSH) and luteinizing hormone (LH) play
essential roles in animal reproduction. They exert their function through
binding to their cognate receptors, which belong to the large family of G
protein-coupled receptors (GPCRs). This recognition at the plasma membrane
triggers a plethora of cellular events, whose processing and integration
ultimately lead to an adapted biological response. Understanding the nature and
the kinetics of these events is essential for innovative approaches in drug
discovery. The study and manipulation of such complex systems requires the use
of computational modeling approaches combined with robust in vitro functional
assays for calibration and validation. Modeling brings a detailed understanding
of the system and can also be used to understand why existing drugs do not work
as well as expected, and how to design more efficient ones.
",0,0,0,0,1,0
829,A Theoretical Perspective of Solving Phaseless Compressed Sensing via Its Nonconvex Relaxation,"  As a natural extension of compressive sensing and the requirement of some
practical problems, Phaseless Compressed Sensing (PCS) has been introduced and
studied recently. Many theoretical results have been obtained for PCS with the
aid of its convex relaxation. Motivated by successful applications of nonconvex
relaxed methods for solving compressive sensing, in this paper, we try to
investigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the
real context by the corresponding $\ell_p$-minimization with $p\in (0,1)$. We
show that there exists a constant $p^\ast\in (0,1]$ such that for any fixed
$p\in(0, p^\ast)$, every optimal solution to the $\ell_p$-minimization also
solves the concerned problem; and derive an expression of such a constant
$p^\ast$ by making use of the known data and the sparsity level of the
concerned problem. These provide a theoretical basis for solving this class of
problems via the corresponding $\ell_p$-minimization.
",0,0,1,0,0,0
725,Multiple Illumination Phaseless Super-Resolution (MIPS) with Applications To Phaseless DOA Estimation and Diffraction Imaging,"  Phaseless super-resolution is the problem of recovering an unknown signal
from measurements of the magnitudes of the low frequency Fourier transform of
the signal. This problem arises in applications where measuring the phase, and
making high-frequency measurements, are either too costly or altogether
infeasible. The problem is especially challenging because it combines the
difficult problems of phase retrieval and classical super-resolution
",1,0,1,0,0,0
570,Permutation Tests for Infection Graphs,"  We formulate and analyze a novel hypothesis testing problem for inferring the
edge structure of an infection graph. In our model, a disease spreads over a
network via contagion or random infection, where the random variables governing
the rates of contracting the disease from neighbors or random infection are
independent exponential random variables with unknown rate parameters. A subset
of nodes is also censored uniformly at random. Given the statuses of nodes in
the network, the goal is to determine the underlying graph. We present a
procedure based on permutation testing, and we derive sufficient conditions for
the validity of our test in terms of automorphism groups of the graphs
corresponding to the null and alternative hypotheses. Further, the test is
valid more generally for infection processes satisfying a basic symmetry
condition. Our test is easy to compute and does not involve estimating unknown
parameters governing the process. We also derive risk bounds for our
permutation test in a variety of settings, and motivate our test statistic in
terms of approximate equivalence to likelihood ratio testing and maximin tests.
We conclude with an application to real data from an HIV infection network.
",1,0,1,1,0,0
281,Non-Parametric Calibration of Probabilistic Regression,"  The task of calibration is to retrospectively adjust the outputs from a
machine learning model to provide better probability estimates on the target
variable. While calibration has been investigated thoroughly in classification,
it has not yet been well-established for regression tasks. This paper considers
the problem of calibrating a probabilistic regression model to improve the
estimated probability densities over the real-valued targets. We propose to
calibrate a regression model through the cumulative probability density, which
can be derived from calibrating a multi-class classifier. We provide three
non-parametric approaches to solve the problem, two of which provide empirical
estimates and the third providing smooth density estimates. The proposed
approaches are experimentally evaluated to show their ability to improve the
performance of regression models on the predictive likelihood.
",0,0,0,1,0,0
650,On fibering compact manifold over the circle,"  In this paper, we show that any compact manifold that carries a
SL(n;R)-foliation is fibered on the circle S^1.
",0,0,1,0,0,0
555,State-dependent Priority Scheduling for Networked Control Systems,"  Networked control systems (NCS) have attracted considerable attention in
recent years. While the stabilizability and optimal control of NCS for a given
communication system has already been studied extensively, the design of the
communication system for NCS has recently seen an increase in more thorough
investigation. In this paper, we address an optimal scheduling problem for a
set of NCS sharing a dedicated communication channel, providing performance
bounds and asymptotic stability. We derive a suboptimal scheduling policy with
dynamic state-based priorities calculated at the sensors, which are then used
for stateless priority queuing in the network, making it both scalable and
efficient to implement on routers or multi-layer switches. These properties are
beneficial towards leveraging existing IP networks for control, which will be a
crucial factor for the proliferation of wide-area NCS applications. By allowing
for an arbitrary number of concurrent transmissions, we are able to investigate
the relationship between available bandwidth, transmission rate, and delay. To
demonstrate the feasibility of our approach, we provide a proof-of-concept
implementation of the priority scheduler using real networking hardware.
",1,0,0,0,0,0
921,A decentralized proximal-gradient method with network independent step-sizes and separated convergence rates,"  This paper considers the problem of decentralized optimization with a
composite objective containing smooth and non-smooth terms. To solve the
problem, a proximal-gradient scheme is studied. Specifically, the smooth and
nonsmooth terms are dealt with by gradient update and proximal update,
respectively. The studied algorithm is closely related to a previous
decentralized optimization algorithm, PG-EXTRA [37], but has a few advantages.
First of all, in our new scheme, agents use uncoordinated step-sizes and the
stable upper bounds on step-sizes are independent from network topology. The
step-sizes depend on local objective functions, and they can be as large as
that of the gradient descent. Secondly, for the special case without non-smooth
terms, linear convergence can be achieved under the strong convexity
assumption. The dependence of the convergence rate on the objective functions
and the network are separated, and the convergence rate of our new scheme is as
good as one of the two convergence rates that match the typical rates for the
general gradient descent and the consensus averaging. We also provide some
numerical experiments to demonstrate the efficacy of the introduced algorithms
and validate our theoretical discoveries.
",0,0,1,1,0,0
417,Spaces of orders of some one-relator groups,"  We show that certain orderable groups admit no isolated left orders. The
groups we consider are cyclic amalgamations of a free group with a general
orderable group, the HNN extensions of free groups over cyclic subgroups, and a
particular class of one-relator groups. In order to prove the results about
orders, we develop perturbation techniques for actions of these groups on the
line.
",0,0,1,0,0,0
575,Language Modeling by Clustering with Word Embeddings for Text Readability Assessment,"  We present a clustering-based language model using word embeddings for text
readability prediction. Presumably, an Euclidean semantic space hypothesis
holds true for word embeddings whose training is done by observing word
co-occurrences. We argue that clustering with word embeddings in the metric
space should yield feature representations in a higher semantic space
appropriate for text regression. Also, by representing features in terms of
histograms, our approach can naturally address documents of varying lengths. An
empirical evaluation using the Common Core Standards corpus reveals that the
features formed on our clustering-based language model significantly improve
the previously known results for the same corpus in readability prediction. We
also evaluate the task of sentence matching based on semantic relatedness using
the Wiki-SimpleWiki corpus and find that our features lead to superior matching
performance.
",1,0,0,0,0,0
309,Metastable Markov chains: from the convergence of the trace to the convergence of the finite-dimensional distributions,"  We consider continuous-time Markov chains which display a family of wells at
the same depth. We provide sufficient conditions which entail the convergence
of the finite-dimensional distributions of the order parameter to the ones of a
finite state Markov chain. We also show that the state of the process can be
represented as a time-dependent convex combination of metastable states, each
of which is supported on one well.
",0,0,1,0,0,0
523,Bistable reaction equations with doubly nonlinear diffusion,"  Reaction-diffusion equations appear in biology and chemistry, and combine
linear diffusion with different kind of reaction terms. Some of them are
remarkable from the mathematical point of view, since they admit families of
travelling waves that describe the asymptotic behaviour of a larger class of
solutions $0\leq u(x,t)\leq 1$ of the problem posed in the real line. We
investigate here the existence of waves with constant propagation speed, when
the linear diffusion is replaced by the ""slow"" doubly nonlinear diffusion. In
the present setting we consider bistable reaction terms, which present
interesting differences w.r.t. the Fisher-KPP framework recently studied in
\cite{AA-JLV:art}. We find different families of travelling waves that are
employed to describe the wave propagation of more general solutions and to
study the stability/instability of the steady states, even when we extend the
study to several space dimensions. A similar study is performed in the critical
case that we call ""pseudo-linear"", i.e., when the operator is still nonlinear
but has homogeneity one. With respect to the classical model and the
""pseudo-linear"" case, the travelling waves of the ""slow"" diffusion setting
exhibit free boundaries. \\ Finally, as a complement of \cite{AA-JLV:art}, we
study the asymptotic behaviour of more general solutions in the presence of a
""heterozygote superior"" reaction function and doubly nonlinear diffusion
(""slow"" and ""pseudo-linear"").
",0,0,1,0,0,0
586,Triplet Network with Attention for Speaker Diarization,"  In automatic speech processing systems, speaker diarization is a crucial
front-end component to separate segments from different speakers. Inspired by
the recent success of deep neural networks (DNNs) in semantic inferencing,
triplet loss-based architectures have been successfully used for this problem.
However, existing work utilizes conventional i-vectors as the input
representation and builds simple fully connected networks for metric learning,
thus not fully leveraging the modeling power of DNN architectures. This paper
investigates the importance of learning effective representations from the
sequences directly in metric learning pipelines for speaker diarization. More
specifically, we propose to employ attention models to learn embeddings and the
metric jointly in an end-to-end fashion. Experiments are conducted on the
CALLHOME conversational speech corpus. The diarization results demonstrate
that, besides providing a unified model, the proposed approach achieves
improved performance when compared against existing approaches.
",0,0,0,1,0,0
901,Nanostructured complex oxides as a route towards thermal behavior in artificial spin ice systems,"  We have used soft x-ray photoemission electron microscopy to image the
magnetization of single domain La$_{0.7}$Sr$_{0.3}$MnO$_{3}$ nano-islands
arranged in geometrically frustrated configurations such as square ice and
kagome ice geometries. Upon thermal randomization, ensembles of nano-islands
with strong inter-island magnetic coupling relax towards low-energy
configurations. Statistical analysis shows that the likelihood of ensembles
falling into low-energy configurations depends strongly on the annealing
temperature. Annealing to just below the Curie temperature of the ferromagnetic
film (T$_{C}$ = 338 K) allows for a much greater probability of achieving low
energy configurations as compared to annealing above the Curie temperature. At
this thermally active temperature of 325 K, the ensemble of ferromagnetic
nano-islands explore their energy landscape over time and eventually transition
to lower energy states as compared to the frozen-in configurations obtained
upon cooling from above the Curie temperature. Thus, this materials system
allows for a facile method to systematically study thermal evolution of
artificial spin ice arrays of nano-islands at temperatures modestly above room
temperature.
",0,1,0,0,0,0
424,Distributive Aronszajn trees,"  Ben-David and Shelah proved that if $\lambda$ is a singular strong-limit
cardinal and $2^\lambda=\lambda^+$, then $\square^*_\lambda$ entails the
existence of a normal $\lambda$-distributive $\lambda^+$-Aronszajn tree. Here,
it is proved that the same conclusion remains valid after replacing the
hypothesis $\square^*_\lambda$ by $\square(\lambda^+,{<}\lambda)$.
As $\square(\lambda^+,{<}\lambda)$ does not impose a bound on the order-type
of the witnessing clubs, our construction is necessarily different from that of
Ben-David and Shelah, and instead uses walks on ordinals augmented with club
guessing.
A major component of this work is the study of postprocessing functions and
their effect on square sequences. A byproduct of this study is the finding that
for $\kappa$ regular uncountable, $\square(\kappa)$ entails the existence of a
partition of $\kappa$ into $\kappa$ many fat sets. When contrasted with a
classic model of Magidor, this shows that it is equiconsistent with the
existence of a weakly compact cardinal that $\omega_2$ cannot be split into two
fat sets.
",0,0,1,0,0,0
5823,Evaluating Overfit and Underfit in Models of Network Community Structure,"  A common data mining task on networks is community detection, which seeks an
unsupervised decomposition of a network into structural groups based on
statistical regularities in the network's connectivity. Although many methods
exist, the No Free Lunch theorem for community detection implies that each
makes some kind of tradeoff, and no algorithm can be optimal on all inputs.
Thus, different algorithms will over or underfit on different inputs, finding
more, fewer, or just different communities than is optimal, and evaluation
methods that use a metadata partition as a ground truth will produce misleading
conclusions about general accuracy. Here, we present a broad evaluation of over
and underfitting in community detection, comparing the behavior of 16
state-of-the-art community detection algorithms on a novel and structurally
diverse corpus of 406 real-world networks. We find that (i) algorithms vary
widely both in the number of communities they find and in their corresponding
composition, given the same input, (ii) algorithms can be clustered into
distinct high-level groups based on similarities of their outputs on real-world
networks, and (iii) these differences induce wide variation in accuracy on link
prediction and link description tasks. We introduce a new diagnostic for
evaluating overfitting and underfitting in practice, and use it to roughly
divide community detection methods into general and specialized learning
algorithms. Across methods and inputs, Bayesian techniques based on the
stochastic block model and a minimum description length approach to
regularization represent the best general learning approach, but can be
outperformed under specific circumstances. These results introduce both a
theoretically principled approach to evaluate over and underfitting in models
of network community structure and a realistic benchmark by which new methods
may be evaluated and compared.
",1,0,0,1,1,0
907,Character Networks and Book Genre Classification,"  We compare the social character networks of biographical, legendary and
fictional texts, in search for marks of genre differentiation. We examine the
degree distribution of character appearance and find a power law that does not
depend on the literary genre or historical content. We also analyze local and
global complex networks measures, in particular, correlation plots between the
recently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and
Closeness centralities. Assortativity plots, which previous literature claims
to separate fictional from real social networks, were also studied. We've found
no relevant differences in the books for these network measures and we give a
plausible explanation why the previous assortativity result is not correct.
",1,1,0,0,0,0
13877,Extended opportunity cost model to find near equilibrium electricity prices under non-convexities,"  This paper finds near equilibrium prices for electricity markets with
nonconvexities due to binary variables, in order to reduce the market
participants' opportunity costs, such as generators' unrecovered costs. The
opportunity cost is defined as the difference between the profit when the
instructions of the market operator are followed and when the market
participants can freely make their own decisions based on the market prices. We
use the minimum complementarity approximation to the minimum total opportunity
cost (MTOC) model, from previous research, with tests on a much more realistic
unit commitment (UC) model than in previous research, including features such
as reserve requirements, ramping constraints, and minimum up and down times.
The developed model incorporates flexible price responsive demand, as in
previous research, but since not all demand is price responsive, we consider
the more realistic case that total demand is a mixture of fixed and flexible.
Another improvement over previous MTOC research is computational: whereas the
previous research had nonconvex terms among the objective function's continuous
variables, we convert the objective to an equivalent form that contains only
linear and convex quadratic terms in the continuous variables. We compare the
unit commitment model with the standard social welfare optimization version of
UC, in a series of sensitivity analyses, varying flexible demand to represent
varying degrees of future penetration of electric vehicles and smart
appliances, different ratios of generation availability, and different values
of transmission line capacities to consider possible congestion. The minimum
total opportunity cost and social welfare solutions are mostly very close in
different scenarios, except in some extreme cases.
",0,0,0,0,0,1
6932,Burst Synchronization in A Scale-Free Neuronal Network with Inhibitory Spike-Timing-Dependent Plasticity,"  We are concerned about burst synchronization (BS), related to neural
information processes in health and disease, in the Barabási-Albert
scale-free network (SFN) composed of inhibitory bursting Hindmarsh-Rose
neurons. This inhibitory neuronal population has adaptive dynamic synaptic
strengths governed by the inhibitory spike-timing-dependent plasticity (iSTDP).
In previous works without considering iSTDP, BS was found to appear in a range
of noise intensities for fixed synaptic inhibition strengths. In contrast, in
our present work, we take into consideration iSTDP and investigate its effect
on BS by varying the noise intensity. Our new main result is to find occurrence
of a Matthew effect in inhibitory synaptic plasticity: good BS gets better via
LTD, while bad BS get worse via LTP. This kind of Matthew effect in inhibitory
synaptic plasticity is in contrast to that in excitatory synaptic plasticity
where good (bad) synchronization gets better (worse) via LTP (LTD). We note
that, due to inhibition, the roles of LTD and LTP in inhibitory synaptic
plasticity are reversed in comparison with those in excitatory synaptic
plasticity. Moreover, emergences of LTD and LTP of synaptic inhibition
strengths are intensively investigated via a microscopic method based on the
distributions of time delays between the pre- and the post-synaptic burst onset
times. Finally, in the presence of iSTDP we investigate the effects of network
architecture on BS by varying the symmetric attachment degree $l^*$ and the
asymmetry parameter $\Delta l$ in the SFN.
",0,0,0,0,1,0
478,"Single and Multiple Vortex Rings in Three-Dimensional Bose-Einstein Condensates: Existence, Stability and Dynamics","  In the present work, we explore the existence, stability and dynamics of
single and multiple vortex ring states that can arise in Bose-Einstein
condensates. Earlier works have illustrated the bifurcation of such states, in
the vicinity of the linear limit, for isotropic or anisotropic
three-dimensional harmonic traps. Here, we extend these states to the regime of
large chemical potentials, the so-called Thomas-Fermi limit, and explore their
properties such as equilibrium radii and inter-ring distance, for multi-ring
states, as well as their vibrational spectra and possible instabilities. In
this limit, both the existence and stability characteristics can be partially
traced to a particle picture that considers the rings as individual particles
oscillating within the trap and interacting pairwise with one another. Finally,
we examine some representative instability scenarios of the multi-ring dynamics
including breakup and reconnections, as well as the transient formation of
vortex lines.
",0,1,0,0,0,0
682,Bias Reduction in Instrumental Variable Estimation through First-Stage Shrinkage,"  The two-stage least-squares (2SLS) estimator is known to be biased when its
first-stage fit is poor. I show that better first-stage prediction can
alleviate this bias. In a two-stage linear regression model with Normal noise,
I consider shrinkage in the estimation of the first-stage instrumental variable
coefficients. For at least four instrumental variables and a single endogenous
regressor, I establish that the standard 2SLS estimator is dominated with
respect to bias. The dominating IV estimator applies James-Stein type shrinkage
in a first-stage high-dimensional Normal-means problem followed by a
control-function approach in the second stage. It preserves invariances of the
structural instrumental variable equations.
",0,0,1,1,0,0
715,"Universality in numerical computation with random data. Case studies, analytic results and some speculations","  We discuss various universality aspects of numerical computations using
standard algorithms. These aspects include empirical observations and rigorous
results. We also make various speculations about computation in a broader
sense.
",0,1,1,0,0,0
693,A Ball Breaking Away from a Fluid,"  We consider the withdrawal of a ball from a fluid reservoir to understand the
longevity of the connection between that ball and the fluid it breaks away
from, at intermediate Reynolds numbers. Scaling arguments based on the
processes observed as the ball interacts with the fluid surface were applied to
the `pinch-off time', when the ball breaks its connection with the fluid from
which it has been withdrawn, measured experimentally. At the lowest Reynolds
numbers tested, pinch-off occurs in a `surface seal' close to the reservoir
surface, where at larger Reynolds numbers pinch-off occurs in an `ejecta seal'
close to the ball. Our scaling analysis shows that the connection between ball
and fluid is controlled by the fluid film draining from the ball as it
continues to be winched away from the fluid reservoir. The draining flow itself
depends on the amount of fluid coating the ball on exit from the reservoir. We
consider the possibilities that this coating was created through: a surface
tension driven Landau Levitch Derjaguin wetting of the surface; a
visco-inertial quick coating; or alternatively through the inertia of the fluid
moving with the ball through the reservoir. We show that although the pinch-off
mechanism is controlled by viscosity, the coating mechanism is governed by a
different length and timescale, dictated by the inertial added mass of the ball
when submersed.
",0,1,0,0,0,0
618,On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis,"  In this paper, we study random subsampling of Gaussian process regression,
one of the simplest approximation baselines, from a theoretical perspective.
Although subsampling discards a large part of training data, we show provable
guarantees on the accuracy of the predictive mean/variance and its
generalization ability. For analysis, we consider embedding kernel matrices
into graphons, which encapsulate the difference of the sample size and enables
us to evaluate the approximation and generalization errors in a unified manner.
The experimental results show that the subsampling approximation achieves a
better trade-off regarding accuracy and runtime than the Nyström and random
Fourier expansion methods.
",1,0,0,1,0,0
678,Hierarchical loss for classification,"  Failing to distinguish between a sheepdog and a skyscraper should be worse
and penalized more than failing to distinguish between a sheepdog and a poodle;
after all, sheepdogs and poodles are both breeds of dogs. However, existing
metrics of failure (so-called ""loss"" or ""win"") used in textual or visual
classification/recognition via neural networks seldom view a sheepdog as more
similar to a poodle than to a skyscraper. We define a metric that, inter alia,
can penalize failure to distinguish between a sheepdog and a skyscraper more
than failure to distinguish between a sheepdog and a poodle. Unlike previously
employed possibilities, this metric is based on an ultrametric tree associated
with any given tree organization into a semantically meaningful hierarchy of a
classifier's classes.
",1,0,0,1,0,0
804,Essentially No Barriers in Neural Network Energy Landscape,"  Training neural networks involves finding minima of a high-dimensional
non-convex loss function. Knowledge of the structure of this energy landscape
is sparse. Relaxing from linear interpolations, we construct continuous paths
between minima of recent neural network architectures on CIFAR10 and CIFAR100.
Surprisingly, the paths are essentially flat in both the training and test
landscapes. This implies that neural networks have enough capacity for
structural changes, or that these changes are small between minima. Also, each
minimum has at least one vanishing Hessian eigenvalue in addition to those
resulting from trivial invariance.
",0,0,0,1,0,0
951,Minimal Effort Back Propagation for Convolutional Neural Networks,"  As traditional neural network consumes a significant amount of computing
resources during back propagation, \citet{Sun2017mePropSB} propose a simple yet
effective technique to alleviate this problem. In this technique, only a small
subset of the full gradients are computed to update the model parameters. In
this paper we extend this technique into the Convolutional Neural Network(CNN)
to reduce calculation in back propagation, and the surprising results verify
its validity in CNN: only 5\% of the gradients are passed back but the model
still achieves the same effect as the traditional CNN, or even better. We also
show that the top-$k$ selection of gradients leads to a sparse calculation in
back propagation, which may bring significant computational benefits for high
computational complexity of convolution operation in CNN.
",1,0,0,1,0,0
813,Fulde-Ferrell-Larkin-Ovchinnikov state in spin-orbit-coupled superconductors,"  We show that in the presence of magnetic field, two superconducting phases
with the center-of-mass momentum of Cooper pair parallel to the magnetic field
are induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,
at small magnetic field, the center-of-mass momentum is induced due to the
energy-spectrum distortion and no unpairing region with vanishing singlet
correlation appears. We refer to this superconducting state as the drift-BCS
state. By further increasing the magnetic field, the superconducting state
falls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the
unpairing regions. The observed abrupt enhancement of the center-of-mass
momenta and suppression on the order parameters during the crossover indicate
the first-order phase transition. Enhanced Pauli limit and hence enlarged
magnetic-field regime of the Fulde-Ferrell-Larkin-Ovchinnikov state, due to the
spin-flip terms of the spin-orbit coupling, are revealed. We also address the
triplet correlations induced by the spin-orbit coupling, and show that the
Cooper-pair spin polarizations, generated by the magnetic field and
center-of-mass momentum with the triplet correlations, exhibit totally
different magnetic-field dependences between the drift-BCS and
Fulde-Ferrell-Larkin-Ovchinnikov states.
",0,1,0,0,0,0
262,Block CUR: Decomposing Matrices using Groups of Columns,"  A common problem in large-scale data analysis is to approximate a matrix
using a combination of specifically sampled rows and columns, known as CUR
decomposition. Unfortunately, in many real-world environments, the ability to
sample specific individual rows or columns of the matrix is limited by either
system constraints or cost. In this paper, we consider matrix approximation by
sampling predefined \emph{blocks} of columns (or rows) from the matrix. We
present an algorithm for sampling useful column blocks and provide novel
guarantees for the quality of the approximation. This algorithm has application
in problems as diverse as biometric data analysis to distributed computing. We
demonstrate the effectiveness of the proposed algorithms for computing the
Block CUR decomposition of large matrices in a distributed setting with
multiple nodes in a compute cluster, where such blocks correspond to columns
(or rows) of the matrix stored on the same node, which can be retrieved with
much less overhead than retrieving individual columns stored across different
nodes. In the biometric setting, the rows correspond to different users and
columns correspond to users' biometric reaction to external stimuli, {\em
e.g.,}~watching video content, at a particular time instant. There is
significant cost in acquiring each user's reaction to lengthy content so we
sample a few important scenes to approximate the biometric response. An
individual time sample in this use case cannot be queried in isolation due to
the lack of context that caused that biometric reaction. Instead, collections
of time segments ({\em i.e.,} blocks) must be presented to the user. The
practical application of these algorithms is shown via experimental results
using real-world user biometric data from a content testing environment.
",1,0,0,1,0,0
544,The Correct Application of Variance Concept in Measurement Theory,"  The existing measurement theory interprets the variance as the dispersion of
measured value, which is actually contrary to a general mathematical knowledge
that the variance of a constant is 0. This paper will fully demonstrate that
the variance in measurement theory is actually the evaluation of probability
interval of an error instead of the dispersion of a measured value, point out
the key point of mistake in the existing interpretation, and fully interpret a
series of changes in conceptual logic and processing method brought about by
this new concept.
",0,0,1,1,0,0
529,Failures of Gradient-Based Deep Learning,"  In recent years, Deep Learning has become the go-to solution for a broad
range of applications, often outperforming state-of-the-art. However, it is
important, for both theoreticians and practitioners, to gain a deeper
understanding of the difficulties and limitations associated with common
approaches and algorithms. We describe four types of simple problems, for which
the gradient-based algorithms commonly used in deep learning either fail or
suffer from significant difficulties. We illustrate the failures through
practical experiments, and provide theoretical insights explaining their
source, and how they might be remedied.
",1,0,0,1,0,0
6789,Improving Protein Gamma-Turn Prediction Using Inception Capsule Networks,"  Protein gamma-turn prediction is useful in protein function studies and
experimental design. Several methods for gamma-turn prediction have been
developed, but the results were unsatisfactory with Matthew correlation
coefficients (MCC) around 0.2-0.4. One reason for the low prediction accuracy
is the limited capacity of the methods; in particular, the traditional
machine-learning methods like SVM may not extract high-level features well to
distinguish between turn or non-turn. Hence, it is worthwhile exploring new
machine-learning methods for the prediction. A cutting-edge deep neural
network, named Capsule Network (CapsuleNet), provides a new opportunity for
gamma-turn prediction. Even when the number of input samples is relatively
small, the capsules from CapsuleNet are very effective to extract high-level
features for classification tasks. Here, we propose a deep inception capsule
network for gamma-turn prediction. Its performance on the gamma-turn benchmark
GT320 achieved an MCC of 0.45, which significantly outperformed the previous
best method with an MCC of 0.38. This is the first gamma-turn prediction method
utilizing deep neural networks. Also, to our knowledge, it is the first
published bioinformatics application utilizing capsule network, which will
provides a useful example for the community.
",0,0,0,0,1,0
1193,Mean-variance portfolio selection under partial information with drift uncertainty,"  This paper studies a mean-variance portfolio selection problem under partial
information with drift uncertainty. It is proved that all the contingent claims
in this model are attainable in the sense of Xiong and Zhou. Further, we
propose a numerical scheme to approximate the optimal portfolio. Malliavin
calculus and the strong law of large numbers play important roles in this
scheme.
",0,0,0,0,0,1
4034,Reconfiguration of Brain Network between Resting-state and Oddball Paradigm,"  The oddball paradigm is widely applied to the investigation of multiple
cognitive functions. Prior studies have explored the cortical oscillation and
power spectral differing from the resting-state conduction to oddball paradigm,
but whether brain networks existing the significant difference is still
unclear. Our study addressed how the brain reconfigures its architecture from a
resting-state condition (i.e., baseline) to P300 stimulus task in the visual
oddball paradigm. In this study, electroencephalogram (EEG) datasets were
collected from 24 postgraduate students, who were required to only mentally
count the number of target stimulus; afterwards the functional EEG networks
constructed in different frequency bands were compared between baseline and
oddball task conditions to evaluate the reconfiguration of functional network
in the brain. Compared to the baseline, our results showed the significantly (p
< 0.05) enhanced delta/theta EEG connectivity and decreased alpha default mode
network in the progress of brain reconfiguration to the P300 task. Furthermore,
the reconfigured coupling strengths were demonstrated to relate to P300
amplitudes, which were then regarded as input features to train a classifier to
differentiate the high and low P300 amplitudes groups with an accuracy of
77.78%. The findings of our study help us to understand the changes of
functional brain connectivity from resting-state to oddball stimulus task, and
the reconfigured network pattern has the potential for the selection of good
subjects for P300-based brain- computer interface.
",0,0,0,0,1,0
345,Deep Learning for Classification Tasks on Geospatial Vector Polygons,"  In this paper, we evaluate the accuracy of deep learning approaches on
geospatial vector geometry classification tasks. The purpose of this evaluation
is to investigate the ability of deep learning models to learn from geometry
coordinates directly. Previous machine learning research applied to geospatial
polygon data did not use geometries directly, but derived properties thereof.
These are produced by way of extracting geometry properties such as Fourier
descriptors. Instead, our introduced deep neural net architectures are able to
learn on sequences of coordinates mapped directly from polygons. In three
classification tasks we show that the deep learning architectures are
competitive with common learning algorithms that require extracted features.
",0,0,0,1,0,0
377,Space dependent adhesion forces mediated by transient elastic linkages : new convergence and global existence results,"  In the first part of this work we show the convergence with respect to an
asymptotic parameter {\epsilon} of a delayed heat equation. It represents a
mathematical extension of works considered previously by the authors [Milisic
et al. 2011, Milisic et al. 2016]. Namely, this is the first result involving
delay operators approximating protein linkages coupled with a spatial elliptic
second order operator. For the sake of simplicity we choose the Laplace
operator, although more general results could be derived. The main arguments
are (i) new energy estimates and (ii) a stability result extended from the
previous work to this more involved context. They allow to prove convergence of
the delay operator to a friction term together with the Laplace operator in the
same asymptotic regime considered without the space dependence in [Milisic et
al, 2011]. In a second part we extend fixed-point results for the fully
non-linear model introduced in [Milisic et al, 2016] and prove global existence
in time. This shows that the blow-up scenario observed previously does not
occur. Since the latter result was interpreted as a rupture of adhesion forces,
we discuss the possibility of bond breaking both from the analytic and
numerical point of view.
",0,0,1,0,0,0
3756,How strong are correlations in strongly recurrent neuronal networks?,"  Cross-correlations in the activity in neural networks are commonly used to
characterize their dynamical states and their anatomical and functional
organizations. Yet, how these latter network features affect the spatiotemporal
structure of the correlations in recurrent networks is not fully understood.
Here, we develop a general theory for the emergence of correlated neuronal
activity from the dynamics in strongly recurrent networks consisting of several
populations of binary neurons. We apply this theory to the case in which the
connectivity depends on the anatomical or functional distance between the
neurons. We establish the architectural conditions under which the system
settles into a dynamical state where correlations are strong, highly robust and
spatially modulated. We show that such strong correlations arise if the network
exhibits an effective feedforward structure. We establish how this feedforward
structure determines the way correlations scale with the network size and the
degree of the connectivity. In networks lacking an effective feedforward
structure correlations are extremely small and only weakly depend on the number
of connections per neuron. Our work shows how strong correlations can be
consistent with highly irregular activity in recurrent networks, two key
features of neuronal dynamics in the central nervous system.
",0,0,0,0,1,0
2109,Spatial analysis of airborne laser scanning point clouds for predicting forest variables,"  With recent developments in remote sensing technologies, plot-level forest
resources can be predicted utilizing airborne laser scanning (ALS). The
prediction is often assisted by mostly vertical summaries of the ALS point
clouds. We present a spatial analysis of the point cloud by studying the
horizontal distribution of the pulse returns through canopy height models
thresholded at different height levels. The resulting patterns of patches of
vegetation and gabs on each layer are summarized to spatial ALS features. We
propose new features based on the Euler number, which is the number of patches
minus the number of gaps, and the empty-space function, which is a spatial
summary function of the gab space. The empty-space function is also used to
describe differences in the gab structure between two different layers. We
illustrate usefulness of the proposed spatial features for predicting different
forest variables that summarize the spatial structure of forests or their
breast height diameter distribution. We employ the proposed spatial features,
in addition to commonly used features from literature, in the well-known k-nn
estimation method to predict the forest variables. We present the methodology
on the example of a study site in Central Finland.
",0,0,0,1,1,0
251,Structured low rank decomposition of multivariate Hankel matrices,"  We study the decomposition of a multivariate Hankel matrix H\_$\sigma$ as a
sum of Hankel matrices of small rank in correlation with the decomposition of
its symbol $\sigma$ as a sum of polynomial-exponential series. We present a new
algorithm to compute the low rank decomposition of the Hankel operator and the
decomposition of its symbol exploiting the properties of the associated
Artinian Gorenstein quotient algebra A\_$\sigma$. A basis of A\_$\sigma$ is
computed from the Singular Value Decomposition of a sub-matrix of the Hankel
matrix H\_$\sigma$. The frequencies and the weights are deduced from the
generalized eigenvectors of pencils of shifted sub-matrices of H $\sigma$.
Explicit formula for the weights in terms of the eigenvectors avoid us to solve
a Vandermonde system. This new method is a multivariate generalization of the
so-called Pencil method for solving Prony-type decomposition problems. We
analyse its numerical behaviour in the presence of noisy input moments, and
describe a rescaling technique which improves the numerical quality of the
reconstruction for frequencies of high amplitudes. We also present a new Newton
iteration, which converges locally to the closest multivariate Hankel matrix of
low rank and show its impact for correcting errors on input moments.
",0,0,1,0,0,0
592,The Linear Point: A cleaner cosmological standard ruler,"  We show how a characteristic length scale imprinted in the galaxy two-point
correlation function, dubbed the ""linear point"", can serve as a comoving
cosmological standard ruler. In contrast to the Baryon Acoustic Oscillation
peak location, this scale is constant in redshift and is unaffected by
non-linear effects to within $0.5$ percent precision. We measure the location
of the linear point in the galaxy correlation function of the LOWZ and CMASS
samples from the Twelfth Data Release (DR12) of the Baryon Oscillation
Spectroscopic Survey (BOSS) collaboration. We combine our linear-point
measurement with cosmic-microwave-background constraints from the Planck
satellite to estimate the isotropic-volume distance $D_{V}(z)$, without relying
on a model-template or reconstruction method. We find $D_V(0.32)=1264\pm 28$
Mpc and $D_V(0.57)=2056\pm 22$ Mpc respectively, consistent with the quoted
values from the BOSS collaboration. This remarkable result suggests that all
the distance information contained in the baryon acoustic oscillations can be
conveniently compressed into the single length associated with the linear
point.
",0,1,0,0,0,0
705,Common change point estimation in panel data from the least squares and maximum likelihood viewpoints,"  We establish the convergence rates and asymptotic distributions of the common
break change-point estimators, obtained by least squares and maximum likelihood
in panel data models and compare their asymptotic variances. Our model
assumptions accommodate a variety of commonly encountered probability
distributions and, in particular, models of particular interest in econometrics
beyond the commonly analyzed Gaussian model, including the zero-inflated
Poisson model for count data, and the probit and tobit models. We also provide
novel results for time dependent data in the signal-plus-noise model, with
emphasis on a wide array of noise processes, including Gaussian process,
MA$(\infty)$ and $m$-dependent processes. The obtained results show that
maximum likelihood estimation requires a stronger signal-to-noise model
identifiability condition compared to its least squares counterpart. Finally,
since there are three different asymptotic regimes that depend on the behavior
of the norm difference of the model parameters before and after the change
point, which cannot be realistically assumed to be known, we develop a novel
data driven adaptive procedure that provides valid confidence intervals for the
common break, without requiring a priori knowledge of the asymptotic regime the
problem falls in.
",0,0,1,1,0,0
465,Vibrational Density Matrix Renormalization Group,"  Variational approaches for the calculation of vibrational wave functions and
energies are a natural route to obtain highly accurate results with
controllable errors. However, the unfavorable scaling and the resulting high
computational cost of standard variational approaches limit their application
to small molecules with only few vibrational modes. Here, we demonstrate how
the density matrix renormalization group (DMRG) can be exploited to optimize
vibrational wave functions (vDMRG) expressed as matrix product states. We study
the convergence of these calculations with respect to the size of the local
basis of each mode, the number of renormalized block states, and the number of
DMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for
small molecules that were intensively studied in the literature. We then
proceed to show that the complete fingerprint region of the sarcosyn-glycin
dipeptide can be calculated with vDMRG.
",0,1,0,0,0,0
4466,Preparation and Measurement in Quantum Memory Models,"  Quantum Cognition has delivered a number of models for semantic memory, but
to date these have tended to assume pure states and projective measurement.
Here we relax these assumptions. A quantum inspired model of human word
association experiments will be extended using a density matrix representation
of human memory and a POVM based upon non-ideal measurements. Our formulation
allows for a consideration of key terms like measurement and contextuality
within a rigorous modern approach. This approach both provides new conceptual
advances and suggests new experimental protocols.
",0,0,0,0,1,0
687,EnergyNet: Energy-based Adaptive Structural Learning of Artificial Neural Network Architectures,"  We present E NERGY N ET , a new framework for analyzing and building
artificial neural network architectures. Our approach adaptively learns the
structure of the networks in an unsupervised manner. The methodology is based
upon the theoretical guarantees of the energy function of restricted Boltzmann
machines (RBM) of infinite number of nodes. We present experimental results to
show that the final network adapts to the complexity of a given problem.
",1,0,0,0,0,0
305,Distinct evolutions of Weyl fermion quasiparticles and Fermi arcs with bulk band topology in Weyl semimetals,"  The Weyl semimetal phase is a recently discovered topological quantum state
of matter characterized by the presence of topologically protected degeneracies
near the Fermi level. These degeneracies are the source of exotic phenomena,
including the realization of chiral Weyl fermions as quasiparticles in the bulk
and the formation of Fermi arc states on the surfaces. Here, we demonstrate
that these two key signatures show distinct evolutions with the bulk band
topology by performing angle-resolved photoemission spectroscopy, supported by
first-principle calculations, on transition-metal monophosphides. While Weyl
fermion quasiparticles exist only when the chemical potential is located
between two saddle points of the Weyl cone features, the Fermi arc states
extend in a larger energy scale and are robust across the bulk Lifshitz
transitions associated with the recombination of two non-trivial Fermi surfaces
enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
points of opposite chirality. Therefore, in some systems (e.g. NbP),
topological Fermi arc states are preserved even if Weyl fermion quasiparticles
are absent in the bulk. Our findings not only provide insight into the
relationship between the exotic physical phenomena and the intrinsic bulk band
topology in Weyl semimetals, but also resolve the apparent puzzle of the
different magneto-transport properties observed in TaAs, TaP and NbP, where the
Fermi arc states are similar.
",0,1,0,0,0,0
411,The Calabi flow with rough initial data,"  In this paper, we prove that there exists a dimensional constant $\delta > 0$
such that given any background Kähler metric $\omega$, the Calabi flow with
initial data $u_0$ satisfying \begin{equation*} \partial \bar \partial u_0 \in
L^\infty (M) \text{ and } (1- \delta )\omega < \omega_{u_0} < (1+\delta
)\omega, \end{equation*} admits a unique short time solution and it becomes
smooth immediately, where $\omega_{u_0} : = \omega +\sqrt{-1}\partial
\bar\partial u_0$. The existence time depends on initial data $u_0$ and the
metric $\omega$. As a corollary, we get that Calabi flow has short time
existence for any initial data satisfying \begin{equation*} \partial \bar
\partial u_0 \in C^0(M) \text{ and } \omega_{u_0} > 0, \end{equation*} which
should be interpreted as a ""continuous Kähler metric"". A main technical
ingredient is Schauder-type estimates for biharmonic heat equation on
Riemannian manifolds with time weighted Hölder norms.
",0,0,1,0,0,0
685,Mean squared displacement and sinuosity of three-dimensional random search movements,"  Correlated random walks (CRW) have been used for a long time as a null model
for animal's random search movement in two dimensions (2D). An increasing
number of studies focus on animals' movement in three dimensions (3D), but the
key properties of CRW, such as the way the mean squared displacement is related
to the path length, are well known only in 1D and 2D. In this paper I derive
such properties for 3D CRW, in a consistent way with the expression of these
properties in 2D. This should allow 3D CRW to act as a null model when
analyzing actual 3D movements similarly to what is done in 2D
",0,0,0,0,1,0
783,Canonical sine and cosine Transforms For Integrable Boehmians,"  In this paper we define canonical sine and cosine transform, convolution
operations, prove convolution theorems in space of integrable functions on real
space. Further, obtain some results require to construct the spaces of
integrable Boehmians then extend this canonical sine and canonical cosine
transforms to space of integrable Boehmians and obtain their properties.
",0,0,1,0,0,0
7134,Asymptotics for Small Nonlinear Price Impact: a PDE Approach to the Multidimensional Case,"  We provide an asymptotic expansion of the value function of a
multidimensional utility maximization problem from consumption with small
non-linear price impact. In our model cross-impacts between assets are allowed.
In the limit for small price impact, we determine the asymptotic expansion of
the value function around its frictionless version. The leading order
correction is characterized by a nonlinear second order PDE related to an
ergodic control problem and a linear parabolic PDE. We illustrate our result on
a multivariate geometric Brownian motion price model.
",0,0,0,0,0,1
784,Deep Learning Sparse Ternary Projections for Compressed Sensing of Images,"  Compressed sensing (CS) is a sampling theory that allows reconstruction of
sparse (or compressible) signals from an incomplete number of measurements,
using of a sensing mechanism implemented by an appropriate projection matrix.
The CS theory is based on random Gaussian projection matrices, which satisfy
recovery guarantees with high probability; however, sparse ternary {0, -1, +1}
projections are more suitable for hardware implementation. In this paper, we
present a deep learning approach to obtain very sparse ternary projections for
compressed sensing. Our deep learning architecture jointly learns a pair of a
projection matrix and a reconstruction operator in an end-to-end fashion. The
experimental results on real images demonstrate the effectiveness of the
proposed approach compared to state-of-the-art methods, with significant
advantage in terms of complexity.
",1,0,0,1,0,0
2694,Stochastic Chemical Reaction Networks for Robustly Approximating Arbitrary Probability Distributions,"  We show that discrete distributions on the $d$-dimensional non-negative
integer lattice can be approximated arbitrarily well via the marginals of
stationary distributions for various classes of stochastic chemical reaction
networks. We begin by providing a class of detailed balanced networks and prove
that they can approximate any discrete distribution to any desired accuracy.
However, these detailed balanced constructions rely on the ability to
initialize a system precisely, and are therefore susceptible to perturbations
in the initial conditions. We therefore provide another construction based on
the ability to approximate point mass distributions and prove that this
construction is capable of approximating arbitrary discrete distributions for
any choice of initial condition. In particular, the developed models are
ergodic, so their limit distributions are robust to a finite number of
perturbations over time in the counts of molecules.
",0,0,0,0,1,0
503,Thermoelectric Cooperative Effect in Three-Terminal Elastic Transport through a Quantum Dot,"  The energy efficiency and power of a three-terminal thermoelectric nanodevice
are studied by considering elastic tunneling through a single quantum dot.
Facilitated by the three-terminal geometry, the nanodevice is able to generate
simultaneously two electrical powers by utilizing only one temperature bias.
These two electrical powers can add up constructively or destructively,
depending on their signs. It is demonstrated that the constructive addition
leads to the enhancement of both energy efficiency and output power for various
system parameters. In fact, such enhancement, dubbed as thermoelectric
cooperative effect, can lead to maximum efficiency and power no less than when
only one of the electrical power is harvested.
",0,1,0,0,0,0
267,High Dimensional Estimation and Multi-Factor Models,"  This paper re-investigates the estimation of multiple factor models relaxing
the convention that the number of factors is small and using a new approach for
identifying factors. We first obtain the collection of all possible factors and
then provide a simultaneous test, security by security, of which factors are
significant. Since the collection of risk factors is large and highly
correlated, high-dimension methods (including the LASSO and prototype
clustering) have to be used. The multi-factor model is shown to have a
significantly better fit than the Fama-French 5-factor model. Robustness tests
are also provided.
",0,0,0,1,0,1
868,Network of vertically c-oriented prism shaped InN nanowalls grown on c-GaN/sapphire template by chemical vapor deposition technique,"  Networks of vertically c-oriented prism shaped InN nanowalls, are grown on
c-GaN/sapphire templates using a CVD technique, where pure indium and ammonia
are used as metal and nitrogen precursors. A systematic study of the growth,
structural and electronic properties of these samples shows a preferential
growth of the islands along [11-20] and [0001] directions leading to the
formation of such a network structure, where the vertically [0001] oriented
tapered walls are laterally align along one of the three [11-20] directions.
Inclined facets of these walls are identified as r-planes [(1-102)-planes] of
wurtzite InN. Onset of absorption for these samples is observed to be higher
than the band gap of InN suggesting a high background carrier concentration in
this material. Study of the valence band edge through XPS indicates the
formation of positive depletion regions below the r-plane side facets of the
walls. This is in contrast with the observation for c-plane InN epilayers,
where electron accumulation is often reported below the top surface.
",0,1,0,0,0,0
266,Magnus integrators on multicore CPUs and GPUs,"  In the present paper we consider numerical methods to solve the discrete
Schrödinger equation with a time dependent Hamiltonian (motivated by problems
encountered in the study of spin systems). We will consider both short-range
interactions, which lead to evolution equations involving sparse matrices, and
long-range interactions, which lead to dense matrices. Both of these settings
show very different computational characteristics. We use Magnus integrators
for time integration and employ a framework based on Leja interpolation to
compute the resulting action of the matrix exponential. We consider both
traditional Magnus integrators (which are extensively used for these types of
problems in the literature) as well as the recently developed commutator-free
Magnus integrators and implement them on modern CPU and GPU (graphics
processing unit) based systems.
We find that GPUs can yield a significant speed-up (up to a factor of $10$ in
the dense case) for these types of problems. In the sparse case GPUs are only
advantageous for large problem sizes and the achieved speed-ups are more
modest. In most cases the commutator-free variant is superior but especially on
the GPU this advantage is rather small. In fact, none of the advantage of
commutator-free methods on GPUs (and on multi-core CPUs) is due to the
elimination of commutators. This has important consequences for the design of
more efficient numerical methods.
",1,1,0,0,0,0
241,Magnetocapillary self-assemblies: locomotion and micromanipulation along a liquid interface,"  This paper presents an overview and discussion of magnetocapillary
self-assemblies. New results are presented, in particular concerning the
possible development of future applications. These self-organizing structures
possess the notable ability to move along an interface when powered by an
oscillatory, uniform magnetic field. The system is constructed as follows. Soft
magnetic particles are placed on a liquid interface, and submitted to a
magnetic induction field. An attractive force due to the curvature of the
interface around the particles competes with an interaction between magnetic
dipoles. Ordered structures can spontaneously emerge from these conditions.
Furthermore, time-dependent magnetic fields can produce a wide range of dynamic
behaviours, including non-time-reversible deformation sequences that produce
translational motion at low Reynolds number. In other words, due to a
spontaneous breaking of time-reversal symmetry, the assembly can turn into a
surface microswimmer. Trajectories have been shown to be precisely
controllable. As a consequence, this system offers a way to produce microrobots
able to perform different tasks. This is illustrated in this paper by the
capture, transport and release of a floating cargo, and the controlled mixing
of fluids at low Reynolds number.
",0,1,0,0,0,0
6405,Fixed points of competitive threshold-linear networks,"  Threshold-linear networks (TLNs) are models of neural networks that consist
of simple, perceptron-like neurons and exhibit nonlinear dynamics that are
determined by the network's connectivity. The fixed points of a TLN, including
both stable and unstable equilibria, play a critical role in shaping its
emergent dynamics. In this work, we provide two novel characterizations for the
set of fixed points of a competitive TLN: the first is in terms of a simple
sign condition, while the second relies on the concept of domination. We apply
these results to a special family of TLNs, called combinatorial
threshold-linear networks (CTLNs), whose connectivity matrices are defined from
directed graphs. This leads us to prove a series of graph rules that enable one
to determine fixed points of a CTLN by analyzing the underlying graph.
Additionally, we study larger networks composed of smaller ""building block""
subnetworks, and prove several theorems relating the fixed points of the full
network to those of its components. Our results provide the foundation for a
kind of ""graphical calculus"" to infer features of the dynamics from a network's
connectivity.
",0,0,0,0,1,0
653,A Bayesian Nonparametrics based Robust Particle Filter Algorithm,"  This paper is concerned with the online estimation of a nonlinear dynamic
system from a series of noisy measurements. The focus is on cases wherein
outliers are present in-between normal noises. We assume that the outliers
follow an unknown generating mechanism which deviates from that of normal
noises, and then model the outliers using a Bayesian nonparametric model called
Dirichlet process mixture (DPM). A sequential particle-based algorithm is
derived for posterior inference for the outlier model as well as the state of
the system to be estimated. The resulting algorithm is termed DPM based robust
PF (DPM-RPF). The nonparametric feature makes this algorithm allow the data to
""speak for itself"" to determine the complexity and structure of the outlier
model. Simulation results show that it performs remarkably better than two
state-of-the-art methods especially when outliers appear frequently along time.
",0,0,0,1,0,0
408,Class-Splitting Generative Adversarial Networks,"  Generative Adversarial Networks (GANs) produce systematically better quality
samples when class label information is provided., i.e. in the conditional GAN
setup. This is still observed for the recently proposed Wasserstein GAN
formulation which stabilized adversarial training and allows considering high
capacity network architectures such as ResNet. In this work we show how to
boost conditional GAN by augmenting available class labels. The new classes
come from clustering in the representation space learned by the same GAN model.
The proposed strategy is also feasible when no class information is available,
i.e. in the unsupervised setup. Our generated samples reach state-of-the-art
Inception scores for CIFAR-10 and STL-10 datasets in both supervised and
unsupervised setup.
",0,0,0,1,0,0
12143,Quantum Blockchain using entanglement in time,"  A conceptual design for a quantum blockchain is proposed. Our method involves
encoding the blockchain into a temporal GHZ (Greenberger-Horne-Zeilinger) state
of photons that do not simultaneously coexist. It is shown that the
entanglement in time, as opposed to an entanglement in space, provides the
crucial quantum advantage. All the subcomponents of this system have already
been shown to be experimentally realized. Perhaps more shockingly, our encoding
procedure can be interpreted as non-classically influencing the past; hence
this decentralized quantum blockchain can be viewed as a quantum networked time
machine.
",0,0,0,0,0,1
2985,"Semi-parametric Dynamic Asymmetric Laplace Models for Tail Risk Forecasting, Incorporating Realized Measures","  The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression
model of Taylor (2017) is extended via incorporating a realized measure, to
drive the tail risk dynamics, as a potentially more efficient driver than daily
returns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte
Carlo method are employed for estimation, whose properties are assessed and
compared via a simulation study; results favour the Bayesian approach, which is
subsequently employed in a forecasting study of seven market indices and two
individual assets. The proposed models are compared to a range of parametric,
non-parametric and semi-parametric models, including GARCH, Realized-GARCH and
the joint VaR and ES quantile regression models in Taylor (2017). The
comparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected
Shortfall forecasts, over a long forecast sample period that includes the
global financial crisis in 2007-2008. The results favor the proposed models
incorporating a realized measure, especially when employing the sub-sampled
Realized Variance and the sub-sampled Realized Range.
",0,0,0,0,0,1
638,Análise comparativa de pesquisas de origens e destinos: uma abordagem baseada em Redes Complexas,"  In this paper, a comparative study was conducted between complex networks
representing origin and destination survey data. Similarities were found
between the characteristics of the networks of Brazilian cities with networks
of foreign cities. Power laws were found in the distributions of edge weights
and this scale - free behavior can occur due to the economic characteristics of
the cities.
",1,0,0,0,0,0
12857,Defining and estimating stochastic rate change in a dynamic general insurance portfolio,"  Rate change calculations in the literature involve deterministic methods that
measure the change in premium for a given policy. The definition of rate change
as a statistical parameter is proposed to address the stochastic nature of the
premium charged for a policy. It promotes the idea that rate change is a
property of an asymptotic population to be estimated, not just a property to
measure or monitor in the sample of observed policies that are written. Various
models and techniques are given for estimating this stochastic rate change and
quantifying the uncertainty in the estimates. The use of matched sampling is
emphasized for rate change estimation, as it adjusts for changes in policy
characteristics by directly searching for similar policies across policy years.
This avoids any of the assumptions and recipes that are required to re-rate
policies in years where they were not written, as is common with deterministic
methods. Such procedures can be subjective or implausible if the structure of
rating algorithms change or there are complex and heterogeneous exposure bases
and coverages. The methods discussed are applied to a motor premium database.
The application includes the use of a genetic algorithm with parallel
computations to automatically optimize the matched sampling.
",0,0,0,0,0,1
486,Strong-coupling of WSe2 in ultra-compact plasmonic nanocavities at room temperature,"  Strong-coupling of monolayer metal dichalcogenide semiconductors with light
offers encouraging prospects for realistic exciton devices at room temperature.
However, the nature of this coupling depends extremely sensitively on the
optical confinement and the orientation of electronic dipoles and fields. Here,
we show how plasmon strong coupling can be achieved in compact robust
easily-assembled gold nano-gap resonators at room temperature. We prove that
strong coupling is impossible with monolayers due to the large exciton
coherence size, but resolve clear anti-crossings for 8 layer devices with Rabi
splittings exceeding 135 meV. We show that such structures improve on prospects
for nonlinear exciton functionalities by at least 10^4, while retaining quantum
efficiencies above 50%.
",0,1,0,0,0,0
348,Leavitt path algebras: Graded direct-finiteness and graded $Σ$-injective simple modules,"  In this paper, we give a complete characterization of Leavitt path algebras
which are graded $\Sigma $-$V$ rings, that is, rings over which a direct sum of
arbitrary copies of any graded simple module is graded injective. Specifically,
we show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded
$\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of
arbitrary size but with finitely many non-zero entries over $K$ or
$K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical
characterization of such a graded $\Sigma $-$V$ ring $L$% . When the graph $E$
is finite, we show that $L$ is a graded $\Sigma $-$V$ ring $\Longleftrightarrow
L$ is graded directly-finite $\Longleftrightarrow L $ has bounded index of
nilpotence $\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that
the equivalence of these properties in the preceding statement no longer holds
when the graph $E$ is infinite. Following this, we also characterize Leavitt
path algebras $L$ which are non-graded $\Sigma $-$V$ rings. Graded rings which
are graded directly-finite are explored and it is shown that if a Leavitt path
algebra $L$ is a graded $\Sigma$-$V$ ring, then $L$ is always graded
directly-finite. Examples show the subtle differences between graded and
non-graded directly-finite rings. Leavitt path algebras which are graded
directly-finite are shown to be directed unions of graded semisimple rings.
Using this, we give an alternative proof of a theorem of Vaš \cite{V} on
directly-finite Leavitt path algebras.
",0,0,1,0,0,0
707,KGAN: How to Break The Minimax Game in GAN,"  Generative Adversarial Networks (GANs) were intuitively and attractively
explained under the perspective of game theory, wherein two involving parties
are a discriminator and a generator. In this game, the task of the
discriminator is to discriminate the real and generated (i.e., fake) data,
whilst the task of the generator is to generate the fake data that maximally
confuses the discriminator. In this paper, we propose a new viewpoint for GANs,
which is termed as the minimizing general loss viewpoint. This viewpoint shows
a connection between the general loss of a classification problem regarding a
convex loss function and a f-divergence between the true and fake data
distributions. Mathematically, we proposed a setting for the classification
problem of the true and fake data, wherein we can prove that the general loss
of this classification problem is exactly the negative f-divergence for a
certain convex function f. This allows us to interpret the problem of learning
the generator for dismissing the f-divergence between the true and fake data
distributions as that of maximizing the general loss which is equivalent to the
min-max problem in GAN if the Logistic loss is used in the classification
problem. However, this viewpoint strengthens GANs in two ways. First, it allows
us to employ any convex loss function for the discriminator. Second, it
suggests that rather than limiting ourselves in NN-based discriminators, we can
alternatively utilize other powerful families. Bearing this viewpoint, we then
propose using the kernel-based family for discriminators. This family has two
appealing features: i) a powerful capacity in classifying non-linear nature
data and ii) being convex in the feature space. Using the convexity of this
family, we can further develop Fenchel duality to equivalently transform the
max-min problem to the max-max dual problem.
",1,0,0,1,0,0
334,Contribution of Data Categories to Readmission Prediction Accuracy,"  Identification of patients at high risk for readmission could help reduce
morbidity and mortality as well as healthcare costs. Most of the existing
studies on readmission prediction did not compare the contribution of data
categories. In this study we analyzed relative contribution of 90,101 variables
across 398,884 admission records corresponding to 163,468 patients, including
patient demographics, historical hospitalization information, discharge
disposition, diagnoses, procedures, medications and laboratory test results. We
established an interpretable readmission prediction model based on Logistic
Regression in scikit-learn, and added the available variables to the model one
by one in order to analyze the influences of individual data categories on
readmission prediction accuracy. Diagnosis related groups (c-statistic
increment of 0.0933) and discharge disposition (c-statistic increment of
0.0269) were the strongest contributors to model accuracy. Additionally, we
also identified the top ten contributing variables in every data category.
",0,0,0,0,1,0
225,Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search Queries,"  Estimating vaccination uptake is an integral part of ensuring public health.
It was recently shown that vaccination uptake can be estimated automatically
from web data, instead of slowly collected clinical records or population
surveys. All prior work in this area assumes that features of vaccination
uptake collected from the web are temporally regular. We present the first ever
method to remove this assumption from vaccination uptake estimation: our method
dynamically adapts to temporal fluctuations in time series web data used to
estimate vaccination uptake. We show our method to outperform the state of the
art compared to competitive baselines that use not only web data but also
curated clinical data. This performance improvement is more pronounced for
vaccines whose uptake has been irregular due to negative media attention (HPV-1
and HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of
12 years old (whose vaccination is more irregular compared to younger
children).
",1,0,0,1,0,0
234,Coherence for lenses and open games,"  Categories of polymorphic lenses in computer science, and of open games in
compositional game theory, have a curious structure that is reminiscent of
compact closed categories, but differs in some crucial ways. Specifically they
have a family of morphisms that behave like the counits of a compact closed
category, but have no corresponding units; and they have a `partial' duality
that behaves like transposition in a compact closed category when it is
defined. We axiomatise this structure, which we refer to as a `teleological
category'. We precisely define a diagrammatic language suitable for these
categories, and prove a coherence theorem for them. This underpins the use of
diagrammatic reasoning in compositional game theory, which has previously been
used only informally.
",1,0,0,0,0,0
445,Deep & Cross Network for Ad Click Predictions,"  Feature engineering has been the key to the success of many prediction
models. However, the process is non-trivial and often requires manual feature
engineering or exhaustive searching. DNNs are able to automatically learn
feature interactions; however, they generate all the interactions implicitly,
and are not necessarily efficient in learning all types of cross features. In
this paper, we propose the Deep & Cross Network (DCN) which keeps the benefits
of a DNN model, and beyond that, it introduces a novel cross network that is
more efficient in learning certain bounded-degree feature interactions. In
particular, DCN explicitly applies feature crossing at each layer, requires no
manual feature engineering, and adds negligible extra complexity to the DNN
model. Our experimental results have demonstrated its superiority over the
state-of-art algorithms on the CTR prediction dataset and dense classification
dataset, in terms of both model accuracy and memory usage.
",1,0,0,1,0,0
11189,Statistical estimation of superhedging prices,"  We consider statistical estimation of superhedging prices using historical
stock returns in a frictionless market with d traded assets. We introduce a
simple plugin estimator based on empirical measures, show it is consistent but
lacks suitable robustness. This is addressed by our improved estimators which
use a larger set of martingale measures defined through a tradeoff between the
radius of Wasserstein balls around the empirical measure and the allowed norm
of martingale densities. We also study convergence rates, convergence of
superhedging strategies, and our study extends, in part, to the case of a
market with traded options and to a multiperiod setting.
",0,0,0,0,0,1
5844,Evaluation of matrix factorisation approaches for muscle synergy extraction,"  The muscle synergy concept provides a widely-accepted paradigm to break down
the complexity of motor control. In order to identify the synergies, different
matrix factorisation techniques have been used in a repertoire of fields such
as prosthesis control and biomechanical and clinical studies. However, the
relevance of these matrix factorisation techniques is still open for discussion
since there is no ground truth for the underlying synergies. Here, we evaluate
factorisation techniques and investigate the factors that affect the quality of
estimated synergies. We compared commonly used matrix factorisation methods:
Principal component analysis (PCA), Independent component analysis (ICA),
Non-negative matrix factorization (NMF) and second-order blind identification
(SOBI). Publicly available real data were used to assess the synergies
extracted by each factorisation method in the classification of wrist
movements. Synthetic datasets were utilised to explore the effect of muscle
synergy sparsity, level of noise and number of channels on the extracted
synergies. Results suggest that the sparse synergy model and a higher number of
channels would result in better-estimated synergies. Without dimensionality
reduction, SOBI showed better results than other factorisation methods. This
suggests that SOBI would be an alternative when a limited number of electrodes
is available but its performance was still poor in that case. Otherwise, NMF
had the best performance when the number of channels was higher than the number
of synergies. Therefore, NMF would be the best method for muscle synergy
extraction.
",0,0,0,0,1,0
569,Exoplanet Atmosphere Retrieval using Multifractal Analysis of Secondary Eclipse Spectra,"  We extend a data-based model-free multifractal method of exoplanet detection
to probe exoplanetary atmospheres. Whereas the transmission spectrum is studied
during the primary eclipse, we analyze the emission spectrum during the
secondary eclipse, thereby probing the atmospheric limb. In addition to the
spectral structure of exoplanet atmospheres, the approach provides information
to study phenomena such as atmospheric flows, tidal-locking behavior, and the
dayside-nightside redistribution of energy. The approach is demonstrated using
Spitzer data for exoplanet HD189733b. The central advantage of the method is
the lack of model assumptions in the detection and observational schemes.
",0,1,0,1,0,0
433,Bounded Depth Ascending HNN Extensions and $π_1$-Semistability at $\infty$,"  A 1-ended finitely presented group has semistable fundamental group at
$\infty$ if it acts geometrically on some (equivalently any) simply connected
and locally finite complex $X$ with the property that any two proper rays in
$X$ are properly homotopic. If $G$ has semistable fundamental group at $\infty$
then one can unambiguously define the fundamental group at $\infty$ for $G$.
The problem, asking if all finitely presented groups have semistable
fundamental group at $\infty$ has been studied for over 40 years. If $G$ is an
ascending HNN extension of a finitely presented group then indeed, $G$ has
semistable fundamental group at $\infty$, but since the early 1980's it has
been suggested that the finitely presented groups that are ascending HNN
extensions of {\it finitely generated} groups may include a group with
non-semistable fundamental group at $\infty$. Ascending HNN extensions
naturally break into two classes, those with bounded depth and those with
unbounded depth. Our main theorem shows that bounded depth finitely presented
ascending HNN extensions of finitely generated groups have semistable
fundamental group at $\infty$. Semistability is equivalent to two weaker
asymptotic conditions on the group holding simultaneously. We show one of these
conditions holds for all ascending HNN extensions, regardless of depth. We give
a technique for constructing ascending HNN extensions with unbounded depth.
This work focuses attention on a class of groups that may contain a group with
non-semistable fundamental group at $\infty$.
",0,0,1,0,0,0
400,On vector measures and extensions of transfunctions,"  We are interested in extending operators defined on positive measures, called
here transfunctions, to signed measures and vector measures. Our methods use a
somewhat nonstandard approach to measures and vector measures. The necessary
background, including proofs of some auxiliary results, is included.
",0,0,1,0,0,0
458,Testing redMaPPer centring probabilities using galaxy clustering and galaxy-galaxy lensing,"  Galaxy cluster centring is a key issue for precision cosmology studies using
galaxy surveys. Mis-identification of central galaxies causes systematics in
various studies such as cluster lensing, satellite kinematics, and galaxy
clustering. The red-sequence Matched-filter Probabilistic Percolation
(redMaPPer) estimates the probability that each member galaxy is central from
photometric information rather than specifying one central galaxy. The
redMaPPer estimates can be used for calibrating the off-centring effect,
however, the centring algorithm has not previously been well-tested. We test
the centring probabilities of redMaPPer cluster catalog using the projected
cross correlation between redMaPPer clusters with photometric red galaxies and
galaxy-galaxy lensing. We focus on the subsample of redMaPPer clusters in which
the redMaPPer central galaxies (RMCGs) are not the brightest member galaxies
(BMEM) and both of them have spectroscopic redshift. This subsample represents
nearly 10% of the whole cluster sample. We find a clear difference in the
cross-correlation measurements between RMCGs and BMEMs, and the estimated
centring probability is 74$\pm$10% for RMCGs and 13$\pm$4% for BMEMs in the
Gaussian offset model and 78$\pm$9% for RMCGs and 5$\pm$5% for BMEMs in the NFW
offset model. These values are in agreement with the centring probability
values reported by redMaPPer (75% for RMCG and 10% for BMEMs) within 1$\sigma$.
Our analysis provides a strong consistency test of the redMaPPer centring
probabilities. Our results suggest that redMaPPer centring probabilities are
reliably estimated. We confirm that the brightest galaxy in the cluster is not
always the central galaxy as has been shown in previous works.
",0,1,0,0,0,0
776,Classification of pro-$p$ PD$^2$ pairs and the pro-$p$ curve complex,"  We classify pro-$p$ Poincaré duality pairs in dimension two. We then use
this classification to build a pro-$p$ analogue of the curve complex and
establish its basic properties. We conclude with some statements concerning
separability properties of the mapping class group.
",0,0,1,0,0,0
322,Bayesian Image Quality Transfer with CNNs: Exploring Uncertainty in dMRI Super-Resolution,"  In this work, we investigate the value of uncertainty modeling in 3D
super-resolution with convolutional neural networks (CNNs). Deep learning has
shown success in a plethora of medical image transformation problems, such as
super-resolution (SR) and image synthesis. However, the highly ill-posed nature
of such problems results in inevitable ambiguity in the learning of networks.
We propose to account for intrinsic uncertainty through a per-patch
heteroscedastic noise model and for parameter uncertainty through approximate
Bayesian inference in the form of variational dropout. We show that the
combined benefits of both lead to the state-of-the-art performance SR of
diffusion MR brain images in terms of errors compared to ground truth. We
further show that the reduced error scores produce tangible benefits in
downstream tractography. In addition, the probabilistic nature of the methods
naturally confers a mechanism to quantify uncertainty over the super-resolved
output. We demonstrate through experiments on both healthy and pathological
brains the potential utility of such an uncertainty measure in the risk
assessment of the super-resolved images for subsequent clinical use.
",1,0,0,0,0,0
342,Greedy-Merge Degrading has Optimal Power-Law,"  Consider a channel with a given input distribution. Our aim is to degrade it
to a channel with at most L output letters. One such degradation method is the
so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction
in mutual information between input and output. For fixed input alphabet size
and variable L, the upper bound is within a constant factor of an
algorithm-independent lower bound. Thus, we establish that greedy-merge is
optimal in the power-law sense.
",1,0,1,0,0,0
576,Discriminant circle bundles over local models of Strebel graphs and Boutroux curves,"  We study special circle bundles over two elementary moduli spaces of
meromorphic quadratic differentials with real periods denoted by $\mathcal
Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$. The space
$\mathcal Q_0^{\mathbb R}(-7)$ is the moduli space of meromorphic quadratic
differentials on the Riemann sphere with one pole of order 7 with real periods;
it appears naturally in the study of a neighbourhood of the Witten's cycle
$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic
differentials of $\mathcal M_{g,n}$. The space $\mathcal Q^{\mathbb
R}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the
Riemann sphere with two poles of order at most 3 with real periods; it appears
in description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the
combinatorial model. The application of the formalism of the Bergman
tau-function to the combinatorial model (with the goal of computing
analytically Poincare dual cycles to certain combinations of tautological
classes) requires the study of special sections of circle bundles over
$\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$; in the
case of the space $\mathcal Q_0^{\mathbb R}(-7)$ a section of this circle
bundle is given by the argument of the modular discriminant. We study the
spaces $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$,
also called the spaces of Boutroux curves, in detail, together with
corresponding circle bundles.
",0,1,1,0,0,0
700,Computationally Efficient Measures of Internal Neuron Importance,"  The challenge of assigning importance to individual neurons in a network is
of interest when interpreting deep learning models. In recent work, Dhamdhere
et al. proposed Total Conductance, a ""natural refinement of Integrated
Gradients"" for attributing importance to internal neurons. Unfortunately, the
authors found that calculating conductance in tensorflow required the addition
of several custom gradient operators and did not scale well. In this work, we
show that the formula for Total Conductance is mathematically equivalent to
Path Integrated Gradients computed on a hidden layer in the network. We provide
a scalable implementation of Total Conductance using standard tensorflow
gradient operators that we call Neuron Integrated Gradients. We compare Neuron
Integrated Gradients to DeepLIFT, a pre-existing computationally efficient
approach that is applicable to calculating internal neuron importance. We find
that DeepLIFT produces strong empirical results and is faster to compute, but
because it lacks the theoretical properties of Neuron Integrated Gradients, it
may not always be preferred in practice. Colab notebook reproducing results:
this http URL
",0,0,0,1,0,0
338,Incarnation of Majorana Fermions in Kitaev Quantum Spin Lattice,"  Kitaev quantum spin liquid is a topological magnetic quantum state
characterized by Majorana fermions of fractionalized spin excitations, which
are identical to their own antiparticles. Here, we demonstrate emergence of
Majorana fermions thermally fractionalized in the Kitaev honeycomb spin lattice
{\alpha}-RuCl3. The specific heat data unveil the characteristic two-stage
release of magnetic entropy involving localized and itinerant Majorana
fermions. The inelastic neutron scattering results further corroborate these
two distinct fermions by exhibiting quasielastic excitations at low energies
around the Brillouin zone center and Y-shaped magnetic continuum at high
energies, which are evident for the ferromagnetic Kitaev model. Our results
provide an opportunity to build a unified conceptual framework of
fractionalized excitations, applicable also for the quantum Hall states,
superconductors, and frustrated magnets.
",0,1,0,0,0,0
13376,Simulation Methods for Stochastic Storage Problems: A Statistical Learning Perspective,"  We consider solution of stochastic storage problems through regression Monte
Carlo (RMC) methods. Taking a statistical learning perspective, we develop the
dynamic emulation algorithm (DEA) that unifies the different existing
approaches in a single modular template. We then investigate the two central
aspects of regression architecture and experimental design that constitute DEA.
For the regression piece, we discuss various non-parametric approaches, in
particular introducing the use of Gaussian process regression in the context of
stochastic storage. For simulation design, we compare the performance of
traditional design (grid discretization), against space-filling, and several
adaptive alternatives. The overall DEA template is illustrated with multiple
examples drawing from natural gas storage valuation and optimal control of
back-up generator in a microgrid.
",0,0,0,0,0,1
5674,Computation of optimal transport and related hedging problems via penalization and neural networks,"  This paper presents a widely applicable approach to solving (multi-marginal,
martingale) optimal transport and related problems via neural networks. The
core idea is to penalize the optimization problem in its dual formulation and
reduce it to a finite dimensional one which corresponds to optimizing a neural
network with smooth objective function. We present numerical examples from
optimal transport, martingale optimal transport, portfolio optimization under
uncertainty and generative adversarial networks that showcase the generality
and effectiveness of the approach.
",0,0,0,1,0,1
862,Network-theoretic approach to sparsified discrete vortex dynamics,"  We examine discrete vortex dynamics in two-dimensional flow through a
network-theoretic approach. The interaction of the vortices is represented with
a graph, which allows the use of network-theoretic approaches to identify key
vortex-to-vortex interactions. We employ sparsification techniques on these
graph representations based on spectral theory for constructing sparsified
models and evaluating the dynamics of vortices in the sparsified setup.
Identification of vortex structures based on graph sparsification and sparse
vortex dynamics are illustrated through an example of point-vortex clusters
interacting amongst themselves. We also evaluate the performance of
sparsification with increasing number of point vortices. The
sparsified-dynamics model developed with spectral graph theory requires reduced
number of vortex-to-vortex interactions but agrees well with the full nonlinear
dynamics. Furthermore, the sparsified model derived from the sparse graphs
conserves the invariants of discrete vortex dynamics. We highlight the
similarities and differences between the present sparsified-dynamics model and
the reduced-order models.
",0,1,0,0,0,0
17184,Atomic Swaptions: Cryptocurrency Derivatives,"  The atomic swap protocol allows for the exchange of cryptocurrencies on
different blockchains without the need to trust a third-party. However, market
participants who desire to hold derivative assets such as options or futures
would also benefit from trustless exchange. In this paper I propose the atomic
swaption, which extends the atomic swap to allow for such exchanges. Crucially,
atomic swaptions do not require the use of oracles. I also introduce the margin
contract, which provides the ability to create leveraged and short positions.
Lastly, I discuss how atomic swaptions may be routed on the Lightning Network.
",0,0,0,0,0,1
6072,A path integral based model for stocks and order dynamics,"  We introduce a model for the short-term dynamics of financial assets based on
an application to finance of quantum gauge theory, developing ideas of Ilinski.
We present a numerical algorithm for the computation of the probability
distribution of prices and compare the results with APPLE stocks prices and the
S&P500 index.
",0,0,0,0,0,1
500,Approximately certifying the restricted isometry property is hard,"  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts
as an approximate isometry when restricted to sparse vectors. Previous work has
shown it to be NP-hard to determine whether a matrix possess this property, but
only in a narrow range of parameters. In this work, we show that it is NP-hard
to make this determination for any accuracy parameter, even when we restrict
ourselves to instances which are either RIP or far from being RIP. This result
implies that it is NP-hard to approximate the range of parameters for which a
matrix possesses the Restricted Isometry Property with accuracy better than
some constant. Ours is the first work to prove such a claim without any
additional assumptions.
",1,0,0,0,0,0
10392,Parameter Learning and Change Detection Using a Particle Filter With Accelerated Adaptation,"  This paper presents the construction of a particle filter, which incorporates
elements inspired by genetic algorithms, in order to achieve accelerated
adaptation of the estimated posterior distribution to changes in model
parameters. Specifically, the filter is designed for the situation where the
subsequent data in online sequential filtering does not match the model
posterior filtered based on data up to a current point in time. The examples
considered encompass parameter regime shifts and stochastic volatility. The
filter adapts to regime shifts extremely rapidly and delivers a clear heuristic
for distinguishing between regime shifts and stochastic volatility, even though
the model dynamics assumed by the filter exhibit neither of those features.
",0,0,0,1,0,1
471,Robust Orchestration of Concurrent Application Workflows in Mobile Device Clouds,"  A hybrid mobile/fixed device cloud that harnesses sensing, computing,
communication, and storage capabilities of mobile and fixed devices in the
field as well as those of computing and storage servers in remote datacenters
is envisioned. Mobile device clouds can be harnessed to enable innovative
pervasive applications that rely on real-time, in-situ processing of sensor
data collected in the field. To support concurrent mobile applications on the
device cloud, a robust and secure distributed computing framework, called
Maestro, is proposed. The key components of Maestro are (i) a task scheduling
mechanism that employs controlled task replication in addition to task
reallocation for robustness and (ii) Dedup for task deduplication among
concurrent pervasive workflows. An architecture-based solution that relies on
task categorization and authorized access to the categories of tasks is
proposed for different levels of protection. Experimental evaluation through
prototype testbed of Android- and Linux-based mobile devices as well as
simulations is performed to demonstrate Maestro's capabilities.
",1,0,0,0,0,0
355,End-to-End Navigation in Unknown Environments using Neural Networks,"  We investigate how a neural network can learn perception actions loops for
navigation in unknown environments. Specifically, we consider how to learn to
navigate in environments populated with cul-de-sacs that represent convex local
minima that the robot could fall into instead of finding a set of feasible
actions that take it to the goal. Traditional methods rely on maintaining a
global map to solve the problem of over coming a long cul-de-sac. However, due
to errors induced from local and global drift, it is highly challenging to
maintain such a map for long periods of time. One way to mitigate this problem
is by using learning techniques that do not rely on hand engineered map
representations and instead output appropriate control policies directly from
their sensory input. We first demonstrate that such a problem cannot be solved
directly by deep reinforcement learning due to the sparse reward structure of
the environment. Further, we demonstrate that deep supervised learning also
cannot be used directly to solve this problem. We then investigate network
models that offer a combination of reinforcement learning and supervised
learning and highlight the significance of adding fully differentiable memory
units to such networks. We evaluate our networks on their ability to generalize
to new environments and show that adding memory to such networks offers huge
jumps in performance
",1,0,0,0,0,0
593,Incompressible limit of the Navier-Stokes model with a growth term,"  Starting from isentropic compressible Navier-Stokes equations with growth
term in the continuity equation, we rigorously justify that performing an
incompressible limit one arrives to the two-phase free boundary fluid system.
",0,0,1,0,0,0
655,Rotating Rayleigh-Taylor turbulence,"  The turbulent Rayleigh--Taylor system in a rotating reference frame is
investigated by direct numerical simulations within the Oberbeck-Boussinesq
approximation. On the basis of theoretical arguments, supported by our
simulations, we show that the Rossby number decreases in time, and therefore
the Coriolis force becomes more important as the system evolves and produces
many effects on Rayleigh--Taylor turbulence. We find that rotation reduces the
intensity of turbulent velocity fluctuations and therefore the growth rate of
the temperature mixing layer. Moreover, in presence of rotation the conversion
of potential energy into turbulent kinetic energy is found to be less effective
and the efficiency of the heat transfer is reduced. Finally, during the
evolution of the mixing layer we observe the development of a
cyclone-anticyclone asymmetry.
",0,1,0,0,0,0
633,Learning Models from Data with Measurement Error: Tackling Underreporting,"  Measurement error in observational datasets can lead to systematic bias in
inferences based on these datasets. As studies based on observational data are
increasingly used to inform decisions with real-world impact, it is critical
that we develop a robust set of techniques for analyzing and adjusting for
these biases. In this paper we present a method for estimating the distribution
of an outcome given a binary exposure that is subject to underreporting. Our
method is based on a missing data view of the measurement error problem, where
the true exposure is treated as a latent variable that is marginalized out of a
joint model. We prove three different conditions under which the outcome
distribution can still be identified from data containing only error-prone
observations of the exposure. We demonstrate this method on synthetic data and
analyze its sensitivity to near violations of the identifiability conditions.
Finally, we use this method to estimate the effects of maternal smoking and
opioid use during pregnancy on childhood obesity, two import problems from
public health. Using the proposed method, we estimate these effects using only
subject-reported drug use data and substantially refine the range of estimates
generated by a sensitivity analysis-based approach. Further, the estimates
produced by our method are consistent with existing literature on both the
effects of maternal smoking and the rate at which subjects underreport smoking.
",1,0,0,1,0,0
7975,Credit Risk Meets Random Matrices: Coping with Non-Stationary Asset Correlations,"  We review recent progress in modeling credit risk for correlated assets. We
start from the Merton model which default events and losses are derived from
the asset values at maturity. To estimate the time development of the asset
values, the stock prices are used whose correlations have a strong impact on
the loss distribution, particularly on its tails. These correlations are
non-stationary which also influences the tails. We account for the asset
fluctuations by averaging over an ensemble of random matrices that models the
truly existing set of measured correlation matrices. As a most welcome side
effect, this approach drastically reduces the parameter dependence of the loss
distribution, allowing us to obtain very explicit results which show
quantitatively that the heavy tails prevail over diversification benefits even
for small correlations. We calibrate our random matrix model with market data
and show how it is capable of grasping different market situations.
Furthermore, we present numerical simulations for concurrent portfolio risks,
i.e., for the joint probability densities of losses for two portfolios. For the
convenience of the reader, we give an introduction to the Wishart random matrix
model.
",0,0,0,0,0,1
5227,Effect of Blast Exposure on Gene-Gene Interactions,"  Repeated exposure to low-level blast may initiate a range of adverse health
problem such as traumatic brain injury (TBI). Although many studies
successfully identified genes associated with TBI, yet the cellular mechanisms
underpinning TBI are not fully elucidated. In this study, we investigated
underlying relationship among genes through constructing transcript Bayesian
networks using RNA-seq data. The data for pre- and post-blast transcripts,
which were collected on 33 individuals in Army training program, combined with
our system approach provide unique opportunity to investigate the effect of
blast-wave exposure on gene-gene interactions. Digging into the networks, we
identified four subnetworks related to immune system and inflammatory process
that are disrupted due to the exposure. Among genes with relatively high fold
change in their transcript expression level, ATP6V1G1, B2M, BCL2A1, PELI,
S100A8, TRIM58 and ZNF654 showed major impact on the dysregulation of the
gene-gene interactions. This study reveals how repeated exposures to traumatic
conditions increase the level of fold change of transcript expression and
hypothesizes new targets for further experimental studies.
",0,0,0,0,1,0
6595,A Critical-like Collective State Leads to Long-range Cell Communication in Dictyostelium discoideum Aggregation,"  The transition from single-cell to multicellular behavior is important in
early development but rarely studied. The starvation-induced aggregation of the
social amoeba Dictyostelium discoideum into a multicellular slug is known to
result from single-cell chemotaxis towards emitted pulses of cyclic adenosine
monophosphate (cAMP). However, how exactly do transient short-range chemical
gradients lead to coherent collective movement at a macroscopic scale? Here, we
use a multiscale model verified by quantitative microscopy to describe
wide-ranging behaviors from chemotaxis and excitability of individual cells to
aggregation of thousands of cells. To better understand the mechanism of
long-range cell-cell communication and hence aggregation, we analyze cell-cell
correlations, showing evidence for self-organization at the onset of
aggregation (as opposed to following a leader cell). Surprisingly, cell
collectives, despite their finite size, show features of criticality known from
phase transitions in physical systems. Application of external cAMP
perturbations in our simulations near the sensitive critical point allows
steering cells into early aggregation and towards certain locations but not
once an aggregation center has been chosen.
",0,0,0,0,1,0
510,Maximum likelihood estimators based on the block maxima method,"  The extreme value index is a fundamental parameter in univariate Extreme
Value Theory (EVT). It captures the tail behavior of a distribution and is
central in the extrapolation beyond observed data. Among other semi-parametric
methods (such as the popular Hill's estimator), the Block Maxima (BM) and
Peaks-Over-Threshold (POT) methods are widely used for assessing the extreme
value index and related normalizing constants. We provide asymptotic theory for
the maximum likelihood estimators (MLE) based on the BM method. Our main result
is the asymptotic normality of the MLE with a non-trivial bias depending on the
extreme value index and on the so-called second order parameter. Our approach
combines asymptotic expansions of the likelihood process and of the empirical
quantile process of block maxima. The results permit to complete the comparison
of most common semi-parametric estimators in EVT (MLE and probability weighted
moment estimators based on the POT or BM methods) through their asymptotic
variances, biases and optimal mean square errors.
",0,0,1,1,0,0
2197,Smart TWAP trading in continuous-time equilibria,"  This paper presents a continuous-time equilibrium model of TWAP trading and
liquidity provision in a market with multiple strategic investors with
heterogeneous intraday trading targets. We solve the model in closed-form and
show there are infinitely many equilibria. We compare the competitive
equilibrium with different non-price-taking equilibria. In addition, we show
intraday TWAP benchmarking reduces market liquidity relative to just terminal
trading targets alone. The model is computationally tractable, and we provide a
number of numerical illustrations. An extension to stochastic VWAP targets is
also provided.
",0,0,0,0,0,1
1011,How Do Classifiers Induce Agents To Invest Effort Strategically?,"  Algorithms are often used to produce decision-making rules that classify or
evaluate individuals. When these individuals have incentives to be classified a
certain way, they may behave strategically to influence their outcomes. We
develop a model for how strategic agents can invest effort in order to change
the outcomes they receive, and we give a tight characterization of when such
agents can be incentivized to invest specified forms of effort into improving
their outcomes as opposed to ""gaming"" the classifier. We show that whenever any
""reasonable"" mechanism can do so, a simple linear mechanism suffices.
",0,0,0,1,0,0
738,Existence results for primitive elements in cubic and quartic extensions of a finite field,"  With $\Fq$ the finite field of $q$ elements, we investigate the following
question. If $\gamma$ generates $\Fqn$ over $\Fq$ and $\beta$ is a non-zero
element of $\Fqn$, is there always an $a \in \Fq$ such that $\beta(\gamma + a)$
is a primitive element? We resolve this case when $n=3$, thereby proving a
conjecture by Cohen. We also improve substantially on what is known when $n=4$.
",0,0,1,0,0,0
238,GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking,"  Model compression is essential for serving large deep neural nets on devices
with limited resources or applications that require real-time responses. As a
case study, a state-of-the-art neural language model usually consists of one or
more recurrent layers sandwiched between an embedding layer used for
representing input tokens and a softmax layer for generating output tokens. For
problems with a very large vocabulary size, the embedding and the softmax
matrices can account for more than half of the model size. For instance, the
bigLSTM model achieves state-of- the-art performance on the One-Billion-Word
(OBW) dataset with around 800k vocabulary, and its word embedding and softmax
matrices use more than 6GBytes space, and are responsible for over 90% of the
model parameters. In this paper, we propose GroupReduce, a novel compression
method for neural language models, based on vocabulary-partition (block) based
low-rank matrix approximation and the inherent frequency distribution of tokens
(the power-law distribution of words). The experimental results show our method
can significantly outperform traditional compression methods such as low-rank
approximation and pruning. On the OBW dataset, our method achieved 6.6 times
compression rate for the embedding and softmax matrices, and when combined with
quantization, our method can achieve 26 times compression rate, which
translates to a factor of 12.8 times compression for the entire model with very
little degradation in perplexity.
",0,0,0,1,0,0
572,Exploring deep learning as an event classification method for the Cherenkov Telescope Array,"  Telescopes based on the imaging atmospheric Cherenkov technique (IACTs)
detect images of the atmospheric showers generated by gamma rays and cosmic
rays as they are absorbed by the atmosphere. The much more frequent cosmic-ray
events form the main background when looking for gamma-ray sources, and
therefore IACT sensitivity is significantly driven by the capability to
distinguish between these two types of events. Supervised learning algorithms,
like random forests and boosted decision trees, have been shown to effectively
classify IACT events. In this contribution we present results from exploratory
work using deep learning as an event classification method for the Cherenkov
Telescope Array (CTA). CTA, conceived as an array of tens of IACTs, is an
international project for a next-generation ground-based gamma-ray observatory,
aiming to improve on the sensitivity of current-generation experiments by an
order of magnitude and provide energy coverage from 20 GeV to more than 300
TeV.
",0,1,0,0,0,0
15090,"Supporting Crowd-Powered Science in Economics: FRACTI, a Conceptual Framework for Large-Scale Collaboration and Transparent Investigation in Financial Markets","  Modern investigation in economics and in other sciences requires the ability
to store, share, and replicate results and methods of experiments that are
often multidisciplinary and yield a massive amount of data. Given the
increasing complexity and growing interaction across diverse bodies of
knowledge it is becoming imperative to define a platform to properly support
collaborative research and track origin, accuracy and use of data. This paper
starts by defining a set of methods leveraging scientific principles and
advocating the importance of those methods in multidisciplinary, computer
intensive fields like computational finance. The next part of this paper
defines a class of systems called scientific support systems, vis-a-vis usages
in other research fields such as bioinformatics, physics and engineering. We
outline a basic set of fundamental concepts, and list our goals and motivation
for leveraging such systems to enable large-scale investigation, ""crowd powered
science"", in economics. The core of this paper provides an outline of FRACTI in
five steps. First we present definitions related to scientific support systems
intrinsic to finance and describe common characteristics of financial use
cases. The second step concentrates on what can be exchanged through the
definition of shareable entities called contributions. The third step is the
description of a classification system for building blocks of the conceptual
framework, called facets. The fourth step introduces the meta-model that will
enable provenance tracking and representation of data fragments and simulation.
Finally we describe intended cases of use to highlight main strengths of
FRACTI: application of the scientific method for investigation in computational
finance, large-scale collaboration and simulation.
",0,0,0,0,0,1
847,Smoothed Noise and Mexican Hat Coupling Produce Pattern in a Stochastic Neural Field,"  The formation of pattern in biological systems may be modeled by a set of
reaction-diffusion equations. A diffusion-type coupling operator biologically
significant in neuroscience is a difference of Gaussian functions (Mexican Hat
operator) used as a spatial-convolution kernel. We are interested in the
difference among behaviors of \emph{stochastic} neural field equations, namely
space-time stochastic differential-integral equations, and similar
deterministic ones. We explore, quantitatively, how the parameters of our model
that measure the shape of the coupling kernel, coupling strength, and aspects
of the spatially-smoothed space-time noise, control the pattern in the
resulting evolving random field. We find that a spatial pattern that is damped
in time in a deterministic system may be sustained and amplified by
stochasticity, most strikingly at an optimal spatio-temporal noise level. In
addition, we find that spatially-smoothed noise alone causes pattern formation
even without spatial coupling.
",0,0,0,0,1,0
279,A Team-Formation Algorithm for Faultline Minimization,"  In recent years, the proliferation of online resumes and the need to evaluate
large populations of candidates for on-site and virtual teams have led to a
growing interest in automated team-formation. Given a large pool of candidates,
the general problem requires the selection of a team of experts to complete a
given task. Surprisingly, while ongoing research has studied numerous
variations with different constraints, it has overlooked a factor with a
well-documented impact on team cohesion and performance: team faultlines.
Addressing this gap is challenging, as the available measures for faultlines in
existing teams cannot be efficiently applied to faultline optimization. In this
work, we meet this challenge with a new measure that can be efficiently used
for both faultline measurement and minimization. We then use the measure to
solve the problem of automatically partitioning a large population into
low-faultline teams. By introducing faultlines to the team-formation
literature, our work creates exciting opportunities for algorithmic work on
faultline optimization, as well as on work that combines and studies the
connection of faultlines with other influential team characteristics.
",1,0,0,0,0,0
676,Stop talking to me -- a communication-avoiding ADER-DG realisation,"  We present a communication- and data-sensitive formulation of ADER-DG for
hyperbolic differential equation systems. Sensitive here has multiple flavours:
First, the formulation reduces the persistent memory footprint. This reduces
pressure on the memory subsystem. Second, the formulation realises the
underlying predictor-corrector scheme with single-touch semantics, i.e., each
degree of freedom is read on average only once per time step from the main
memory. This reduces communication through the memory controllers. Third, the
formulation breaks up the tight coupling of the explicit time stepping's
algorithmic steps to mesh traversals. This averages out data access peaks.
Different operations and algorithmic steps are ran on different grid entities.
Finally, the formulation hides distributed memory data transfer behind the
computation aligned with the mesh traversal. This reduces pressure on the
machine interconnects. All techniques applied by our formulation are elaborated
by means of a rigorous task formalism. They break up ADER-DG's tight causal
coupling of compute steps and can be generalised to other predictor-corrector
schemes.
",1,0,0,0,0,0
809,Deep Spatio-Temporal Random Fields for Efficient Video Segmentation,"  In this work we introduce a time- and memory-efficient method for structured
prediction that couples neuron decisions across both space at time. We show
that we are able to perform exact and efficient inference on a densely
connected spatio-temporal graph by capitalizing on recent advances on deep
Gaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)
efficient, (b) has a unique global minimum, and (c) can be trained end-to-end
alongside contemporary deep networks for video understanding. We experiment
with multiple connectivity patterns in the temporal domain, and present
empirical improvements over strong baselines on the tasks of both semantic and
instance segmentation of videos.
",0,0,0,1,0,0
9905,"The Network of U.S. Mutual Fund Investments: Diversification, Similarity and Fragility throughout the Global Financial Crisis","  Network theory proved recently to be useful in the quantification of many
properties of financial systems. The analysis of the structure of investment
portfolios is a major application since their eventual correlation and overlap
impact the actual risk diversification by individual investors. We investigate
the bipartite network of US mutual fund portfolios and their assets. We follow
its evolution during the Global Financial Crisis and analyse the interplay
between diversification, as understood in classical portfolio theory, and
similarity of the investments of different funds. We show that, on average,
portfolios have become more diversified and less similar during the crisis.
However, we also find that large overlap is far more likely than expected from
models of random allocation of investments. This indicates the existence of
strong correlations between fund portfolio strategies. We introduce a
simplified model of propagation of financial shocks, that we exploit to show
that a systemic risk component origins from the similarity of portfolios. The
network is still vulnerable after crisis because of this effect, despite the
increase in the diversification of portfolios. Our results indicate that
diversification may even increase systemic risk when funds diversify in the
same way. Diversification and similarity can play antagonistic roles and the
trade-off between the two should be taken into account to properly assess
systemic risk.
",0,0,0,1,0,1
217,The challenge of realistic music generation: modelling raw audio at scale,"  Realistic music generation is a challenging task. When building generative
models of music that are learnt from data, typically high-level representations
such as scores or MIDI are used that abstract away the idiosyncrasies of a
particular performance. But these nuances are very important for our perception
of musicality and realism, so in this work we embark on modelling music in the
raw audio domain. It has been shown that autoregressive models excel at
generating raw audio waveforms of speech, but when applied to music, we find
them biased towards capturing local signal structure at the expense of
modelling long-range correlations. This is problematic because music exhibits
structure at many different timescales. In this work, we explore autoregressive
discrete autoencoders (ADAs) as a means to enable autoregressive models to
capture long-range correlations in waveforms. We find that they allow us to
unconditionally generate piano music directly in the raw audio domain, which
shows stylistic consistency across tens of seconds.
",0,0,0,1,0,0
732,The tumbling rotational state of 1I/`Oumuamua,"  The discovery of 1I/2017 U1 ('Oumuamua) has provided the first glimpse of a
planetesimal born in another planetary system. This interloper exhibits a
variable colour within a range that is broadly consistent with local small
bodies such as the P/D type asteroids, Jupiter Trojans, and dynamically excited
Kuiper Belt Objects. 1I/'Oumuamua appears unusually elongated in shape, with an
axial ratio exceeding 5:1. Rotation period estimates are inconsistent and
varied, with reported values between 6.9 and 8.3 hours. Here we analyse all
available optical photometry reported to date. No single rotation period can
explain the exhibited brightness variations. Rather, 1I/'Oumuamua appears to be
in an excited rotational state undergoing Non-Principal Axis (NPA) rotation, or
tumbling. A satisfactory solution has apparent lightcurve frequencies of 0.135
and 0.126 hr-1 and implies a longest-to-shortest axis ratio of 5:1, though the
available data are insufficient to uniquely constrain the true frequencies and
shape. Assuming a body that responds to NPA rotation in a similar manner to
Solar System asteroids and comets, the timescale to damp 1I/'Oumuamua's
tumbling is at least a billion years. 1I/'Oumuamua was likely set tumbling
within its parent planetary system, and will remain tumbling well after it has
left ours.
",0,1,0,0,0,0
12405,Disentangling and Assessing Uncertainties in Multiperiod Corporate Default Risk Predictions,"  Measuring the corporate default risk is broadly important in economics and
finance. Quantitative methods have been developed to predictively assess future
corporate default probabilities. However, as a more difficult yet crucial
problem, evaluating the uncertainties associated with the default predictions
remains little explored. In this paper, we attempt to fill this blank by
developing a procedure for quantifying the level of associated uncertainties
upon carefully disentangling multiple contributing sources. Our framework
effectively incorporates broad information from historical default data,
corporates' financial records, and macroeconomic conditions by a)
characterizing the default mechanism, and b) capturing the future dynamics of
various features contributing to the default mechanism. Our procedure overcomes
the major challenges in this large scale statistical inference problem and
makes it practically feasible by using parsimonious models, innovative methods,
and modern computational facilities. By predicting the marketwide total number
of defaults and assessing the associated uncertainties, our method can also be
applied for evaluating the aggregated market credit risk level. Upon analyzing
a US market data set, we demonstrate that the level of uncertainties associated
with default risk assessments is indeed substantial. More informatively, we
also find that the level of uncertainties associated with the default risk
predictions is correlated with the level of default risks, indicating potential
for new scopes in practical applications including improving the accuracy of
default risk assessments.
",0,0,0,1,0,1
681,Deep Fluids: A Generative Network for Parameterized Fluid Simulations,"  This paper presents a novel generative model to synthesize fluid simulations
from a set of reduced parameters. A convolutional neural network is trained on
a collection of discrete, parameterizable fluid simulation velocity fields. Due
to the capability of deep learning architectures to learn representative
features of the data, our generative model is able to accurately approximate
the training data set, while providing plausible interpolated in-betweens. The
proposed generative model is optimized for fluids by a novel loss function that
guarantees divergence-free velocity fields at all times. In addition, we
demonstrate that we can handle complex parameterizations in reduced spaces, and
advance simulations in time by integrating in the latent space with a second
network. Our method models a wide variety of fluid behaviors, thus enabling
applications such as fast construction of simulations, interpolation of fluids
with different parameters, time re-sampling, latent space simulations, and
compression of fluid simulation data. Reconstructed velocity fields are
generated up to 700x faster than traditional CPU solvers, while achieving
compression rates of over 1300x.
",0,0,0,1,0,0
317,Deep Multimodal Image-Repurposing Detection,"  Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor's
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
",1,0,0,0,0,0
947,A Macdonald refined topological vertex,"  We consider the refined topological vertex of Iqbal et al, as a function of
two parameters (x, y), and deform it by introducing Macdonald parameters (q,
t), as in the work of Vuletic on plane partitions, to obtain 'a Macdonald
refined topological vertex'. In the limit q -> t, we recover the refined
topological vertex of Iqbal et al. In the limit x -> y, we obtain a
qt-deformation of the topological vertex of Aganagic et al. Copies of the
vertex can be glued to obtain qt-deformed 5D instanton partition functions that
have well-defined 4D limits and, for generic values of (q, t), contain
infinite-towers of poles for every pole in the limit q -> t.
",0,0,1,0,0,0
1012,Guiding Reinforcement Learning Exploration Using Natural Language,"  In this work we present a technique to use natural language to help
reinforcement learning generalize to unseen environments. This technique uses
neural machine translation, specifically the use of encoder-decoder networks,
to learn associations between natural language behavior descriptions and
state-action information. We then use this learned model to guide agent
exploration using a modified version of policy shaping to make it more
effective at learning in unseen environments. We evaluate this technique using
the popular arcade game, Frogger, under ideal and non-ideal conditions. This
evaluation shows that our modified policy shaping algorithm improves over a
Q-learning agent as well as a baseline version of policy shaping.
",1,0,0,1,0,0
3071,Assessing the state of e-Readiness for Small and Medium Companies in Mexico: a Proposed Taxonomy and Adoption Model,"  Emerging economies frequently show a large component of their Gross Domestic
Product to be dependant on the economic activity of small and medium
enterprises. Nevertheless, e-business solutions are more likely designed for
large companies. SMEs seem to follow a classical family-based management, used
to traditional activities, rather than seeking new ways of adding value to
their business strategy. Thus, a large portion of a nations economy may be at
disadvantage for competition. This paper aims at assessing the state of
e-business readiness of Mexican SMEs based on already published e-business
evolution models and by means of a survey research design. Data is being
collected in three cities with differing sizes and infrastructure conditions.
Statistical results are expected to be presented. A second part of this
research aims at applying classical adoption models to suggest potential causal
relationships, as well as more suitable recommendations for development.
",0,0,0,0,0,1
929,On a question of Buchweitz about ranks of syzygies of modules of finite length,"  Let R be a local ring of dimension d. Buchweitz asks if the rank of the d-th
syzygy of a module of finite lengh is greater than or equal to the rank of the
d-th syzygy of the residue field, unless the module has finite projective
dimension. Assuming that R is Gorenstein, we prove that if the question is
affrmative, then R is a hypersurface. If moreover R has dimension two, then we
show that the converse also holds true.
",0,0,1,0,0,0
669,Bayesian Semisupervised Learning with Deep Generative Models,"  Neural network based generative models with discriminative components are a
powerful approach for semi-supervised learning. However, these techniques a)
cannot account for model uncertainty in the estimation of the model's
discriminative component and b) lack flexibility to capture complex stochastic
patterns in the label generation process. To avoid these problems, we first
propose to use a discriminative component with stochastic inputs for increased
noise flexibility. We show how an efficient Gibbs sampling procedure can
marginalize the stochastic inputs when inferring missing labels in this model.
Following this, we extend the discriminative component to be fully Bayesian and
produce estimates of uncertainty in its parameter values. This opens the door
for semi-supervised Bayesian active learning.
",0,0,0,1,0,0
976,Social versus Moral preferences in the Ultimatum Game: A theoretical model and an experiment,"  In the Ultimatum Game (UG) one player, named ""proposer"", has to decide how to
allocate a certain amount of money between herself and a ""responder"". If the
offer is greater than or equal to the responder's minimum acceptable offer
(MAO), then the money is split as proposed, otherwise, neither the proposer nor
the responder get anything. The UG has intrigued generations of behavioral
scientists because people in experiments blatantly violate the equilibrium
predictions that self-interested proposers offer the minimum available non-zero
amount, and self-interested responders accept. Why are these predictions
violated? Previous research has mainly focused on the role of social
preferences. Little is known about the role of general moral preferences for
doing the right thing, preferences that have been shown to play a major role in
other social interactions (e.g., Dictator Game and Prisoner's Dilemma). Here I
develop a theoretical model and an experiment designed to pit social
preferences against moral preferences. I find that, although people recognize
that offering half and rejecting low offers are the morally right things to do,
moral preferences have no causal impact on UG behavior. The experimental data
are indeed well fit by a model according to which: (i) high UG offers are
motivated by inequity aversion and, to a lesser extent, self-interest; (ii)
high MAOs are motivated by inequity aversion.
",0,0,0,0,1,0
824,Stochastic functional differential equations and sensitivity to their initial path,"  We consider systems with memory represented by stochastic functional
differential equations. Substantially, these are stochastic differential
equations with coefficients depending on the past history of the process
itself. Such coefficients are hence defined on a functional space. Models with
memory appear in many applications ranging from biology to finance. Here we
consider the results of some evaluations based on these models (e.g. the prices
of some financial products) and the risks connected to the choice of these
models. In particular we focus on the impact of the initial condition on the
evaluations. This problem is known as the analysis of sensitivity to the
initial condition and, in the terminology of finance, it is referred to as the
Delta. In this work the initial condition is represented by the relevant past
history of the stochastic functional differential equation. This naturally
leads to the redesign of the definition of Delta. We suggest to define it as a
functional directional derivative, this is a natural choice. For this we study
a representation formula which allows for its computation without requiring
that the evaluation functional is differentiable. This feature is particularly
relevant for applications. Our formula is achieved by studying an appropriate
relationship between Malliavin derivative and functional directional
derivative. For this we introduce the technique of {\it randomisation of the
initial condition}.
",0,0,1,0,0,0
872,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,"  We show that training a deep network using batch normalization is equivalent
to approximate inference in Bayesian models. We further demonstrate that this
finding allows us to make meaningful estimates of the model uncertainty using
conventional architectures, without modifications to the network or the
training procedure. Our approach is thoroughly validated by measuring the
quality of uncertainty in a series of empirical experiments on different tasks.
It outperforms baselines with strong statistical significance, and displays
competitive performance with recent Bayesian approaches.
",0,0,0,1,0,0
11552,Trading Strategies Generated by Path-dependent Functionals of Market Weights,"  Almost twenty years ago, E.R. Fernholz introduced portfolio generating
functions which can be used to construct a variety of portfolios, solely in the
terms of the individual companies' market weights. I. Karatzas and J. Ruf
recently developed another methodology for the functional construction of
portfolios, which leads to very simple conditions for strong relative arbitrage
with respect to the market. In this paper, both of these notions of functional
portfolio generation are generalized in a pathwise, probability-free setting;
portfolio generating functions are substituted by path-dependent functionals,
which involve the current market weights, as well as additional
bounded-variation functions of past and present market weights. This
generalization leads to a wider class of functionally-generated portfolios than
was heretofore possible, and yields improved conditions for outperforming the
market portfolio over suitable time-horizons.
",0,0,0,0,0,1
492,Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation,"  We address the problem of localisation of objects as bounding boxes in images
with weak labels. This weakly supervised object localisation problem has been
tackled in the past using discriminative models where each object class is
localised independently from other classes. We propose a novel framework based
on Bayesian joint topic modelling. Our framework has three distinctive
advantages over previous works: (1) All object classes and image backgrounds
are modelled jointly together in a single generative model so that ""explaining
away"" inference can resolve ambiguity and lead to better learning and
localisation. (2) The Bayesian formulation of the model enables easy
integration of prior knowledge about object appearance to compensate for
limited supervision. (3) Our model can be learned with a mixture of weakly
labelled and unlabelled data, allowing the large volume of unlabelled images on
the Internet to be exploited for learning. Extensive experiments on the
challenging VOC dataset demonstrate that our approach outperforms the
state-of-the-art competitors.
",1,0,0,0,0,0
498,Forecasting Transformative AI: An Expert Survey,"  Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.
",1,0,0,0,0,0
583,Kites and Residuated Lattices,"  We investigate a construction of an integral residuated lattice starting from
an integral residuated lattice and two sets with an injective mapping from one
set into the second one. The resulting algebra has a shape of a Chinese cascade
kite, therefore, we call this algebra simply a kite. We describe subdirectly
irreducible kites and we classify them. We show that the variety of integral
residuated lattices generated by kites is generated by all finite-dimensional
kites. In particular, we describe some homomorphisms among kites.
",0,0,1,0,0,0
1391,Solving constraint-satisfaction problems with distributed neocortical-like neuronal networks,"  Finding actions that satisfy the constraints imposed by both external inputs
and internal representations is central to decision making. We demonstrate that
some important classes of constraint satisfaction problems (CSPs) can be solved
by networks composed of homogeneous cooperative-competitive modules that have
connectivity similar to motifs observed in the superficial layers of neocortex.
The winner-take-all modules are sparsely coupled by programming neurons that
embed the constraints onto the otherwise homogeneous modular computational
substrate. We show rules that embed any instance of the CSPs planar four-color
graph coloring, maximum independent set, and Sudoku on this substrate, and
provide mathematical proofs that guarantee these graph coloring problems will
convergence to a solution. The network is composed of non-saturating linear
threshold neurons. Their lack of right saturation allows the overall network to
explore the problem space driven through the unstable dynamics generated by
recurrent excitation. The direction of exploration is steered by the constraint
neurons. While many problems can be solved using only linear inhibitory
constraints, network performance on hard problems benefits significantly when
these negative constraints are implemented by non-linear multiplicative
inhibition. Overall, our results demonstrate the importance of instability
rather than stability in network computation, and also offer insight into the
computational role of dual inhibitory mechanisms in neural circuits.
",0,0,0,0,1,0
6776,How production networks amplify economic growth,"  Technological improvement is the most important cause of long-term economic
growth, but the factors that drive it are still not fully understood. In
standard growth models technology is treated in the aggregate, and a main goal
has been to understand how growth depends on factors such as knowledge
production. But an economy can also be viewed as a network, in which producers
purchase goods, convert them to new goods, and sell them to households or other
producers. Here we develop a simple theory that shows how the network
properties of an economy can amplify the effects of technological improvements
as they propagate along chains of production. A key property of an industry is
its output multiplier, which can be understood as the average number of
production steps required to make a good. The model predicts that the output
multiplier of an industry predicts future changes in prices, and that the
average output multiplier of a country predicts future economic growth. We test
these predictions using data from the World Input Output Database and find
results in good agreement with the model. The results show how purely
structural properties of an economy, that have nothing to do with innovation or
human creativity, can exert an important influence on long-term growth.
",0,0,0,0,0,1
462,Shortening binary complexes and commutativity of $K$-theory with infinite products,"  We show that in Grayson's model of higher algebraic $K$-theory using binary
acyclic complexes, the complexes of length two suffice to generate the whole
group. Moreover, we prove that the comparison map from Nenashev's model for
$K_1$ to Grayson's model for $K_1$ is an isomorphism. It follows that algebraic
$K$-theory of exact categories commutes with infinite products.
",0,0,1,0,0,0
4280,Convolution Forgetting Curve Model for Repeated Learning,"  Most of mathematic forgetting curve models fit well with the forgetting data
under the learning condition of one time rather than repeated. In the paper, a
convolution model of forgetting curve is proposed to simulate the memory
process during learning. In this model, the memory ability (i.e. the central
procedure in the working memory model) and learning material (i.e. the input in
the working memory model) is regarded as the system function and the input
function, respectively. The status of forgetting (i.e. the output in the
working memory model) is regarded as output function or the convolution result
of the memory ability and learning material. The model is applied to simulate
the forgetting curves in different situations. The results show that the model
is able to simulate the forgetting curves not only in one time learning
condition but also in multi-times condition. The model is further verified in
the experiments of Mandarin tone learning for Japanese learners. And the
predicted curve fits well on the test points.
",1,0,0,0,1,0
276,On algebraically integrable domains in Euclidean spaces,"  Let $D$ be a bounded domain $D$ in $\mathbb R^n $ with infinitely smooth
boundary and $n$ is odd. We prove that if the volume cut off from the domain by
a hyperplane is an algebraic function of the hyperplane, free of real singular
points, then the domain is an ellipsoid. This partially answers a question of
V.I. Arnold: whether odd-dimensional ellipsoids are the only algebraically
integrable domains?
",0,0,1,0,0,0
3047,Deep Learning for Forecasting Stock Returns in the Cross-Section,"  Many studies have been undertaken by using machine learning techniques,
including neural networks, to predict stock returns. Recently, a method known
as deep learning, which achieves high performance mainly in image recognition
and speech recognition, has attracted attention in the machine learning field.
This paper implements deep learning to predict one-month-ahead stock returns in
the cross-section in the Japanese stock market and investigates the performance
of the method. Our results show that deep neural networks generally outperform
shallow neural networks, and the best networks also outperform representative
machine learning models. These results indicate that deep learning shows
promise as a skillful machine learning method to predict stock returns in the
cross-section.
",0,0,0,0,0,1
765,Surface tension of flowing soap films,"  The surface tension of flowing soap films is measured with respect to the
film thickness and the concentration of soap solution. We perform this
measurement by measuring the curvature of the nylon wires that bound the soap
film channel and use the measured curvature to parametrize the relation between
the surface tension and the tension of the wire. We find the surface tension of
our soap films increases when the film is relatively thin or made of soap
solution of low concentration, otherwise it approaches an asymptotic value 30
mN/m. A simple adsorption model with only two parameters describes our
observations reasonably well. With our measurements, we are also able to
measure Gibbs elasticity for our soap film.
",0,1,0,0,0,0
749,Reassessing Graphene Absorption and Emission Spectroscopy,"  We present a new paradigm for understanding optical absorption and hot
electron dynamics experiments in graphene. Our analysis pivots on assigning
proper importance to phonon assisted indirect processes and bleaching of direct
processes. We show indirect processes figure in the excess absorption in the UV
region. Experiments which were thought to indicate ultrafast relaxation of
electrons and holes, reaching a thermal distribution from an extremely
non-thermal one in under 5-10 fs, instead are explained by the nascent electron
and hole distributions produced by indirect transitions. These need no
relaxation or ad-hoc energy removal to agree with the observed emission spectra
and fast pulsed absorption spectra. The fast emission following pulsed
absorption is dominated by phonon assisted processes, which vastly outnumber
direct ones and are always available, connecting any electron with any hole any
time. Calculations are given, including explicitly calculating the magnitude of
indirect processes, supporting these views.
",0,1,0,0,0,0
299,ACVAE-VC: Non-parallel many-to-many voice conversion with auxiliary classifier variational autoencoder,"  This paper proposes a non-parallel many-to-many voice conversion (VC) method
using a variant of the conditional variational autoencoder (VAE) called an
auxiliary classifier VAE (ACVAE). The proposed method has three key features.
First, it adopts fully convolutional architectures to construct the encoder and
decoder networks so that the networks can learn conversion rules that capture
time dependencies in the acoustic feature sequences of source and target
speech. Second, it uses an information-theoretic regularization for the model
training to ensure that the information in the attribute class label will not
be lost in the conversion process. With regular CVAEs, the encoder and decoder
are free to ignore the attribute class label input. This can be problematic
since in such a situation, the attribute class label will have little effect on
controlling the voice characteristics of input speech at test time. Such
situations can be avoided by introducing an auxiliary classifier and training
the encoder and decoder so that the attribute classes of the decoder outputs
are correctly predicted by the classifier. Third, it avoids producing
buzzy-sounding speech at test time by simply transplanting the spectral details
of the input speech into its converted version. Subjective evaluation
experiments revealed that this simple method worked reasonably well in a
non-parallel many-to-many speaker identity conversion task.
",1,0,0,1,0,0
1137,Landau-Ginzburg theory of cortex dynamics: Scale-free avalanches emerge at the edge of synchronization,"  Understanding the origin, nature, and functional significance of complex
patterns of neural activity, as recorded by diverse electrophysiological and
neuroimaging techniques, is a central challenge in neuroscience. Such patterns
include collective oscillations emerging out of neural synchronization as well
as highly heterogeneous outbursts of activity interspersed by periods of
quiescence, called ""neuronal avalanches."" Much debate has been generated about
the possible scale invariance or criticality of such avalanches and its
relevance for brain function. Aimed at shedding light onto this, here we
analyze the large-scale collective properties of the cortex by using a
mesoscopic approach following the principle of parsimony of Landau-Ginzburg.
Our model is similar to that of Wilson-Cowan for neural dynamics but crucially,
includes stochasticity and space; synaptic plasticity and inhibition are
considered as possible regulatory mechanisms. Detailed analyses uncover a phase
diagram including down-state, synchronous, asynchronous, and up-state phases
and reveal that empirical findings for neuronal avalanches are consistently
reproduced by tuning our model to the edge of synchronization. This reveals
that the putative criticality of cortical dynamics does not correspond to a
quiescent-to-active phase transition as usually assumed in theoretical
approaches but to a synchronization phase transition, at which incipient
oscillations and scale-free avalanches coexist. Furthermore, our model also
accounts for up and down states as they occur (e.g., during deep sleep). This
approach constitutes a framework to rationalize the possible collective phases
and phase transitions of cortical networks in simple terms, thus helping to
shed light on basic aspects of brain functioning from a very broad perspective.
",0,0,0,0,1,0
550,Thermophoretic MHD Flow and Non-linear Radiative Heat Transfer with Convective Boundary Conditions over a Non-linearly Stretching Sheet,"  The effects of MHD boundary layer flow of non-linear thermal radiation with
convective heat transfer and non-uniform heat source/sink in presence of
thermophortic velocity and chemical reaction investigated in this study.
Suitable similarity transformation are used to solve the partial ordinary
differential equation of considered governing flow. Runge-Kutta fourth fifth
order Fehlberg method with shooting techniques are used to solved
non-dimensional governing equations. The variation of different parameters such
as thermophoretic parameter, chemical reaction parameter, non- uniform heat
source/sink parameters are studied on velocity, temperature and concentration
profiles, and are described by suitable graphs and tables. The obtained results
are in very well agreement with previous results.
",0,1,0,0,0,0
6560,A Game of Martingales,"  We consider a two player dynamic game played over $T \leq \infty$ periods. In
each period each player chooses any probability distribution with support on
$[0,1]$ with a given mean, where the mean is the realized value of the draw
from the previous period. The player with the highest realization in the period
achieves a payoff of $1$, and the other player, $0$; and each player seeks to
maximize the discounted sum of his per-period payoffs over the whole time
horizon. We solve for the unique subgame perfect equilibrium of this game, and
establish properties of the equilibrium strategies and payoffs in the limit.
The solution and comparative statics thereof provide insight about
intertemporal choice with status concerns. In particular we find that patient
players take fewer risks.
",0,0,0,0,0,1
11365,Kinetic Theory for Finance Brownian Motion from Microscopic Dynamics,"  Recent technological development has enabled researchers to study social
phenomena scientifically in detail and financial markets has particularly
attracted physicists since the Brownian motion has played the key role as in
physics. In our previous report (arXiv:1703.06739; to appear in Phys. Rev.
Lett.), we have presented a microscopic model of trend-following high-frequency
traders (HFTs) and its theoretical relation to the dynamics of financial
Brownian motion, directly supported by a data analysis of tracking trajectories
of individual HFTs in a financial market. Here we show the mathematical
foundation for the HFT model paralleling to the traditional kinetic theory in
statistical physics. We first derive the time-evolution equation for the
phase-space distribution for the HFT model exactly, which corresponds to the
Liouville equation in conventional analytical mechanics. By a systematic
reduction of the Liouville equation for the HFT model, the
Bogoliubov-Born-Green-Kirkwood-Yvon hierarchal equations are derived for
financial Brownian motion. We then derive the Boltzmann-like and Langevin-like
equations for the order-book and the price dynamics by making the assumption of
molecular chaos. The qualitative behavior of the model is asymptotically
studied by solving the Boltzmann-like and Langevin-like equations for the large
number of HFTs, which is numerically validated through the Monte-Carlo
simulation. Our kinetic description highlights the parallel mathematical
structure between the financial Brownian motion and the physical Brownian
motion.
",0,0,0,0,0,1
2819,Exploring the Interconnectedness of Cryptocurrencies using Correlation Networks,"  Correlation networks were used to detect characteristics which, although
fixed over time, have an important influence on the evolution of prices over
time. Potentially important features were identified using the websites and
whitepapers of cryptocurrencies with the largest userbases. These were assessed
using two datasets to enhance robustness: one with fourteen cryptocurrencies
beginning from 9 November 2017, and a subset with nine cryptocurrencies
starting 9 September 2016, both ending 6 March 2018. Separately analysing the
subset of cryptocurrencies raised the number of data points from 115 to 537,
and improved robustness to changes in relationships over time. Excluding USD
Tether, the results showed a positive association between different
cryptocurrencies that was statistically significant. Robust, strong positive
associations were observed for six cryptocurrencies where one was a fork of the
other; Bitcoin / Bitcoin Cash was an exception. There was evidence for the
existence of a group of cryptocurrencies particularly associated with Cardano,
and a separate group correlated with Ethereum. The data was not consistent with
a token's functionality or creation mechanism being the dominant determinants
of the evolution of prices over time but did suggest that factors other than
speculation contributed to the price.
",0,0,0,0,0,1
301,A dual framework for low-rank tensor completion,"  One of the popular approaches for low-rank tensor completion is to use the
latent trace norm regularization. However, most existing works in this
direction learn a sparse combination of tensors. In this work, we fill this gap
by proposing a variant of the latent trace norm that helps in learning a
non-sparse combination of tensors. We develop a dual framework for solving the
low-rank tensor completion problem. We first show a novel characterization of
the dual solution space with an interesting factorization of the optimal
solution. Overall, the optimal solution is shown to lie on a Cartesian product
of Riemannian manifolds. Furthermore, we exploit the versatile Riemannian
optimization framework for proposing computationally efficient trust region
algorithm. The experiments illustrate the efficacy of the proposed algorithm on
several real-world datasets across applications.
",1,0,0,1,0,0
506,Fast non-destructive parallel readout of neutral atom registers in optical potentials,"  We demonstrate the parallel and non-destructive readout of the hyperfine
state for optically trapped $^{87}$Rb atoms. The scheme is based on
state-selective fluorescence imaging and achieves detection fidelities $>$98%
within 10$\,$ms, while keeping 99% of the atoms trapped. For the read-out of
dense arrays of neutral atoms in optical lattices, where the fluorescence
images of neighboring atoms overlap, we apply a novel image analysis technique
using Bayesian inference to determine the internal state of multiple atoms. Our
method is scalable to large neutral atom registers relevant for future quantum
information processing tasks requiring fast and non-destructive readout and can
also be used for the simultaneous read-out of quantum information stored in
internal qubit states and in the atoms' positions.
",0,1,0,0,0,0
941,Feedback optimal controllers for the Heston model,"  We prove the existence of an optimal feedback controller for a stochastic
optimization problem constituted by a variation of the Heston model, where a
stochastic input process is added in order to minimize a given performance
criterion. The stochastic feedback controller is searched by solving a
nonlinear backward parabolic equation for which one proves the existence of a
martingale solution.
",0,0,1,0,0,0
537,End-to-end Planning of Fixed Millimeter-Wave Networks,"  This article discusses a framework to support the design and end-to-end
planning of fixed millimeter-wave networks. Compared to traditional techniques,
the framework allows an organization to quickly plan a deployment in a
cost-effective way. We start by using LiDAR data---basically, a 3D point cloud
captured from a city---to estimate potential sites to deploy antennas and
whether there is line-of-sight between them. With that data on hand, we use
combinatorial optimization techniques to determine the optimal set of locations
and how they should communicate with each other, to satisfy engineering (e.g.,
latency, polarity), design (e.g., reliability) and financial (e.g., total cost
of operation) constraints. The primary goal is to connect as many people as
possible to the network. Our methodology can be used for strategic planning
when an organization is in the process of deciding whether to adopt a
millimeter-wave technology or choosing between locations, or for operational
planning when conducting a detailed design of the actual network to be deployed
in a selected location.
",1,0,1,0,0,0
333,On types of degenerate critical points of real polynomial functions,"  In this paper, we consider the problem of identifying the type (local
minimizer, maximizer or saddle point) of a given isolated real critical point
$c$, which is degenerate, of a multivariate polynomial function $f$. To this
end, we introduce the definition of faithful radius of $c$ by means of the
curve of tangency of $f$. We show that the type of $c$ can be determined by the
global extrema of $f$ over the Euclidean ball centered at $c$ with a faithful
radius.We propose algorithms to compute a faithful radius of $c$ and determine
its type.
",0,0,1,0,0,0
3337,Consistent Inter-Model Specification for Time-Homogeneous SPX Stochastic Volatility and VIX Market Models,"  This paper shows how to recover stochastic volatility models (SVMs) from
market models for the VIX futures term structure. Market models have more
flexibility for fitting of curves than do SVMs, and therefore they are
better-suited for pricing VIX futures and derivatives. But the VIX itself is a
derivative of the S&P500 (SPX) and it is common practice to price SPX
derivatives using an SVM. Hence, a consistent model for both SPX and VIX
derivatives would be one where the SVM is obtained by inverting the market
model. This paper's main result is a method for the recovery of a stochastic
volatility function as the output of an inverse problem, with the inputs given
by a VIX futures market model. Analysis will show that some conditions need to
be met in order for there to not be any inter-model arbitrage or mis-priced
derivatives. Given these conditions the inverse problem can be solved. Several
models are analyzed and explored numerically to gain a better understanding of
the theory and its limitations.
",0,0,0,0,0,1
1558,Higher order molecular organisation as a source of biological function,"  Molecular interactions have widely been modelled as networks. The local
wiring patterns around molecules in molecular networks are linked with their
biological functions. However, networks model only pairwise interactions
between molecules and cannot explicitly and directly capture the higher order
molecular organisation, such as protein complexes and pathways. Hence, we ask
if hypergraphs (hypernetworks), that directly capture entire complexes and
pathways along with protein-protein interactions (PPIs), carry additional
functional information beyond what can be uncovered from networks of pairwise
molecular interactions. The mathematical formalism of a hypergraph has long
been known, but not often used in studying molecular networks due to the lack
of sophisticated algorithms for mining the underlying biological information
hidden in the wiring patterns of molecular systems modelled as hypernetworks.
We propose a new, multi-scale, protein interaction hypernetwork model that
utilizes hypergraphs to capture different scales of protein organization,
including PPIs, protein complexes and pathways. In analogy to graphlets, we
introduce hypergraphlets, small, connected, non-isomorphic, induced
sub-hypergraphs of a hypergraph, to quantify the local wiring patterns of these
multi-scale molecular hypergraphs and to mine them for new biological
information. We apply them to model the multi-scale protein networks of baker
yeast and human and show that the higher order molecular organisation captured
by these hypergraphs is strongly related to the underlying biology.
Importantly, we demonstrate that our new models and data mining tools reveal
different, but complementary biological information compared to classical PPI
networks. We apply our hypergraphlets to successfully predict biological
functions of uncharacterised proteins.
",0,0,0,0,1,0
15808,Complexity of products: the effect of data regularisation,"  Among several developments, the field of Economic Complexity (EC) has notably
seen the introduction of two new techniques. One is the Bootstrapped Selective
Predictability Scheme (SPSb), which can provide quantitative forecasts of the
Gross Domestic Product of countries. The other, Hidden Markov Model (HMM)
regularisation, denoises the datasets typically employed in the literature. We
contribute to EC along three different directions. First, we prove the
convergence of the SPSb algorithm to a well-known statistical learning
technique known as Nadaraya-Watson Kernel regression. The latter has
significantly lower time complexity, produces deterministic results, and it is
interchangeable with SPSb for the purpose of making predictions. Second, we
study the effects of HMM regularization on the Product Complexity and logPRODY
metrics, for which a model of time evolution has been recently proposed. We
find confirmation for the original interpretation of the logPRODY model as
describing the change in the global market structure of products with new
insights allowing a new interpretation of the Complexity measure, for which we
propose a modification. Third, we explore new effects of regularisation on the
data. We find that it reduces noise, and observe for the first time that it
increases nestedness in the export network adjacency matrix.
",0,0,0,0,0,1
6354,Robust parameter determination in epidemic models with analytical descriptions of uncertainties,"  Compartmental equations are primary tools in disease spreading studies. Their
predictions are accurate for large populations but disagree with empirical and
simulated data for finite populations, where uncertainties become a relevant
factor. Starting from the agent-based approach, we investigate the role of
uncertainties and autocorrelation functions in SIS epidemic model, including
their relationship with epidemiological variables. We find new differential
equations that take uncertainties into account. The findings provide improved
predictions to the SIS model and it can offer new insights for emerging
diseases.
",0,0,0,0,1,0
473,"Time-Reversal Breaking in QCD$_4$, Walls, and Dualities in 2+1 Dimensions","  We study $SU(N)$ Quantum Chromodynamics (QCD) in 3+1 dimensions with $N_f$
degenerate fundamental quarks with mass $m$ and a $\theta$-parameter. For
generic $m$ and $\theta$ the theory has a single gapped vacuum. However, as
$\theta$ is varied through $\theta=\pi$ for large $m$ there is a first order
transition. For $N_f=1$ the first order transition line ends at a point with a
massless $\eta'$ particle (for all $N$) and for $N_f>1$ the first order
transition ends at $m=0$, where, depending on the value of $N_f$, the IR theory
has free Nambu-Goldstone bosons, an interacting conformal field theory, or a
free gauge theory. Even when the $4d$ bulk is smooth, domain walls and
interfaces can have interesting phase transitions separating different $3d$
phases. These turn out to be the phases of the recently studied $3d$
Chern-Simons matter theories, thus relating the dynamics of QCD$_4$ and
QCD$_3$, and, in particular, making contact with the recently discussed
dualities in 2+1 dimensions. For example, when the massless $4d$ theory has an
$SU(N_f)$ sigma model, the domain wall theory at low (nonzero) mass supports a
$3d$ massless $CP^{N_f-1}$ nonlinear $\sigma$-model with a Wess-Zumino term, in
agreement with the conjectured dynamics in 2+1 dimensions.
",0,1,0,0,0,0
421,Flows along arch filaments observed in the GRIS 'very fast spectroscopic mode',"  A new generation of solar instruments provides improved spectral, spatial,
and temporal resolution, thus facilitating a better understanding of dynamic
processes on the Sun. High-resolution observations often reveal
multiple-component spectral line profiles, e.g., in the near-infrared He I
10830 \AA\ triplet, which provides information about the chromospheric velocity
and magnetic fine structure. We observed an emerging flux region, including two
small pores and an arch filament system, on 2015 April 17 with the 'very fast
spectroscopic mode' of the GREGOR Infrared Spectrograph (GRIS) situated at the
1.5-meter GREGOR solar telescope at Observatorio del Teide, Tenerife, Spain. We
discuss this method of obtaining fast (one per minute) spectral scans of the
solar surface and its potential to follow dynamic processes on the Sun. We
demonstrate the performance of the 'very fast spectroscopic mode' by tracking
chromospheric high-velocity features in the arch filament system.
",0,1,0,0,0,0
6937,Neural correlates of episodic memory in the Memento cohort,"  IntroductionThe free and cued selective reminding test is used to identify
memory deficits in mild cognitive impairment and demented patients. It allows
assessing three processes: encoding, storage, and recollection of verbal
episodic memory.MethodsWe investigated the neural correlates of these three
memory processes in a large cohort study. The Memento cohort enrolled 2323
outpatients presenting either with subjective cognitive decline or mild
cognitive impairment who underwent cognitive, structural MRI and, for a subset,
fluorodeoxyglucose--positron emission tomography evaluations.ResultsEncoding
was associated with a network including parietal and temporal cortices; storage
was mainly associated with entorhinal and parahippocampal regions, bilaterally;
retrieval was associated with a widespread network encompassing frontal
regions.DiscussionThe neural correlates of episodic memory processes can be
assessed in large and standardized cohorts of patients at risk for Alzheimer's
disease. Their relation to pathophysiological markers of Alzheimer's disease
remains to be studied.
",0,0,0,0,1,0
202,On a common refinement of Stark units and Gross-Stark units,"  The purpose of this paper is to formulate and study a common refinement of a
version of Stark's conjecture and its $p$-adic analogue, in terms of Fontaine's
$p$-adic period ring and $p$-adic Hodge theory. We construct period-ring-valued
functions under a generalization of Yoshida's conjecture on the transcendental
parts of CM-periods. Then we conjecture a reciprocity law on their special
values concerning the absolute Frobenius action. We show that our conjecture
implies a part of Stark's conjecture when the base field is an arbitrary real
field and the splitting place is its real place. It also implies a refinement
of the Gross-Stark conjecture under a certain assumption. When the base field
is the rational number field, our conjecture follows from Coleman's formula on
Fermat curves. We also prove some partial results in other cases.
",0,0,1,0,0,0
5941,Zinc oxide induces the stringent response and major reorientations in the central metabolism of Bacillus subtilis,"  Microorganisms, such as bacteria, are one of the first targets of
nanoparticles in the environment. In this study, we tested the effect of two
nanoparticles, ZnO and TiO2, with the salt ZnSO4 as the control, on the
Gram-positive bacterium Bacillus subtilis by 2D gel electrophoresis-based
proteomics. Despite a significant effect on viability (LD50), TiO2 NPs had no
detectable effect on the proteomic pattern, while ZnO NPs and ZnSO4
significantly modified B. subtilis metabolism. These results allowed us to
conclude that the effects of ZnO observed in this work were mainly attributable
to Zn dissolution in the culture media. Proteomic analysis highlighted twelve
modulated proteins related to central metabolism: MetE and MccB (cysteine
metabolism), OdhA, AspB, IolD, AnsB, PdhB and YtsJ (Krebs cycle) and XylA,
YqjI, Drm and Tal (pentose phosphate pathway). Biochemical assays, such as free
sulfhydryl, CoA-SH and malate dehydrogenase assays corroborated the observed
central metabolism reorientation and showed that Zn stress induced oxidative
stress, probably as a consequence of thiol chelation stress by Zn ions. The
other patterns affected by ZnO and ZnSO4 were the stringent response and the
general stress response. Nine proteins involved in or controlled by the
stringent response showed a modified expression profile in the presence of ZnO
NPs or ZnSO4: YwaC, SigH, YtxH, YtzB, TufA, RplJ, RpsB, PdhB and Mbl. An
increase in the ppGpp concentration confirmed the involvement of the stringent
response during a Zn stress. All these metabolic reorientations in response to
Zn stress were probably the result of complex regulatory mechanisms including
at least the stringent response via YwaC.
",0,0,0,0,1,0
632,On the Limitations of Representing Functions on Sets,"  Recent work on the representation of functions on sets has considered the use
of summation in a latent space to enforce permutation invariance. In
particular, it has been conjectured that the dimension of this latent space may
remain fixed as the cardinality of the sets under consideration increases.
However, we demonstrate that the analysis leading to this conjecture requires
mappings which are highly discontinuous and argue that this is only of limited
practical use. Motivated by this observation, we prove that an implementation
of this model via continuous mappings (as provided by e.g. neural networks or
Gaussian processes) actually imposes a constraint on the dimensionality of the
latent space. Practical universal function representation for set inputs can
only be achieved with a latent dimension at least the size of the maximum
number of input elements.
",1,0,0,1,0,0
792,Phase Synchronization on Spacially Embeded Duplex Networks with Total Cost Constraint,"  Synchronization on multiplex networks have attracted increasing attention in
the past few years. We investigate collective behaviors of Kuramoto oscillators
on single layer and duplex spacial networks with total cost restriction, which
was introduced by Li et. al [Li G., Reis S. D., Moreira A. A., Havlin S.,
Stanley H. E. and Jr A. J., {\it Phys. Rev. Lett.} 104, 018701 (2010)] and
termed as the Li network afterwards. In the Li network model, with the increase
of its spacial exponent, the network's structure will vary from the random type
to the small-world one, and finally to the regular lattice.We first explore how
the spacial exponent influences the synchronizability of Kuramoto oscillators
on single layer Li networks and find that the closer the Li network is to a
regular lattice, the more difficult for it to evolve into synchronization. Then
we investigate synchronizability of duplex Li networks and find that the
existence of inter-layer interaction can greatly enhance inter-layer and global
synchronizability. When the inter-layer coupling strength is larger than a
certain critical value, whatever the intra-layer coupling strength is, the
inter-layer synchronization will always occur. Furthermore, on single layer Li
networks, nodes with larger degrees more easily reach global synchronization,
while on duplex Li networks, this phenomenon becomes much less obvious.
Finally, we study the impact of inter-link density on global synchronization
and obtain that sparse inter-links can lead to the emergence of global
synchronization for duplex Li networks just as dense inter-links do. In a word,
inter-layer interaction plays a vital role in determining synchronizability for
duplex spacial networks with total cost constraint.
",0,1,0,0,0,0
401,Deep Within-Class Covariance Analysis for Robust Audio Representation Learning,"  Convolutional Neural Networks (CNNs) can learn effective features, though
have been shown to suffer from a performance drop when the distribution of the
data changes from training to test data. In this paper we analyze the internal
representations of CNNs and observe that the representations of unseen data in
each class, spread more (with higher variance) in the embedding space of the
CNN compared to representations of the training data. More importantly, this
difference is more extreme if the unseen data comes from a shifted
distribution. Based on this observation, we objectively evaluate the degree of
representation's variance in each class via eigenvalue decomposition on the
within-class covariance of the internal representations of CNNs and observe the
same behaviour. This can be problematic as larger variances might lead to
mis-classification if the sample crosses the decision boundary of its class. We
apply nearest neighbor classification on the representations and empirically
show that the embeddings with the high variance actually have significantly
worse KNN classification performances, although this could not be foreseen from
their end-to-end classification results. To tackle this problem, we propose
Deep Within-Class Covariance Analysis (DWCCA), a deep neural network layer that
significantly reduces the within-class covariance of a DNN's representation,
improving performance on unseen test data from a shifted distribution. We
empirically evaluate DWCCA on two datasets for Acoustic Scene Classification
(DCASE2016 and DCASE2017). We demonstrate that not only does DWCCA
significantly improve the network's internal representation, it also increases
the end-to-end classification accuracy, especially when the test set exhibits a
distribution shift. By adding DWCCA to a VGG network, we achieve around 6
percentage points improvement in the case of a distribution mismatch.
",1,0,0,0,0,0
420,Tuning quantum non-local effects in graphene plasmonics,"  The response of an electron system to electromagnetic fields with sharp
spatial variations is strongly dependent on quantum electronic properties, even
in ambient conditions, but difficult to access experimentally. We use
propagating graphene plasmons, together with an engineered dielectric-metallic
environment, to probe the graphene electron liquid and unveil its detailed
electronic response at short wavelengths.The near-field imaging experiments
reveal a parameter-free match with the full theoretical quantum description of
the massless Dirac electron gas, in which we identify three types of quantum
effects as keys to understanding the experimental response of graphene to
short-ranged terahertz electric fields. The first type is of single-particle
nature and is related to shape deformations of the Fermi surface during a
plasmon oscillations. The second and third types are a many-body effect
controlled by the inertia and compressibility of the interacting electron
liquid in graphene. We demonstrate how, in principle, our experimental approach
can determine the full spatiotemporal response of an electron system.
",0,1,0,0,0,0
11006,A Numerical Study of Carr and Lee's Correlation Immunization Strategy for Volatility Derivatives,"  In their seminal work `Robust Replication of Volatility Derivatives,' Carr
and Lee show how to robustly price and replicate a variety of claims written on
the quadratic variation of a risky asset under the assumption that the asset's
volatility process is independent of the Brownian motion that drives the
asset's price. Additionally, they propose a correlation immunization method
that minimizes the pricing and hedging error that results when the correlation
between the risky asset's price and volatility is nonzero. In this paper, we
perform a number of Monte Carlo experiments to test the effectiveness of Carr
and Lee's immunization strategy. Our results indicate that the correlation
immunization method is an effective means of reducing pricing and hedging
errors that result from nonzero correlation.
",0,0,0,0,0,1
13991,Bayesian mean-variance analysis: Optimal portfolio selection under parameter uncertainty,"  The paper solves the problem of optimal portfolio choice when the parameters
of the asset returns distribution, like the mean vector and the covariance
matrix are unknown and have to be estimated by using historical data of the
asset returns. The new approach employs the Bayesian posterior predictive
distribution which is the distribution of the future realization of the asset
returns given the observable sample. The parameters of the posterior predictive
distributions are functions of the observed data values and, consequently, the
solution of the optimization problem is expressed in terms of data only and
does not depend on unknown quantities. In contrast, the optimization problem of
the traditional approach is based on unknown quantities which are estimated in
the second step leading to a suboptimal solution. We also derive a very useful
stochastic representation of the posterior predictive distribution whose
application leads not only to the solution of the considered optimization
problem, but provides the posterior predictive distribution of the optimal
portfolio return used to construct a prediction interval. A Bayesian efficient
frontier, a set of optimal portfolios obtained by employing the posterior
predictive distribution, is constructed as well. Theoretically and using real
data we show that the Bayesian efficient frontier outperforms the sample
efficient frontier, a common estimator of the set of optimal portfolios known
to be overoptimistic.
",0,0,0,0,0,1
257,Gaussian Kernel in Quantum Paradigm,"  The Gaussian kernel is a very popular kernel function used in many
machine-learning algorithms, especially in support vector machines (SVM). For
nonlinear training instances in machine learning, it often outperforms
polynomial kernels in model accuracy. We use Gaussian kernel profoundly in
formulating nonlinear classical SVM. In the recent research, P. Rebentrost
et.al. discuss a very elegant quantum version of least square support vector
machine using the quantum version of polynomial kernel, which is exponentially
faster than the classical counterparts. In this paper, we have demonstrated a
quantum version of the Gaussian kernel and analyzed its complexity in the
context of quantum SVM. Our analysis shows that the computational complexity of
the quantum Gaussian kernel is O(\epsilon^(-1)logN) with N-dimensional
instances and \epsilon with a Taylor remainder error term |R_m (\epsilon^(-1)
logN)|.
",1,0,0,0,0,0
2104,Spatial risk measures and rate of spatial diversification,"  An accurate assessment of the risk of extreme environmental events is of
great importance for populations, authorities and the banking/insurance
industry. Koch (2017) introduced a notion of spatial risk measure and a
corresponding set of axioms which are well suited to analyze the risk due to
events having a spatial extent, precisely such as environmental phenomena. The
axiom of asymptotic spatial homogeneity is of particular interest since it
allows one to quantify the rate of spatial diversification when the region
under consideration becomes large. In this paper, we first investigate the
general concepts of spatial risk measures and corresponding axioms further. We
also explain the usefulness of this theory for the actuarial practice. Second,
in the case of a general cost field, we especially give sufficient conditions
such that spatial risk measures associated with expectation, variance,
Value-at-Risk as well as expected shortfall and induced by this cost field
satisfy the axioms of asymptotic spatial homogeneity of order 0, -2, -1 and -1,
respectively. Last but not least, in the case where the cost field is a
function of a max-stable random field, we mainly provide conditions on both the
function and the max-stable field ensuring the latter properties. Max-stable
random fields are relevant when assessing the risk of extreme events since they
appear as a natural extension of multivariate extreme-value theory to the level
of random fields. Overall, this paper improves our understanding of spatial
risk measures as well as of their properties with respect to the space variable
and generalizes many results obtained in Koch (2017).
",0,0,0,0,0,1
535,Local methods for blocks of finite simple groups,"  This survey is about old and new results about the modular representation
theory of finite reductive groups with a strong emphasis on local methods. This
includes subpairs, Brauer's Main Theorems, fusion, Rickard equivalences. In the
defining characteristic we describe the relation between $p$-local subgroups
and parabolic subgroups, then give classical consequences on simple modules and
blocks, including the Alperin weight conjecture in that case. In the
non-defining characteristics, we sketch a picture of the local methods
pioneered by Fong-Srinivasan in the determination of blocks and their ordinary
characters. This includes the relationship with Lusztig's twisted induction and
the determination of defect groups. We conclude with a survey of the results
and methods by Bonnafé-Dat-Rouquier giving Morita equivalences between blocks
that preserve defect groups and the local structures.
The text grew out of the course and talks given by the author in July and
September 2016 during the program ""Local representation theory and simple
groups"" at CIB Lausanne. Written Oct 2017, to appear in a proceedings volume
published by EMS.
",0,0,1,0,0,0
741,The statistical significance filter leads to overconfident expectations of replicability,"  We show that publishing results using the statistical significance
filter---publishing only when the p-value is less than 0.05---leads to a
vicious cycle of overoptimistic expectation of the replicability of results.
First, we show analytically that when true statistical power is relatively low,
computing power based on statistically significant results will lead to
overestimates of power. Then, we present a case study using 10 experimental
comparisons drawn from a recently published meta-analysis in psycholinguistics
(Jäger et al., 2017). We show that the statistically significant results
yield an illusion of replicability. This illusion holds even if the researcher
doesn't conduct any formal power analysis but just uses statistical
significance to informally assess robustness (i.e., replicability) of results.
",0,0,1,1,0,0
224,Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines: A Majorization--Minimization Algorithm Approach,"  Support vector machines (SVMs) are an important tool in modern data analysis.
Traditionally, support vector machines have been fitted via quadratic
programming, either using purpose-built or off-the-shelf algorithms. We present
an alternative approach to SVM fitting via the majorization--minimization (MM)
paradigm. Algorithms that are derived via MM algorithm constructions can be
shown to monotonically decrease their objectives at each iteration, as well as
be globally convergent to stationary points. We demonstrate the construction of
iteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,
for SVM risk minimization problems involving the hinge, least-square,
squared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net
penalizations. Successful implementations of our algorithms are presented via
some numerical examples.
",1,0,0,1,0,0
836,Affine Rough Models,"  The goal of this survey article is to explain and elucidate the affine
structure of recent models appearing in the rough volatility literature, and
show how it leads to exponential-affine transform formulas.
",0,0,0,0,0,1
414,On Functional Graphs of Quadratic Polynomials,"  We study functional graphs generated by quadratic polynomials over prime
fields. We introduce efficient algorithms for methodical computations and
provide the values of various direct and cumulative statistical parameters of
interest. These include: the number of connected functional graphs, the number
of graphs having a maximal cycle, the number of cycles of fixed size, the
number of components of fixed size, as well as the shape of trees extracted
from functional graphs. We particularly focus on connected functional graphs,
that is, the graphs which contain only one component (and thus only one cycle).
Based on the results of our computations, we formulate several conjectures
highlighting the similarities and differences between these functional graphs
and random mappings.
",0,0,1,0,0,0
750,A deep generative model for single-cell RNA sequencing with application to detecting differentially expressed genes,"  We propose a probabilistic model for interpreting gene expression levels that
are observed through single-cell RNA sequencing. In the model, each cell has a
low-dimensional latent representation. Additional latent variables account for
technical effects that may erroneously set some observations of gene expression
levels to zero. Conditional distributions are specified by neural networks,
giving the proposed model enough flexibility to fit the data well. We use
variational inference and stochastic optimization to approximate the posterior
distribution. The inference procedure scales to over one million cells, whereas
competing algorithms do not. Even for smaller datasets, for several tasks, the
proposed procedure outperforms state-of-the-art methods like ZIFA and
ZINB-WaVE. We also extend our framework to take into account batch effects and
other confounding factors and propose a natural Bayesian hypothesis framework
for differential expression that outperforms tradition DESeq2.
",1,0,0,1,0,0
822,Gorenstein homological properties of tensor rings,"  Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule,
which is finitely generated on both sides. We study Gorenstein homological
properties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$
is Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein
projective $T_R(M)$-modules in terms of $R$-modules.
",0,0,1,0,0,0
280,A Survey of Model Compression and Acceleration for Deep Neural Networks,"  Deep convolutional neural networks (CNNs) have recently achieved great
success in many visual recognition tasks. However, existing deep neural network
models are computationally expensive and memory intensive, hindering their
deployment in devices with low memory resources or in applications with strict
latency requirements. Therefore, a natural thought is to perform model
compression and acceleration in deep networks without significantly decreasing
the model performance. During the past few years, tremendous progress has been
made in this area. In this paper, we survey the recent advanced techniques for
compacting and accelerating CNNs model developed. These techniques are roughly
categorized into four schemes: parameter pruning and sharing, low-rank
factorization, transferred/compact convolutional filters, and knowledge
distillation. Methods of parameter pruning and sharing will be described at the
beginning, after that the other techniques will be introduced. For each scheme,
we provide insightful analysis regarding the performance, related applications,
advantages, and drawbacks etc. Then we will go through a few very recent
additional successful methods, for example, dynamic capacity networks and
stochastic depths networks. After that, we survey the evaluation matrix, the
main datasets used for evaluating the model performance and recent benchmarking
efforts. Finally, we conclude this paper, discuss remaining challenges and
possible directions on this topic.
",1,0,0,0,0,0
571,A novel distribution-free hybrid regression model for manufacturing process efficiency improvement,"  This work is motivated by a particular problem of a modern paper
manufacturing industry, in which maximum efficiency of the fiber-filler
recovery process is desired. A lot of unwanted materials along with valuable
fibers and fillers come out as a by-product of the paper manufacturing process
and mostly goes as waste. The job of an efficient Krofta supracell is to
separate the unwanted materials from the valuable ones so that fibers and
fillers can be collected from the waste materials and reused in the
manufacturing process. The efficiency of Krofta depends on several crucial
process parameters and monitoring them is a difficult proposition. To solve
this problem, we propose a novel hybridization of regression trees (RT) and
artificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of
low recovery percentage of the supracell. This model is used to achieve the
goal of improving supracell efficiency, viz., gain in percentage recovery. In
addition, theoretical results for the universal consistency of the proposed
model are given with the optimal value of a vital model parameter. Experimental
findings show that the proposed hybrid RT-ANN model achieves higher accuracy in
predicting Krofta recovery percentage than other conventional regression models
for solving the Krofta efficiency problem. This work will help the paper
manufacturing company to become environmentally friendly with minimal
ecological damage and improved waste recovery.
",0,0,0,1,0,0
3054,Fast Nonconvex Deconvolution of Calcium Imaging Data,"  Calcium imaging data promises to transform the field of neuroscience by
making it possible to record from large populations of neurons simultaneously.
However, determining the exact moment in time at which a neuron spikes, from a
calcium imaging data set, amounts to a non-trivial deconvolution problem which
is of critical importance for downstream analyses. While a number of
formulations have been proposed for this task in the recent literature, in this
paper we focus on a formulation recently proposed in Jewell and Witten (2017)
which has shown initial promising results. However, this proposal is slow to
run on fluorescence traces of hundreds of thousands of timesteps.
Here we develop a much faster online algorithm for solving the optimization
problem of Jewell and Witten (2017) that can be used to deconvolve a
fluorescence trace of 100,000 timesteps in less than a second. Furthermore,
this algorithm overcomes a technical challenge of Jewell and Witten (2017) by
avoiding the occurrence of so-called ""negative"" spikes. We demonstrate that
this algorithm has superior performance relative to existing methods for spike
deconvolution on calcium imaging datasets that were recently released as part
of the spikefinder challenge (this http URL).
Our C++ implementation, along with R and python wrappers, is publicly
available on Github at this https URL.
",0,0,0,1,1,0
1186,Learning to attend in a brain-inspired deep neural network,"  Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.
",0,0,0,0,1,0
2889,Efficient Pricing of Barrier Options on High Volatility Assets using Subset Simulation,"  Barrier options are one of the most widely traded exotic options on stock
exchanges. In this paper, we develop a new stochastic simulation method for
pricing barrier options and estimating the corresponding execution
probabilities. We show that the proposed method always outperforms the standard
Monte Carlo approach and becomes substantially more efficient when the
underlying asset has high volatility, while it performs better than multilevel
Monte Carlo for special cases of barrier options and underlying assets. These
theoretical findings are confirmed by numerous simulation results.
",0,0,0,1,0,1
210,Difference analogue of second main theorems for meromorphic mapping into algebraic variety,"  In this paper, we prove some difference analogue of second main theorems of
meromorphic mapping from Cm into an algebraic variety V intersecting a finite
set of fixed hypersurfaces in subgeneral position. As an application, we prove
a result on algebraically degenerate of holomorphic curves intersecting
hypersurfaces and difference analogue of Picard's theorem on holomorphic
curves. Furthermore, we obtain a second main theorem of meromorphic mappings
intersecting hypersurfaces in N-subgeneral position for Veronese embedding in
Pn(C) and a uniqueness theorem sharing hypersurfaces.
",0,0,1,0,0,0
1580,FRET-based nanocommunication with luciferase and channelrhodopsin molecules for in-body medical systems,"  The paper is concerned with an in-body system gathering data for medical
purposes. It is focused on communication between the following two components
of the system: liposomes gathering the data inside human veins and a detector
collecting the data from liposomes. Foerster Resonance Energy Transfer (FRET)
is considered as a mechanism for communication between the system components.
The usage of bioluminescent molecules as an energy source for generating FRET
signals is suggested and the performance evaluation of this approach is given.
FRET transmission may be initiated without an aid of an external laser, which
is crucial in case of communication taking place inside of human body. It is
also shown how to solve the problem of FRET signals recording. The usage of
channelrhodopsin molecules, able to receive FRET signals and convert them into
voltage, is proposed. The communication system is modelled with molecular
structures and spectral characteristics of the proposed molecules and further
validated by using Monte Carlo computer simulations, calculating the data
throughput and the bit error rate.
",0,0,0,0,1,0
629,"Contributed Discussion to Uncertainty Quantification for the Horseshoe by Stéphanie van der Pas, Botond Szabó and Aad van der Vaart","  We begin by introducing the main ideas of the paper under discussion. We
discuss some interesting issues regarding adaptive component-wise credible
intervals. We then briefly touch upon the concepts of self-similarity and
excessive bias restriction. This is then followed by some comments on the
extensive simulation study carried out in the paper.
",0,0,1,1,0,0
786,Learning Nonlinear Brain Dynamics: van der Pol Meets LSTM,"  Many real-world data sets, especially in biology, are produced by highly
multivariate and nonlinear complex dynamical systems. In this paper, we focus
on brain imaging data, including both calcium imaging and functional MRI data.
Standard vector-autoregressive models are limited by their linearity
assumptions, while nonlinear general-purpose, large-scale temporal models, such
as LSTM networks, typically require large amounts of training data, not always
readily available in biological applications; furthermore, such models have
limited interpretability. We introduce here a novel approach for learning a
nonlinear differential equation model aimed at capturing brain dynamics.
Specifically, we propose a variable-projection optimization approach to
estimate the parameters of the multivariate (coupled) van der Pol oscillator,
and demonstrate that such a model can accurately represent nonlinear dynamics
of the brain data. Furthermore, in order to improve the predictive accuracy
when forecasting future brain-activity time series, we use this analytical
model as an unlimited source of simulated data for pretraining LSTM; such
model-specific data augmentation approach consistently improves LSTM
performance on both calcium and fMRI imaging data.
",0,0,0,1,1,0
15285,Discrete Time Dynamic Programming with Recursive Preferences: Optimality and Applications,"  This paper provides an alternative approach to the theory of dynamic
programming, designed to accommodate the kinds of recursive preference
specifications that have become popular in economic and financial analysis,
while still supporting traditional additively separable rewards. The approach
exploits the theory of monotone convex operators, which turns out to be well
suited to dynamic maximization. The intuition is that convexity is preserved
under maximization, so convexity properties found in preferences extend
naturally to the Bellman operator.
",0,0,0,0,0,1
882,Mass Preconditioning for the Exact One-Flavor Action in Lattice QCD with Domain-Wall Fermion,"  The mass-preconditioning (MP) technique has become a standard tool to enhance
the efficiency of the hybrid Monte-Carlo simulation (HMC) of lattice QCD with
dynamical quarks, for 2-flavors QCD with degenerate quark masses, as well as
its extension to the case of one-flavor by taking the square-root of the
fermion determinant of 2-flavors with degenerate masses. However, for lattice
QCD with domain-wall fermion, the fermion determinant of any single fermion
flavor can be expressed as a functional integral with an exact pseudofermion
action $ \phi^\dagger H^{-1} \phi $, where $ H^{-1} $ is a positive-definite
Hermitian operator without taking square-root, and with the chiral structure
\cite{Chen:2014hyy}. Consequently, the mass-preconditioning for the exact
one-flavor action (EOFA) does not necessarily follow the conventional (old) MP
pattern. In this paper, we present a new mass-preconditioning for the EOFA,
which is more efficient than the old MP which we have used in Refs.
\cite{Chen:2014hyy,Chen:2014bbc}. We perform numerical tests in lattice QCD
with $ N_f = 1 $ and $ N_f = 1+1+1+1 $ optimal domain-wall quarks, with one
mass-preconditioner applied to one of the exact one-flavor actions, and we find
that the efficiency of the new MP is more than 20\% higher than that of the old
MP.
",0,1,0,0,0,0
385,The list chromatic number of graphs with small clique number,"  We prove that every triangle-free graph with maximum degree $\Delta$ has list
chromatic number at most $(1+o(1))\frac{\Delta}{\ln \Delta}$. This matches the
best-known bound for graphs of girth at least 5. We also provide a new proof
that for any $r\geq 4$ every $K_r$-free graph has list-chromatic number at most
$200r\frac{\Delta\ln\ln\Delta}{\ln\Delta}$.
",0,0,1,0,0,0
840,Stable representations of posets,"  The purpose of this paper is to study stable representations of partially
ordered sets (posets) and compare it to the well known theory for quivers. In
particular, we prove that every indecomposable representation of a poset of
finite type is stable with respect to some weight and construct that weight
explicitly in terms of the dimension vector. We show that if a poset is
primitive then Coxeter transformations preserve stable representations. When
the base field is the field of complex numbers we establish the connection
between the polystable representations and the unitary $\chi$-representations
of posets. This connection explains the similarity of the results obtained in
the series of papers.
",0,0,1,0,0,0
468,Spoken English Intelligibility Remediation with PocketSphinx Alignment and Feature Extraction Improves Substantially over the State of the Art,"  We use automatic speech recognition to assess spoken English learner
pronunciation based on the authentic intelligibility of the learners' spoken
responses determined from support vector machine (SVM) classifier or deep
learning neural network model predictions of transcription correctness. Using
numeric features produced by PocketSphinx alignment mode and many recognition
passes searching for the substitution and deletion of each expected phoneme and
insertion of unexpected phonemes in sequence, the SVM models achieve 82 percent
agreement with the accuracy of Amazon Mechanical Turk crowdworker
transcriptions, up from 75 percent reported by multiple independent
researchers. Using such features with SVM classifier probability prediction
models can help computer-aided pronunciation teaching (CAPT) systems provide
intelligibility remediation.
",1,0,0,1,0,0
548,Semi-Analytical Perturbative Approaches to Third Body Resonant Trajectories,"  In the framework of multi-body dynamics, successive encounters with a third
body, even if well outside of its sphere of influence, can noticeably alter the
trajectory of a spacecraft. Examples of these effects have already been
exploited by past missions such as SMART-1, as well as are proposed to benefit
future missions to Jupiter, Saturn or Neptune, and disposal strategies from
Earth's High Eccentric or Libration Point Orbits. This paper revises three
totally different descriptions of the effects of the third body gravitational
perturbation. These are the averaged dynamics of the classical third body
perturbing function, the Opik's close encounter theory and the Keplerian map
approach. The first two techniques have respectively been applied to the cases
of a spacecraft either always remaining very far or occasionally experiencing
extremely close approaches to the third body. However, the paper also seeks
solutions for trajectories that undergo one or more close approaches at
distances in the order of the sphere of influence of the third body. The paper
attempts to gain insight into the accuracy of these different perturbative
techniques into each of these scenarios, as compared with the motion in the
Circular Restricted Three Body Problem.
",0,1,0,0,0,0
886,Controlling Chiral Domain Walls in Antiferromagnets Using Spin-Wave Helicity,"  In antiferromagnets, the Dzyaloshinskii-Moriya interaction lifts the
degeneracy of left- and right-circularly polarized spin waves. This
relativistic coupling increases the efficiency of spin-wave-induced domain wall
motion and leads to higher drift velocities. We show that in biaxial
antiferromagnets, the spin-wave helicity controls both the direction and
magnitude of the magnonic force on chiral domain walls. By contrast, in
uniaxial antiferromagnets, the magnonic force is propulsive with a helicity
dependent strength.
",0,1,0,0,0,0
896,Performance of a small size telescope (SST-1M) camera for gamma-ray astronomy with the Cherenkov Telescope Array,"  The foreseen implementations of the Small Size Telescopes (SST) in CTA will
provide unique insights into the highest energy gamma rays offering fundamental
means to discover and under- stand the sources populating the Galaxy and our
local neighborhood. Aiming at such a goal, the SST-1M is one of the three
different implementations that are being prototyped and tested for CTA. SST-1M
is a Davies-Cotton single mirror telescope equipped with a unique camera
technology based on SiPMs with demonstrated advantages over classical
photomultipliers in terms of duty-cycle. In this contribution, we describe the
telescope components, the camera, and the trigger and readout system. The
results of the commissioning of the camera using a dedicated test setup are
then presented. The performances of the camera first prototype in terms of
expected trigger rates and trigger efficiencies for different night-sky
background conditions are presented, and the camera response is compared to
end-to-end simulations.
",0,1,0,0,0,0
533,Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer,"  MicroRNAs play important roles in many biological processes. Their aberrant
expression can have oncogenic or tumor suppressor function directly
participating to carcinogenesis, malignant transformation, invasiveness and
metastasis. Indeed, miRNA profiles can distinguish not only between normal and
cancerous tissue but they can also successfully classify different subtypes of
a particular cancer. Here, we focus on a particular class of transcripts
encoding polycistronic miRNA genes that yields multiple miRNA components. We
describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully
redesigned release of the MMRA computational pipeline (MiRNA Master Regulator
Analysis), developed to search for clustered miRNAs potentially driving cancer
molecular subtyping. Genomically clustered miRNAs are frequently co-expressed
to target different components of pro-tumorigenic signalling pathways. By
applying ClustMMRA to breast cancer patient data, we identified key miRNA
clusters driving the phenotype of different tumor subgroups. The pipeline was
applied to two independent breast cancer datasets, providing statistically
concordant results between the two analysis. We validated in cell lines the
miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative
subtype phenotype through its control of proliferation and EMT.
",0,0,0,0,1,0
912,On a binary system of Prendiville: The cubic case,"  We prove sharp decoupling inequalities for a class of two dimensional
non-degenerate surfaces in R^5, introduced by Prendiville. As a consequence, we
obtain sharp bounds on the number of integer solutions of the Diophantine
systems associated with these surfaces.
",0,0,1,0,0,0
6109,Online Estimation of Multiple Dynamic Graphs in Pattern Sequences,"  Many time-series data including text, movies, and biological signals can be
represented as sequences of correlated binary patterns. These patterns may be
described by weighted combinations of a few dominant structures that underpin
specific interactions among the binary elements. To extract the dominant
correlation structures and their contributions to generating data in a
time-dependent manner, we model the dynamics of binary patterns using the
state-space model of an Ising-type network that is composed of multiple
undirected graphs. We provide a sequential Bayes algorithm to estimate the
dynamics of weights on the graphs while gaining the graph structures online.
This model can uncover overlapping graphs underlying the data better than a
traditional orthogonal decomposition method, and outperforms an original
time-dependent full Ising model. We assess the performance of the method by
simulated data, and demonstrate that spontaneous activity of cultured
hippocampal neurons is represented by dynamics of multiple graphs.
",1,0,0,1,1,0
5952,Asynchronous stochastic price pump,"  We propose a model for equity trading in a population of agents where each
agent acts to achieve his or her target stock-to-bond ratio, and, as a feedback
mechanism, follows a market adaptive strategy. In this model only a fraction of
agents participates in buying and selling stock during a trading period, while
the rest of the group accepts the newly set price. Using numerical simulations
we show that the stochastic process settles on a stationary regime for the
returns. The mean return can be greater or less than the return on the bond and
it is determined by the parameters of the adaptive mechanism. When the number
of interacting agents is fixed, the distribution of the returns follows the
log-normal density. In this case, we give an analytic formula for the mean rate
of return in terms of the rate of change of agents' risk levels and confirm the
formula by numerical simulations. However, when the number of interacting
agents per period is random, the distribution of returns can significantly
deviate from the log-normal, especially as the variance of the distribution for
the number of interacting agents increases.
",0,0,0,0,0,1
4728,Optimal proportional reinsurance and investment for stochastic factor models,"  In this work we investigate the optimal proportional reinsurance-investment
strategy of an insurance company which wishes to maximize the expected
exponential utility of its terminal wealth in a finite time horizon. Our goal
is to extend the classical Cramer-Lundberg model introducing a stochastic
factor which affects the intensity of the claims arrival process, described by
a Cox process, as well as the insurance and reinsurance premia. Using the
classical stochastic control approach based on the Hamilton-Jacobi-Bellman
equation we characterize the optimal strategy and provide a verification result
for the value function via classical solutions of two backward partial
differential equations. Existence and uniqueness of these solutions are
discussed. Results under various premium calculation principles are illustrated
and a new premium calculation rule is proposed in order to get more realistic
strategies and to better fit our stochastic factor model. Finally, numerical
simulations are performed to obtain sensitivity analyses.
",0,0,0,0,0,1
643,One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network,"  There is an inherent need for autonomous cars, drones, and other robots to
have a notion of how their environment behaves and to anticipate changes in the
near future. In this work, we focus on anticipating future appearance given the
current frame of a video. Existing work focuses on either predicting the future
appearance as the next frame of a video, or predicting future motion as optical
flow or motion trajectories starting from a single video frame. This work
stretches the ability of CNNs (Convolutional Neural Networks) to predict an
anticipation of appearance at an arbitrarily given future time, not necessarily
the next video frame. We condition our predicted future appearance on a
continuous time variable that allows us to anticipate future frames at a given
temporal distance, directly from the input video frame. We show that CNNs can
learn an intrinsic representation of typical appearance changes over time and
successfully generate realistic predictions at a deliberate time difference in
the near future.
",1,0,0,0,0,0
15603,Multivariate stable distributions and their applications for modelling cryptocurrency-returns,"  In this paper we extend the known methodology for fitting stable
distributions to the multivariate case and apply the suggested method to the
modelling of daily cryptocurrency-return data. The investigated time period is
cut into 10 non-overlapping sections, thus the changes can also be observed. We
apply bootstrap tests for checking the models and compare our approach to the
more traditional extreme-value and copula models.
",0,0,0,0,0,1
754,Generative Adversarial Networks for Electronic Health Records: A Framework for Exploring and Evaluating Methods for Predicting Drug-Induced Laboratory Test Trajectories,"  Generative Adversarial Networks (GANs) represent a promising class of
generative networks that combine neural networks with game theory. From
generating realistic images and videos to assisting musical creation, GANs are
transforming many fields of arts and sciences. However, their application to
healthcare has not been fully realized, more specifically in generating
electronic health records (EHR) data. In this paper, we propose a framework for
exploring the value of GANs in the context of continuous laboratory time series
data. We devise an unsupervised evaluation method that measures the predictive
power of synthetic laboratory test time series. Further, we show that when it
comes to predicting the impact of drug exposure on laboratory test data,
incorporating representation learning of the training cohorts prior to training
GAN models is beneficial.
",1,0,0,1,0,0
891,On the Status of the Measurement Problem: Recalling the Relativistic Transactional Interpretation,"  In view of a resurgence of concern about the measurement problem, it is
pointed out that the Relativistic Transactional Interpretation (RTI) remedies
issues previously considered as drawbacks or refutations of the original TI.
Specifically, once one takes into account relativistic processes that are not
representable at the non-relativistic level (such as particle creation and
annihilation, and virtual propagation), absorption is quantitatively defined in
unambiguous physical terms. In addition, specifics of the relativistic
transactional model demonstrate that the Maudlin `contingent absorber'
challenge to the original TI cannot even be mounted: basic features of
established relativistic field theories (in particular, the asymmetry between
field sources and the bosonic fields, and the fact that slow-moving bound
states, such as atoms, are not offer waves) dictate that the `slow-moving offer
wave' required for the challenge scenario cannot exist. It is concluded that
issues previously considered obstacles for TI are no longer legitimately viewed
as such, and that reconsideration of the transactional picture is warranted in
connection with solving the measurement problem.
",0,1,0,0,0,0
6202,Matrix Completion and Performance Guarantees for Single Individual Haplotyping,"  Single individual haplotyping is an NP-hard problem that emerges when
attempting to reconstruct an organism's inherited genetic variations using data
typically generated by high-throughput DNA sequencing platforms. Genomes of
diploid organisms, including humans, are organized into homologous pairs of
chromosomes that differ from each other in a relatively small number of variant
positions. Haplotypes are ordered sequences of the nucleotides in the variant
positions of the chromosomes in a homologous pair; for diploids, haplotypes
associated with a pair of chromosomes may be conveniently represented by means
of complementary binary sequences. In this paper, we consider a binary matrix
factorization formulation of the single individual haplotyping problem and
efficiently solve it by means of alternating minimization. We analyze the
convergence properties of the alternating minimization algorithm and establish
theoretical bounds for the achievable haplotype reconstruction error. The
proposed technique is shown to outperform existing methods when applied to
synthetic as well as real-world Fosmid-based HapMap NA12878 datasets.
",0,0,0,1,1,0
7932,Dynamics of observables in rank-based models and performance of functionally generated portfolios,"  In the seminal work [9], several macroscopic market observables have been
introduced, in an attempt to find characteristics capturing the diversity of a
financial market. Despite the crucial importance of such observables for
investment decisions, a concise mathematical description of their dynamics has
been missing. We fill this gap in the setting of rank-based models and expect
our ideas to extend to other models of large financial markets as well. The
results are then used to study the performance of multiplicatively and
additively functionally generated portfolios, in particular, over short-term
and medium-term horizons.
",0,0,0,0,0,1
4250,Improving Stock Movement Prediction with Adversarial Training,"  This paper contributes a new machine learning solution for stock movement
prediction, which aims to predict whether the price of a stock will be up or
down in the near future. The key novelty is that we propose to employ
adversarial training to improve the generalization of a recurrent neural
network model. The rationality of adversarial training here is that the input
features to stock prediction are typically based on stock price, which is
essentially a stochastic variable and continuously changed with time by nature.
As such, normal training with stationary price-based features (e.g. the closing
price) can easily overfit the data, being insufficient to obtain reliable
models. To address this problem, we propose to add perturbations to simulate
the stochasticity of continuous price variable, and train the model to work
well under small yet intentional perturbations. Extensive experiments on two
real-world stock data show that our method outperforms the state-of-the-art
solution with 3.11% relative improvements on average w.r.t. accuracy, verifying
the usefulness of adversarial training for stock prediction task. Codes will be
made available upon acceptance.
",0,0,0,0,0,1
1047,Temporal processing and context dependency in C. elegans mechanosensation,"  A quantitative understanding of how sensory signals are transformed into
motor outputs places useful constraints on brain function and helps reveal the
brain's underlying computations. We investigate how the nematode C. elegans
responds to time-varying mechanosensory signals using a high-throughput
optogenetic assay and automated behavior quantification. In the prevailing
picture of the touch circuit, the animal's behavior is determined by which
neurons are stimulated and by the stimulus amplitude. In contrast, we find that
the behavioral response is tuned to temporal properties of mechanosensory
signals, like its integral and derivative, that extend over many seconds.
Mechanosensory signals, even in the same neurons, can be tailored to elicit
different behavioral responses. Moreover, we find that the animal's response
also depends on its behavioral context. Most dramatically, the animal ignores
all tested mechanosensory stimuli during turns. Finally, we present a
linear-nonlinear model that predicts the animal's behavioral response to
stimulus.
",0,0,0,0,1,0
4182,On the variance of internode distance under the multispecies coalescent,"  We consider the problem of estimating species trees from unrooted gene tree
topologies in the presence of incomplete lineage sorting, a common phenomenon
that creates gene tree heterogeneity in multilocus datasets. One popular class
of reconstruction methods in this setting is based on internode distances, i.e.
the average graph distance between pairs of species across gene trees. While
statistical consistency in the limit of large numbers of loci has been
established in some cases, little is known about the sample complexity of such
methods. Here we make progress on this question by deriving a lower bound on
the worst-case variance of internode distance which depends linearly on the
corresponding graph distance in the species tree. We also discuss some
algorithmic implications.
",0,0,0,0,1,0
714,Construction of constant mean curvature n-noids using the DPW method,"  We construct constant mean curvature surfaces in euclidean space with genus
zero and n ends asymptotic to Delaunay surfaces using the DPW method.
",0,0,1,0,0,0
527,Experimental data over quantum mechanics simulations for inferring the repulsive exponent of the Lennard-Jones potential in Molecular Dynamics,"  The Lennard-Jones (LJ) potential is a cornerstone of Molecular Dynamics (MD)
simulations and among the most widely used computational kernels in science.
The potential models atomistic attraction and repulsion with century old
prescribed parameters ($q=6, \; p=12$, respectively), originally related by a
factor of two for simplicity of calculations. We re-examine the value of the
repulsion exponent through data driven uncertainty quantification. We perform
Hierarchical Bayesian inference on MD simulations of argon using experimental
data of the radial distribution function (RDF) for a range of thermodynamic
conditions, as well as dimer interaction energies from quantum mechanics
simulations. The experimental data suggest a repulsion exponent ($p \approx
6.5$), in contrast to the quantum simulations data that support values closer
to the original ($p=12$) exponent. Most notably, we find that predictions of
RDF, diffusion coefficient and density of argon are more accurate and robust in
producing the correct argon phase around its triple point, when using the
values inferred from experimental data over those from quantum mechanics
simulations. The present results suggest the need for data driven recalibration
of the LJ potential across MD simulations.
",0,1,0,1,0,0
665,Multi-dimensional Graph Fourier Transform,"  Many signals on Cartesian product graphs appear in the real world, such as
digital images, sensor observation time series, and movie ratings on Netflix.
These signals are ""multi-dimensional"" and have directional characteristics
along each factor graph. However, the existing graph Fourier transform does not
distinguish these directions, and assigns 1-D spectra to signals on product
graphs. Further, these spectra are often multi-valued at some frequencies. Our
main result is a multi-dimensional graph Fourier transform that solves such
problems associated with the conventional GFT. Using algebraic properties of
Cartesian products, the proposed transform rearranges 1-D spectra obtained by
the conventional GFT into the multi-dimensional frequency domain, of which each
dimension represents a directional frequency along each factor graph. Thus, the
multi-dimensional graph Fourier transform enables directional frequency
analysis, in addition to frequency analysis with the conventional GFT.
Moreover, this rearrangement resolves the multi-valuedness of spectra in some
cases. The multi-dimensional graph Fourier transform is a foundation of novel
filterings and stationarities that utilize dimensional information of graph
signals, which are also discussed in this study. The proposed methods are
applicable to a wide variety of data that can be regarded as signals on
Cartesian product graphs. This study also notes that multivariate graph signals
can be regarded as 2-D univariate graph signals. This correspondence provides
natural definitions of the multivariate graph Fourier transform and the
multivariate stationarity based on their 2-D univariate versions.
",1,0,0,1,0,0
406,Cayley properties of the line graphs induced by of consecutive layers of the hypercube,"  Let $n >3$ and $ 0< k < \frac{n}{2} $ be integers. In this paper, we
investigate some algebraic properties of the line graph of the graph $
{Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$
which is induced by the set of vertices of weights $k$ and $k+1$. In the first
step, we determine the automorphism groups of these graphs for all values of
$k$. In the second step, we study Cayley properties of the line graph of these
graphs. In particular, we show that for $ k>2, $ if $ 2k+1 \neq n$, then the
line graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley
graph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a
Cayley graph if and only if $ n$ is a power of a prime $p$.
",0,0,1,0,0,0
240,Preliminary corrosion studies of IN-RAFM steel with stagnant Lead Lithium at 550 C,"  Corrosion of Indian RAFMS (reduced activation ferritic martensitic steel)
material with liquid metal, Lead Lithium ( Pb-Li) has been studied under static
condition, maintaining Pb-Li at 550 C for different time durations, 2500, 5000
and 9000 hours. Corrosion rate was calculated from weight loss measurements.
Microstructure analysis was carried out using SEM and chemical composition by
SEM-EDX measurements. Micro Vickers hardness and tensile testing were also
carried out. Chromium was found leaching from the near surface regions and
surface hardness was found to decrease in all the three cases. Grain boundaries
were affected. Some grains got detached from the surface giving rise to pebble
like structures in the surface micrographs. There was no significant reduction
in the tensile strength, after exposure to liquid metal. This paper discusses
the experimental details and the results obtained.
",0,1,0,0,0,0
207,Pointed $p^2q$-dimensional Hopf algebras in positive characteristic,"  Let $\K$ be an algebraically closed field of positive characteristic $p$. We
mainly classify pointed Hopf algebras over $\K$ of dimension $p^2q$, $pq^2$ and
$pqr$ where $p,q,r$ are distinct prime numbers. We obtain a complete
classification of such Hopf algebras except two subcases when they are not
generated by the first terms of coradical filtration. In particular, we obtain
many new examples of non-commutative and non-cocommutative finite-dimensional
Hopf algebras.
",0,0,1,0,0,0
277,Vocabulary-informed Extreme Value Learning,"  The novel unseen classes can be formulated as the extreme values of known
classes. This inspired the recent works on open-set recognition
\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no
way of naming the novel unseen classes. To solve this problem, we propose the
Extreme Value Learning (EVL) formulation to learn the mapping from visual
feature to semantic space. To model the margin and coverage distributions of
each class, the Vocabulary-informed Learning (ViL) is adopted by using vast
open vocabulary in the semantic space. Essentially, by incorporating the EVL
and ViL, we for the first time propose a novel semantic embedding paradigm --
Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual
features into semantic space in a probabilistic way. The learned embedding can
be directly used to solve supervised learning, zero-shot and open set
recognition simultaneously. Experiments on two benchmark datasets demonstrate
the effectiveness of proposed frameworks.
",1,0,1,1,0,0
815,Geometric Insights into Support Vector Machine Behavior using the KKT Conditions,"  The support vector machine (SVM) is a powerful and widely used classification
algorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide
rigorous mathematical proof for new insights into the behavior of SVM. These
insights provide perhaps unexpected relationships between SVM and two other
linear classifiers: the mean difference and the maximal data piling direction.
For example, we show that in many cases SVM can be viewed as a cropped version
of these classifiers. By carefully exploring these connections we show how SVM
tuning behavior is affected by characteristics including: balanced vs.
unbalanced classes, low vs. high dimension, separable vs. non-separable data.
These results provide further insights into tuning SVM via cross-validation by
explaining observed pathological behavior and motivating improved
cross-validation methodology. Finally, we also provide new results on the
geometry of complete data piling directions in high dimensional space.
",0,0,0,1,0,0
612,Technical Report for Real-Time Certified Probabilistic Pedestrian Forecasting,"  The success of autonomous systems will depend upon their ability to safely
navigate human-centric environments. This motivates the need for a real-time,
probabilistic forecasting algorithm for pedestrians, cyclists, and other agents
since these predictions will form a necessary step in assessing the risk of any
action. This paper presents a novel approach to probabilistic forecasting for
pedestrians based on weighted sums of ordinary differential equations that are
learned from historical trajectory information within a fixed scene. The
resulting algorithm is embarrassingly parallel and is able to work at real-time
speeds using a naive Python implementation. The quality of predicted locations
of agents generated by the proposed algorithm is validated on a variety of
examples and considerably higher than existing state of the art approaches over
long time horizons.
",1,0,1,0,0,0
453,EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras (Extended Abstract),"  Marker-based and marker-less optical skeletal motion-capture methods use an
outside-in arrangement of cameras placed around a scene, with viewpoints
converging on the center. They often create discomfort by possibly needed
marker suits, and their recording volume is severely restricted and often
constrained to indoor scenes with controlled backgrounds. We therefore propose
a new method for real-time, marker-less and egocentric motion capture which
estimates the full-body skeleton pose from a lightweight stereo pair of fisheye
cameras that are attached to a helmet or virtual-reality headset. It combines
the strength of a new generative pose estimation framework for fisheye views
with a ConvNet-based body-part detector trained on a new automatically
annotated and augmented dataset. Our inside-in method captures full-body motion
in general indoor and outdoor scenes, and also crowded scenes.
",1,0,0,0,0,0
14561,"Token Economics in Energy Systems: Concept, Functionality and Applications","  Traditional centralized energy systems have the disadvantages of difficult
management and insufficient incentives. Blockchain is an emerging technology,
which can be utilized in energy systems to enhance their management and
control. Integrating token economy and blockchain technology, token economic
systems in energy possess the characteristics of strong incentives and low
cost, facilitating integrating renewable energy and demand side management, and
providing guarantees for improving energy efficiency and reducing emission.
This article describes the concept and functionality of token economics, and
then analyzes the feasibility of applying token economics in the energy
systems, and finally discuss the applications of token economics with an
example in integrated energy systems.
",0,0,0,0,0,1
4600,Online classification of imagined speech using functional near-infrared spectroscopy signals,"  Most brain-computer interfaces (BCIs) based on functional near-infrared
spectroscopy (fNIRS) require that users perform mental tasks such as motor
imagery, mental arithmetic, or music imagery to convey a message or to answer
simple yes or no questions. These cognitive tasks usually have no direct
association with the communicative intent, which makes them difficult for users
to perform. In this paper, a 3-class intuitive BCI is presented which enables
users to directly answer yes or no questions by covertly rehearsing the word
'yes' or 'no' for 15 s. The BCI also admits an equivalent duration of
unconstrained rest which constitutes the third discernable task. Twelve
participants each completed one offline block and six online blocks over the
course of 2 sessions. The mean value of the change in oxygenated hemoglobin
concentration during a trial was calculated for each channel and used to train
a regularized linear discriminant analysis (RLDA) classifier. By the final
online block, 9 out of 12 participants were performing above chance (p<0.001),
with a 3-class accuracy of 83.8+9.4%. Even when considering all participants,
the average online 3-class accuracy over the last 3 blocks was 64.1+20.6%, with
only 3 participants scoring below chance (p<0.001). For most participants,
channels in the left temporal and temporoparietal cortex provided the most
discriminative information. To our knowledge, this is the first report of an
online fNIRS 3-class imagined speech BCI. Our findings suggest that imagined
speech can be used as a reliable activation task for selected users for the
development of more intuitive BCIs for communication.
",0,0,0,0,1,0
3223,A Novel Model of Cancer-Induced Peripheral Neuropathy and the Role of TRPA1 in Pain Transduction,"  Background. Models of cancer-induced neuropathy are designed by injecting
cancer cells near the peripheral nerves. The interference of tissue-resident
immune cells does not allow a direct contact with nerve fibres which affects
the tumor microenvironment and the invasion process. Methods. Anaplastic
tumor-1 (AT-1) cells were inoculated within the sciatic nerves (SNs) of male
Copenhagen rats. Lumbar dorsal root ganglia (DRGs) and the SNs were collected
on days 3, 7, 14, and 21. SN tissues were examined for morphological changes
and DRG tissues for immunofluorescence, electrophoretic tendency, and mRNA
quantification. Hypersensitivities to cold, mechanical, and thermal stimuli
were determined. HC-030031, a selective TRPA1 antagonist, was used to treat
cold allodynia. Results. Nociception thresholds were identified on day 6.
Immunofluorescent micrographs showed overexpression of TRPA1 on days 7 and 14
and of CGRP on day 14 until day 21. Both TRPA1 and CGRP were coexpressed on the
same cells. Immunoblots exhibited an increase in TRPA1 expression on day 14.
TRPA1 mRNA underwent an increase on day 7 (normalized to 18S). Injection of
HC-030031 transiently reversed the cold allodynia. Conclusion. A novel and a
promising model of cancer-induced neuropathy was established, and the role of
TRPA1 and CGRP in pain transduction was examined.
",0,0,0,0,1,0
558,"Approximate Program Smoothing Using Mean-Variance Statistics, with Application to Procedural Shader Bandlimiting","  This paper introduces a general method to approximate the convolution of an
arbitrary program with a Gaussian kernel. This process has the effect of
smoothing out a program. Our compiler framework models intermediate values in
the program as random variables, by using mean and variance statistics. Our
approach breaks the input program into parts and relates the statistics of the
different parts, under the smoothing process. We give several approximations
that can be used for the different parts of the program. These include the
approximation of Dorn et al., a novel adaptive Gaussian approximation, Monte
Carlo sampling, and compactly supported kernels. Our adaptive Gaussian
approximation is accurate up to the second order in the standard deviation of
the smoothing kernel, and mathematically smooth. We show how to construct a
compiler that applies chosen approximations to given parts of the input
program. Because each expression can have multiple approximation choices, we
use a genetic search to automatically select the best approximations. We apply
this framework to the problem of automatically bandlimiting procedural shader
programs. We evaluate our method on a variety of complex shaders, including
shaders with parallax mapping, animation, and spatially varying statistics. The
resulting smoothed shader programs outperform previous approaches both
numerically, and aesthetically, due to the smoothing properties of our
approximations.
",1,0,0,0,0,0
11222,Will a Large Economy Be Stable?,"  We study networks of firms with Leontief production functions. Relying on
results from Random Matrix Theory, we argue that such networks generically
become unstable when their size increases, or when the heterogeneity in
productivities/connectivities becomes too strong. At marginal stability and for
large heterogeneities, we find that the distribution of firm sizes develops a
power-law tail, as observed empirically. Crises can be triggered by small
idiosyncratic shocks, which lead to ""avalanches"" of defaults characterized by a
power-law distribution of total output losses. We conjecture that evolutionary
and behavioural forces conspire to keep the economy close to marginal
stability. This scenario would naturally explain the well-known ""small shocks,
large business cycles"" puzzle, as anticipated long ago by Bak, Chen, Scheinkman
and Woodford.
",0,0,0,0,0,1
376,The Effect of Site-Specific Spectral Densities on the High-Dimensional Exciton-Vibrational Dynamics in the FMO Complex,"  The coupled exciton-vibrational dynamics of a three-site model of the FMO
complex is investigated using the Multi-layer Multi-configuration
Time-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of
the spectral density on the exciton state populations as well as on the
vibrational and vibronic non-equilibrium excitations. Models which use either a
single or site-specific spectral densities are contrasted to a spectral density
adapted from experiment. For the transfer efficiency, the total integrated
Huang-Rhys factor is found to be more important than details of the spectral
distributions. However, the latter are relevant for the obtained
non-equilibrium vibrational and vibronic distributions and thus influence the
actual pattern of population relaxation.
",0,1,0,0,0,0
443,Adelic point groups of elliptic curves,"  We show that for an elliptic curve E defined over a number field K, the group
E(A) of points of E over the adele ring A of K is a topological group that can
be analyzed in terms of the Galois representation associated to the torsion
points of E. An explicit description of E(A) is given, and we prove that for K
of degree n, almost all elliptic curves over K have an adelic point group
topologically isomorphic to a universal group depending on n. We also show that
there exist infinitely many elliptic curves over K having a different adelic
point group.
",0,0,1,0,0,0
1061,Density estimation on small datasets,"  How might a smooth probability distribution be estimated, with accurately
quantified uncertainty, from a limited amount of sampled data? Here we describe
a field-theoretic approach that addresses this problem remarkably well in one
dimension, providing an exact nonparametric Bayesian posterior without relying
on tunable parameters or large-data approximations. Strong non-Gaussian
constraints, which require a non-perturbative treatment, are found to play a
major role in reducing distribution uncertainty. A software implementation of
this method is provided.
",1,0,0,0,1,0
16238,Critical factors and enablers of food quality and safety compliance risk management in the Vietnamese seafood supply chain,"  Recently, along with the emergence of food scandals, food supply chains have
to face with ever-increasing pressure from compliance with food quality and
safety regulations and standards. This paper aims to explore critical factors
of compliance risk in food supply chain with an illustrated case in Vietnamese
seafood industry. To this end, this study takes advantage of both primary and
secondary data sources through a comprehensive literature research of
industrial and scientific papers, combined with expert interview. Findings
showed that there are three main critical factor groups influencing on
compliance risk including challenges originating from Vietnamese food supply
chain itself, characteristics of regulation and standards, and business
environment. Furthermore, author proposed enablers to eliminate compliance
risks to food supply chain managers as well as recommendations to government
and other influencers and supporters.
",0,0,0,0,0,1
538,A giant planet undergoing extreme ultraviolet irradiation by its hot massive-star host,"  The amount of ultraviolet irradiation and ablation experienced by a planet
depends strongly on the temperature of its host star. Of the thousands of
extra-solar planets now known, only four giant planets have been found that
transit hot, A-type stars (temperatures of 7300-10,000K), and none are known to
transit even hotter B-type stars. WASP-33 is an A-type star with a temperature
of ~7430K, which hosts the hottest known transiting planet; the planet is
itself as hot as a red dwarf star of type M. The planet displays a large heat
differential between its day-side and night-side, and is highly inflated,
traits that have been linked to high insolation. However, even at the
temperature of WASP-33b's day-side, its atmosphere likely resembles the
molecule-dominated atmospheres of other planets, and at the level of
ultraviolet irradiation it experiences, its atmosphere is unlikely to be
significantly ablated over the lifetime of its star. Here we report
observations of the bright star HD 195689, which reveal a close-in (orbital
period ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star
is at the dividing line between stars of type A and B, and we measure the
KELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar
type K4. The molecules in K stars are entirely dissociated, and thus the
primary sources of opacity in the day-side atmosphere of KELT-9b are likely
atomic metals. Furthermore, KELT-9b receives ~700 times more extreme
ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,
leading to a predicted range of mass-loss rates that could leave the planet
largely stripped of its envelope during the main-sequence lifetime of the host
star.
",0,1,0,0,0,0
589,Adaptive Path-Integral Autoencoder: Representation Learning and Planning for Dynamical Systems,"  We present a representation learning algorithm that learns a low-dimensional
latent dynamical system from high-dimensional \textit{sequential} raw data,
e.g., video. The framework builds upon recent advances in amortized inference
methods that use both an inference network and a refinement procedure to output
samples from a variational distribution given an observation sequence, and
takes advantage of the duality between control and inference to approximately
solve the intractable inference problem using the path integral control
approach. The learned dynamical model can be used to predict and plan the
future states; we also present the efficient planning method that exploits the
learned low-dimensional latent dynamics. Numerical experiments show that the
proposed path-integral control based variational inference method leads to
tighter lower bounds in statistical model learning of sequential data. The
supplementary video: this https URL
",1,0,0,1,0,0
885,Weighted Low Rank Approximation for Background Estimation Problems,"  Classical principal component analysis (PCA) is not robust to the presence of
sparse outliers in the data. The use of the $\ell_1$ norm in the Robust PCA
(RPCA) method successfully eliminates the weakness of PCA in separating the
sparse outliers. In this paper, by sticking a simple weight to the Frobenius
norm, we propose a weighted low rank (WLR) method to avoid the often
computationally expensive algorithms relying on the $\ell_1$ norm. As a proof
of concept, a background estimation model has been presented and compared with
two $\ell_1$ norm minimization algorithms. We illustrate that as long as a
simple weight matrix is inferred from the data, one can use the weighted
Frobenius norm and achieve the same or better performance.
",0,0,1,0,0,0
16795,Mean-Field Games with Differing Beliefs for Algorithmic Trading,"  Even when confronted with the same data, agents often disagree on a model of
the real-world. Here, we address the question of how interacting heterogenous
agents, who disagree on what model the real-world follows, optimize their
trading actions. The market has latent factors that drive prices, and agents
account for the permanent impact they have on prices. This leads to a large
stochastic game, where each agents' performance criteria is computed under a
different probability measure. We analyse the mean-field game (MFG) limit of
the stochastic game and show that the Nash equilibria is given by the solution
to a non-standard vector-valued forward-backward stochastic differential
equation. Under some mild assumptions, we construct the solution in terms of
expectations of the filtered states. We prove the MFG strategy forms an
\epsilon-Nash equilibrium for the finite player game. Lastly, we present a
least-squares Monte Carlo based algorithm for computing the optimal control and
illustrate the results through simulation in market where agents disagree on
the model.
",0,0,0,0,0,1
1001,Statistical Latent Space Approach for Mixed Data Modelling and Applications,"  The analysis of mixed data has been raising challenges in statistics and
machine learning. One of two most prominent challenges is to develop new
statistical techniques and methodologies to effectively handle mixed data by
making the data less heterogeneous with minimum loss of information. The other
challenge is that such methods must be able to apply in large-scale tasks when
dealing with huge amount of mixed data. To tackle these challenges, we
introduce parameter sharing and balancing extensions to our recent model, the
mixed-variate restricted Boltzmann machine (MV.RBM) which can transform
heterogeneous data into homogeneous representation. We also integrate
structured sparsity and distance metric learning into RBM-based models. Our
proposed methods are applied in various applications including latent patient
profile modelling in medical data analysis and representation learning for
image retrieval. The experimental results demonstrate the models perform better
than baseline methods in medical data and outperform state-of-the-art rivals in
image dataset.
",1,0,0,1,0,0
384,PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target Relationship Analyses,"  Visualizing a complex network is computationally intensive process and
depends heavily on the number of components in the network. One way to solve
this problem is not to render the network in real time. PRE-render Content
Using Tiles (PRECUT) is a process to convert any complex network into a
pre-rendered network. Tiles are generated from pre-rendered images at different
zoom levels, and navigating the network simply becomes delivering relevant
tiles. PRECUT is exemplified by performing large-scale compound-target
relationship analyses. Matched molecular pair (MMP) networks were created using
compounds and the target class description found in the ChEMBL database. To
visualize MMP networks, the MMP network viewer has been implemented in COMBINE
and as a web application, hosted at this http URL.
",1,0,0,0,0,0
677,The effect of prior probabilities on quantification and propagation of imprecise probabilities resulting from small datasets,"  This paper outlines a methodology for Bayesian multimodel uncertainty
quantification (UQ) and propagation and presents an investigation into the
effect of prior probabilities on the resulting uncertainties. The UQ
methodology is adapted from the information-theoretic method previously
presented by the authors (Zhang and Shields, 2018) to a fully Bayesian
construction that enables greater flexibility in quantifying uncertainty in
probability model form. Being Bayesian in nature and rooted in UQ from small
datasets, prior probabilities in both probability model form and model
parameters are shown to have a significant impact on quantified uncertainties
and, consequently, on the uncertainties propagated through a physics-based
model. These effects are specifically investigated for a simplified plate
buckling problem with uncertainties in material properties derived from a small
number of experiments using noninformative priors and priors derived from past
studies of varying appropriateness. It is illustrated that prior probabilities
can have a significant impact on multimodel UQ for small datasets and
inappropriate (but seemingly reasonable) priors may even have lingering effects
that bias probabilities even for large datasets. When applied to uncertainty
propagation, this may result in probability bounds on response quantities that
do not include the true probabilities.
",0,0,0,1,0,0
9031,Infinitesimal perturbation analysis for risk measures based on the Smith max-stable random field,"  When using risk or dependence measures based on a given underlying model, it
is essential to be able to quantify the sensitivity or robustness of these
measures with respect to the model parameters. In this paper, we consider an
underlying model which is very popular in spatial extremes, the Smith
max-stable random field. We study the sensitivity properties of risk or
dependence measures based on the values of this field at a finite number of
locations. Max-stable fields play a key role, e.g., in the modelling of natural
disasters. As their multivariate density is generally not available for more
than three locations, the Likelihood Ratio Method cannot be used to estimate
the derivatives of the risk measures with respect to the model parameters.
Thus, we focus on a pathwise method, the Infinitesimal Perturbation Analysis
(IPA). We provide a convenient and tractable sufficient condition for
performing IPA, which is intricate to obtain because of the very structure of
max-stable fields involving pointwise maxima over an infinite number of random
functions. IPA enables the consistent estimation of the considered measures'
derivatives with respect to the parameters characterizing the spatial
dependence. We carry out a simulation study which shows that the approach
performs well in various configurations.
",0,0,0,0,0,1
335,Tropical recurrent sequences,"  Tropical recurrent sequences are introduced satisfying a given vector (being
a tropical counterpart of classical linear recurrent sequences). We consider
the case when Newton polygon of the vector has a single (bounded) edge. In this
case there are periodic tropical recurrent sequences which are similar to
classical linear recurrent sequences. A question is studied when there exists a
non-periodic tropical recurrent sequence satisfying a given vector, and partial
answers are provided to this question. Also an algorithm is designed which
tests existence of non-periodic tropical recurrent sequences satisfying a given
vector with integer coordinates. Finally, we introduce a tropical entropy of a
vector and provide some bounds on it.
",1,0,0,0,0,0
250,"Improvement in the UAV position estimation with low-cost GPS, INS and vision-based system: Application to a quadrotor UAV","  In this paper, we develop a position estimation system for Unmanned Aerial
Vehicles formed by hardware and software. It is based on low-cost devices: GPS,
commercial autopilot sensors and dense optical flow algorithm implemented in an
onboard microcomputer. Comparative tests were conducted using our approach and
the conventional one, where only fusion of GPS and inertial sensors are used.
Experiments were conducted using a quadrotor in two flying modes: hovering and
trajectory tracking in outdoor environments. Results demonstrate the
effectiveness of the proposed approach in comparison with the conventional
approaches presented in the vast majority of commercial drones.
",1,0,0,0,0,0
5167,Trends in the Diffusion of Misinformation on Social Media,"  We measure trends in the diffusion of misinformation on Facebook and Twitter
between January 2015 and July 2018. We focus on stories from 570 sites that
have been identified as producers of false stories. Interactions with these
sites on both Facebook and Twitter rose steadily through the end of 2016.
Interactions then fell sharply on Facebook while they continued to rise on
Twitter, with the ratio of Facebook engagements to Twitter shares falling by
approximately 60 percent. We see no similar pattern for other news, business,
or culture sites, where interactions have been relatively stable over time and
have followed similar trends on the two platforms both before and after the
election.
",1,0,0,0,0,1
536,Motion Planning for a Humanoid Mobile Manipulator System,"  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is
presented in this paper where the motion planning to realize ""human-like""
autonomous navigation and manipulation tasks is studied. Firstly, an improved
MaxiMin NSGA-II algorithm, which optimizes five objective functions to solve
the problems of singularity, redundancy, and coupling between mobile base and
manipulator simultaneously, is proposed to design the optimal pose to
manipulate the target object. Then, in order to link the initial pose and that
optimal pose, an off-line motion planning algorithm is designed. In detail, an
efficient direct-connect bidirectional RRT and gradient descent algorithm is
proposed to reduce the sampled nodes largely, and a geometric optimization
method is proposed for path pruning. Besides, head forward behaviors are
realized by calculating the reasonable orientations and assigning them to the
mobile base to improve the quality of human-robot interaction. Thirdly, the
extension to on-line planning is done by introducing real-time sensing,
collision-test and control cycles to update robotic motion in dynamic
environments. Fourthly, an EEs' via-point-based multi-objective genetic
algorithm is proposed to design the ""human-like"" via-poses by optimizing four
objective functions. Finally, numerous simulations are presented to validate
the effectiveness of proposed algorithms.
",1,0,0,0,0,0
440,"On variation of dynamical canonical heights, and Intersection numbers","  We study families of varieties endowed with polarized canonical eigensystems
of several maps, inducing canonical heights on the dominating variety as well
as on the ""good"" fibers of the family. We show explicitely the dependence on
the parameter for global and local canonical heights defined by Kawaguchi when
the fibers change, extending previous works of J. Silverman and others.
Finally, fixing an absolute value $v \in K$ and a variety $V/K$, we descript
the Kawaguchi`s canonical local height $\hat{\lambda}_{V,E,\mathcal{Q},}(.,v)$
as an intersection number, provided that the polarized system $(V,\mathcal{Q})$
has a certain weak Néron model over Spec$(\mathcal{O}_v)$ to be defined and
under some conditions depending on the special fiber. With this we extend
Néron's work strengthening Silverman's results, which were for systems
having only one map.
",0,0,1,0,0,0
14325,"Practical volume computation of structured convex bodies, and an application to modeling portfolio dependencies and financial crises","  We examine volume computation of general-dimensional polytopes and more
general convex bodies, defined as the intersection of a simplex by a family of
parallel hyperplanes, and another family of parallel hyperplanes or a family of
concentric ellipsoids. Such convex bodies appear in modeling and predicting
financial crises. The impact of crises on the economy (labor, income, etc.)
makes its detection of prime interest. Certain features of dependencies in the
markets clearly identify times of turmoil. We describe the relationship between
asset characteristics by means of a copula; each characteristic is either a
linear or quadratic form of the portfolio components, hence the copula can be
constructed by computing volumes of convex bodies. We design and implement
practical algorithms in the exact and approximate setting, we experimentally
juxtapose them and study the tradeoff of exactness and accuracy for speed. We
analyze the following methods in order of increasing generality: rejection
sampling relying on uniformly sampling the simplex, which is the fastest
approach, but inaccurate for small volumes; exact formulae based on the
computation of integrals of probability distribution functions; an optimized
Lawrence sign decomposition method, since the polytopes at hand are shown to be
simple; Markov chain Monte Carlo algorithms using random walks based on the
hit-and-run paradigm generalized to nonlinear convex bodies and relying on new
methods for computing a ball enclosed; the latter is experimentally extended to
non-convex bodies with very encouraging results. Our C++ software, based on
CGAL and Eigen and available on github, is shown to be very effective in up to
100 dimensions. Our results offer novel, effective means of computing portfolio
dependencies and an indicator of financial crises, which is shown to correctly
identify past crises.
",0,0,0,0,0,1
833,Linear Stochastic Approximation: Constant Step-Size and Iterate Averaging,"  We consider $d$-dimensional linear stochastic approximation algorithms (LSAs)
with a constant step-size and the so called Polyak-Ruppert (PR) averaging of
iterates. LSAs are widely applied in machine learning and reinforcement
learning (RL), where the aim is to compute an appropriate $\theta_{*} \in
\mathbb{R}^d$ (that is an optimum or a fixed point) using noisy data and $O(d)$
updates per iteration. In this paper, we are motivated by the problem (in RL)
of policy evaluation from experience replay using the \emph{temporal
difference} (TD) class of learning algorithms that are also LSAs. For LSAs with
a constant step-size, and PR averaging, we provide bounds for the mean squared
error (MSE) after $t$ iterations. We assume that data is \iid with finite
variance (underlying distribution being $P$) and that the expected dynamics is
Hurwitz. For a given LSA with PR averaging, and data distribution $P$
satisfying the said assumptions, we show that there exists a range of constant
step-sizes such that its MSE decays as $O(\frac{1}{t})$.
We examine the conditions under which a constant step-size can be chosen
uniformly for a class of data distributions $\mathcal{P}$, and show that not
all data distributions `admit' such a uniform constant step-size. We also
suggest a heuristic step-size tuning algorithm to choose a constant step-size
of a given LSA for a given data distribution $P$. We compare our results with
related work and also discuss the implication of our results in the context of
TD algorithms that are LSAs.
",1,0,0,1,0,0
5402,Option Pricing Models Driven by the Space-Time Fractional Diffusion: Series Representation and Applications,"  In this paper, we focus on option pricing models based on space-time
fractional diffusion. We briefly revise recent results which show that the
option price can be represented in the terms of rapidly converging
double-series and apply these results to the data from real markets. We focus
on estimation of model parameters from the market data and estimation of
implied volatility within the space-time fractional option pricing models.
",0,0,0,0,0,1
670,Robust Detection of Covariate-Treatment Interactions in Clinical Trials,"  Detection of interactions between treatment effects and patient descriptors
in clinical trials is critical for optimizing the drug development process. The
increasing volume of data accumulated in clinical trials provides a unique
opportunity to discover new biomarkers and further the goal of personalized
medicine, but it also requires innovative robust biomarker detection methods
capable of detecting non-linear, and sometimes weak, signals. We propose a set
of novel univariate statistical tests, based on the theory of random walks,
which are able to capture non-linear and non-monotonic covariate-treatment
interactions. We also propose a novel combined test, which leverages the power
of all of our proposed univariate tests into a single general-case tool. We
present results for both synthetic trials as well as real-world clinical
trials, where we compare our method with state-of-the-art techniques and
demonstrate the utility and robustness of our approach.
",0,0,0,1,0,0
4285,Generalizing the first-difference correlated random walk for marine animal movement data,"  Animal telemetry data are often analysed with discrete time movement models
assuming rotation in the movement. These models are defined with equidistant
distant time steps. However, telemetry data from marine animals are observed
irregularly. To account for irregular data, a time-irregularised
first-difference correlated random walk model with drift is introduced. The
model generalizes the commonly used first-difference correlated random walk
with regular time steps by allowing irregular time steps, including a drift
term, and by allowing different autocorrelation in the two coordinates. The
model is applied to data from a ringed seal collected through the Argos
satellite system, and is compared to related movement models through
simulations. Accounting for irregular data in the movement model results in
accurate parameter estimates and reconstruction of movement paths. Measured by
distance, the introduced model can provide more accurate movement paths than
the regular time counterpart. Extracting accurate movement paths from uncertain
telemetry data is important for evaluating space use patterns for marine
animals, which in turn is crucial for management. Further, handling irregular
data directly in the movement model allows efficient simultaneous analysis of
several animals.
",0,0,0,0,1,0
14138,Gaussian Approximation of a Risk Model with Stationary Hawkes Arrivals of Claims,"  We consider a classical risk process with arrival of claims following a
stationary Hawkes process. We study the asymptotic regime when the premium rate
and the baseline intensity of the claims arrival process are large, and claim
size is small. The main goal of this article is to establish a diffusion
approximation by verifying a functional central limit theorem of this model and
to compute both the finite-time and infinite-time horizon ruin probabilities.
Numerical results will also be given.
",0,0,0,0,0,1
746,Bounded height in families of dynamical systems,"  Let a and b be algebraic numbers such that exactly one of a and b is an
algebraic integer, and let f_t(z):=z^2+t be a family of polynomials
parametrized by t. We prove that the set of all algebraic numbers t for which
there exist positive integers m and n such that f_t^m(a)=f_t^n(b) has bounded
Weil height. This is a special case of a more general result supporting a new
bounded height conjecture in dynamics. Our results fit into the general setting
of the principle of unlikely intersections in arithmetic dynamics.
",0,0,1,0,0,0
501,A compact design for velocity-map imaging energetic electrons and ions,"  We present a compact design for a velocity-map imaging spectrometer for
energetic electrons and ions. The standard geometry by Eppink and Parker [A. T.
J. B. Eppink and D. H. Parker, Rev. Sci. Instrum. 68, 3477 (1997)] is augmented
by just two extended electrodes so as to realize an additional einzel lens. In
this way, for a maximum electrode voltage of 7 kV we experimentally demonstrate
imaging of electrons with energies up to 65 eV. Simulations show that energy
acceptances of <270 and <1,200 eV with an energy resolution of dE / E <5% are
achievable for electrode voltages of <20 kV when using diameters of the
position-sensitive detector of 42 and 78 mm, respectively.
",0,1,0,0,0,0
5991,Fast Characterization of Segmental Duplications in Genome Assemblies,"  Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA
greater than 1 Kbp with high sequence identity that are copied to other regions
of the genome. SDs are among the most important sources of evolution, a common
cause of genomic structural variation, and several are associated with diseases
of genomic origin. Despite their functional importance, SDs present one of the
major hurdles for de novo genome assembly due to the ambiguity they cause in
building and traversing both state-of-the-art overlap-layout-consensus and de
Bruijn graphs. This causes SD regions to be misassembled, collapsed into a
unique representation, or completely missing from assembled reference genomes
for various organisms. In turn, this missing or incorrect information limits
our ability to fully understand the evolution and the architecture of the
genomes. Despite the essential need to accurately characterize SDs in
assemblies, there is only one tool that has been developed for this purpose,
called Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several
steps that employ different tools and custom scripts, which makes it difficult
and time consuming to use. Thus there is still a need for algorithms to
characterize within-assembly SDs quickly, accurately, and in a user friendly
manner.
Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to
rapidly detect SDs through sophisticated filtering strategies based on Jaccard
similarity and local chaining. We show that SEDEF accurately detects SDs while
maintaining substantial speed up over WGAC that translates into practical run
times of minutes instead of weeks. Notably, our algorithm captures up to 25%
pairwise error between segments, where previous studies focused on only 10%,
allowing us to more deeply track the evolutionary history of the genome.
SEDEF is available at this https URL
",0,0,0,0,1,0
541,Gravitational radiation from compact binary systems in screened modified gravity,"  Screened modified gravity (SMG) is a kind of scalar-tensor theory with
screening mechanisms, which can suppress the fifth force in dense regions and
allow theories to evade the solar system and laboratory tests. In this paper,
we investigate how the screening mechanisms in SMG affect the gravitational
radiation damping effects, calculate in detail the rate of the energy loss due
to the emission of tensor and scalar gravitational radiations, and derive their
contributions to the change in the orbital period of the binary system. We find
that the scalar radiation depends on the screened parameters and the
propagation speed of scalar waves, and the scalar dipole radiation dominates
the orbital decay of the binary system. For strongly self-gravitating bodies,
all effects of scalar sector are strongly suppressed by the screening
mechanisms in SMG. By comparing our results to observations of binary system
PSR J1738+0333, we place the stringent constraints on the screening mechanisms
in SMG. As an application of these results, we focus on three specific models
of SMG (chameleon, symmetron, and dilaton), and derive the constraints on the
model parameters, respectively.
",0,1,0,0,0,0
297,Quantum Speed Limit is Not Quantum,"  The quantum speed limit (QSL), or the energy-time uncertainty relation,
describes the fundamental maximum rate for quantum time evolution and has been
regarded as being unique in quantum mechanics. In this study, we obtain a
classical speed limit corresponding to the QSL using the Hilbert space for the
classical Liouville equation. Thus, classical mechanics has a fundamental speed
limit, and QSL is not a purely quantum phenomenon but a universal dynamical
property of the Hilbert space. Furthermore, we obtain similar speed limits for
the imaginary-time Schroedinger equations such as the master equation.
",0,1,0,0,0,0
874,Convolved subsampling estimation with applications to block bootstrap,"  The block bootstrap approximates sampling distributions from dependent data
by resampling data blocks. A fundamental problem is establishing its
consistency for the distribution of a sample mean, as a prototypical statistic.
We use a structural relationship with subsampling to characterize the bootstrap
in a new and general manner. While subsampling and block bootstrap differ, the
block bootstrap distribution of a sample mean equals that of a $k$-fold
self-convolution of a subsampling distribution. Motivated by this, we provide
simple necessary and sufficient conditions for a convolved subsampling
estimator to produce a normal limit that matches the target of bootstrap
estimation. These conditions may be linked to consistency properties of an
original subsampling distribution, which are often obtainable under minimal
assumptions. Through several examples, the results are shown to validate the
block bootstrap for means under significantly weakened assumptions in many
existing (and some new) dependence settings, which also addresses a standing
conjecture of Politis, Romano and Wolf(1999). Beyond sample means, the
convolved subsampling estimator may not match the block bootstrap, but instead
provides a hybrid-resampling estimator of interest in its own right. For
general statistics with normal limits, results also establish the consistency
of convolved subsampling under minimal dependence conditions, including
non-stationarity.
",0,0,1,1,0,0
4475,Scaling Limits for Super--replication with Transient Price Impact,"  We prove limit theorems for the super-replication cost of European options in
a Binomial model with transient price impact. We show that if the time step
goes to zero and the effective resilience between consecutive trading times
remains constant then the limit of the super--replication prices coincide with
the scaling limit for temporary price impact with a modified market depth.
",0,0,0,0,0,1
5182,Stochastic Model of SIR Epidemic Modelling,"  Threshold theorem is probably the most important development of mathematical
epidemic modelling. Unfortunately, some models may not behave according to the
threshold. In this paper, we will focus on the final outcome of SIR model with
demography. The behaviour of the model approached by deteministic and
stochastic models will be introduced, mainly using simulations. Furthermore, we
will also investigate the dynamic of susceptibles in population in absence of
infective. We have successfully showed that both deterministic and stochastic
models performed similar results when $R_0 \leq 1$. That is, the disease-free
stage in the epidemic. But when $R_0 > 1$, the deterministic and stochastic
approaches had different interpretations.
",0,0,0,1,1,0
610,Communication Reducing Algorithms for Distributed Hierarchical N-Body Problems with Boundary Distributions,"  Reduction of communication and efficient partitioning are key issues for
achieving scalability in hierarchical $N$-Body algorithms like FMM. In the
present work, we propose four independent strategies to improve partitioning
and reduce communication. First of all, we show that the conventional wisdom of
using space-filling curve partitioning may not work well for boundary integral
problems, which constitute about 50% of FMM's application user base. We propose
an alternative method which modifies orthogonal recursive bisection to solve
the cell-partition misalignment that has kept it from scaling previously.
Secondly, we optimize the granularity of communication to find the optimal
balance between a bulk-synchronous collective communication of the local
essential tree and an RDMA per task per cell. Finally, we take the dynamic
sparse data exchange proposed by Hoefler et al. and extend it to a hierarchical
sparse data exchange, which is demonstrated at scale to be faster than the MPI
library's MPI_Alltoallv that is commonly used.
",1,0,0,0,0,0
989,Characterizing complex networks using Entropy-degree diagrams: unveiling changes in functional brain connectivity induced by Ayahuasca,"  Open problems abound in the theory of complex networks, which has found
successful application to diverse fields of science. With the aim of further
advancing the understanding of the brain's functional connectivity, we propose
to evaluate a network metric which we term the geodesic entropy. This entropy,
in a way that can be made precise, quantifies the Shannon entropy of the
distance distribution to a specific node from all other nodes. Measurements of
geodesic entropy allow for the characterization of the structural information
of a network that takes into account the distinct role of each node into the
network topology. The measurement and characterization of this structural
information has the potential to greatly improve our understanding of sustained
activity and other emergent behaviors in networks, such as self-organized
criticality sometimes seen in such contexts. We apply these concepts and
methods to study the effects of how the psychedelic Ayahuasca affects the
functional connectivity of the human brain. We show that the geodesic entropy
is able to differentiate the functional networks of the human brain in two
different states of consciousness in the resting state: (i) the ordinary waking
state and (ii) a state altered by ingestion of the Ayahuasca. The entropy of
the nodes of brain networks from subjects under the influence of Ayahuasca
diverge significantly from those of the ordinary waking state. The functional
brain networks from subjects in the altered state have, on average, a larger
geodesic entropy compared to the ordinary state. We conclude that geodesic
entropy is a useful tool for analyzing complex networks and discuss how and why
it may bring even further valuable insights into the study of the human brain
and other empirical networks.
",0,0,0,0,1,0
394,On the optimality and sharpness of Laguerre's lower bound on the smallest eigenvalue of a symmetric positive definite matrix,"  Lower bounds on the smallest eigenvalue of a symmetric positive definite
matrices $A\in\mathbb{R}^{m\times m}$ play an important role in condition
number estimation and in iterative methods for singular value computation. In
particular, the bounds based on ${\rm Tr}(A^{-1})$ and ${\rm Tr}(A^{-2})$
attract attention recently because they can be computed in $O(m)$ work when $A$
is tridiagonal. In this paper, we focus on these bounds and investigate their
properties in detail. First, we consider the problem of finding the optimal
bound that can be computed solely from ${\rm Tr}(A^{-1})$ and ${\rm
Tr}(A^{-2})$ and show that so called Laguerre's lower bound is the optimal one
in terms of sharpness. Next, we study the gap between the Laguerre bound and
the smallest eigenvalue. We characterize the situation in which the gap becomes
largest in terms of the eigenvalue distribution of $A$ and show that the gap
becomes smallest when ${\rm Tr}(A^{-2})/\{{\rm Tr}(A^{-1})\}^2$ approaches 1 or
$\frac{1}{m}$. These results will be useful, for example, in designing
efficient shift strategies for singular value computation algorithms.
",1,0,1,0,0,0
984,On the complexity of solving a decision problem with flow-depending costs: the case of the IJsselmeer dikes,"  We consider a fundamental integer programming (IP) model for cost-benefit
analysis flood protection through dike building in the Netherlands, due to
Verweij and Zwaneveld.
Experimental analysis with data for the Ijsselmeer lead to integral optimal
solution of the linear programming relaxation of the IP model.
This naturally led to the question of integrality of the polytope associated
with the IP model.
In this paper we first give a negative answer to this question by
establishing non-integrality of the polytope.
Second, we establish natural conditions that guarantee the linear programming
relaxation of the IP model to be integral.
We then test the most recent data on flood probabilities, damage and
investment costs of the IJsselmeer for these conditions.
Third, we show that the IP model can be solved in polynomial time when the
number of dike segments, or the number of feasible barrier heights, are
constant.
",0,0,0,0,0,1
3293,Nutritionally recommended food for semi- to strict vegetarian diets based on large-scale nutrient composition data,"  Diet design for vegetarian health is challenging due to the limited food
repertoire of vegetarians. This challenge can be partially overcome by
quantitative, data-driven approaches that utilise massive nutritional
information collected for many different foods. Based on large-scale data of
foods' nutrient compositions, the recent concept of nutritional fitness helps
quantify a nutrient balance within each food with regard to satisfying daily
nutritional requirements. Nutritional fitness offers prioritisation of
recommended foods using the foods' occurrence in nutritionally adequate food
combinations. Here, we systematically identify nutritionally recommendable
foods for semi- to strict vegetarian diets through the computation of
nutritional fitness. Along with commonly recommendable foods across different
diets, our analysis reveals favourable foods specific to each diet, such as
immature lima beans for a vegan diet as an amino acid and choline source, and
mushrooms for ovo-lacto vegetarian and vegan diets as a vitamin D source.
Furthermore, we find that selenium and other essential micronutrients can be
subject to deficiency in plant-based diets, and suggest nutritionally-desirable
dietary patterns. We extend our analysis to two hypothetical scenarios of
highly personalised, plant-based methionine-restricted diets. Our
nutrient-profiling approach may provide a useful guide for designing different
types of personalised vegetarian diets.
",1,0,0,0,1,0
3796,"Volvox barberi flocks, forming near-optimal, two-dimensional, polydisperse lattice packings","  Volvox barberi is a multicellular green alga forming spherical colonies of
10000-50000 differentiated somatic and germ cells. Here, I show that these
colonies actively self-organize over minutes into ""flocks"" that can contain
more than 100 colonies moving and rotating collectively for hours. The colonies
in flocks form two-dimensional, irregular, ""active crystals"", with lattice
angles and colony diameters both following log-normal distributions. Comparison
with a dynamical simulation of soft spheres with diameters matched to the
Volvox samples, and a weak long-range attractive force, show that the Volvox
flocks achieve optimal random close-packing. A dye tracer in the Volvox medium
revealed large hydrodynamic vortices generated by colony and flock rotations,
providing a likely source of the forces leading to flocking and optimal
packing.
",0,0,0,0,1,0
371,Polygons with prescribed edge slopes: configuration space and extremal points of perimeter,"  We describe the configuration space $\mathbf{S}$ of polygons with prescribed
edge slopes, and study the perimeter $\mathcal{P}$ as a Morse function on
$\mathbf{S}$. We characterize critical points of $\mathcal{P}$ (these are
\textit{tangential} polygons) and compute their Morse indices. This setup is
motivated by a number of results about critical points and Morse indices of the
oriented area function defined on the configuration space of polygons with
prescribed edge lengths (flexible polygons). As a by-product, we present an
independent computation of the Morse index of the area function (obtained
earlier by G. Panina and A. Zhukova).
",0,0,1,0,0,0
1498,"Thermoregulation in mice, rats and humans: An insight into the evolution of human hairlessness","  The thermoregulation system in animals removes body heat in hot temperatures
and retains body heat in cold temperatures. The better the animal removes heat,
the worse the animal retains heat and visa versa. It is the balance between
these two conflicting goals that determines the mammal's size, heart rate and
amount of hair. The rat's loss of tail hair and human's loss of its body hair
are responses to these conflicting thermoregulation needs as these animals
evolved to larger size over time.
",0,0,0,0,1,0
423,More new classes of permutation trinomials over $\mathbb{F}_{2^n}$,"  Permutation polynomials over finite fields have wide applications in many
areas of science and engineering. In this paper, we present six new classes of
permutation trinomials over $\mathbb{F}_{2^n}$ which have explicit forms by
determining the solutions of some equations.
",0,0,1,0,0,0
390,Toward Optimal Coupon Allocation in Social Networks: An Approximate Submodular Optimization Approach,"  CMO Council reports that 71\% of internet users in the U.S. were influenced
by coupons and discounts when making their purchase decisions. It has also been
shown that offering coupons to a small fraction of users (called seed users)
may affect the purchase decisions of many other users in a social network. This
motivates us to study the optimal coupon allocation problem, and our objective
is to allocate coupons to a set of users so as to maximize the expected
cascade. Different from existing studies on influence maximizaton (IM), our
framework allows a general utility function and a more complex set of
constraints. In particular, we formulate our problem as an approximate
submodular maximization problem subject to matroid and knapsack constraints.
Existing techniques relying on the submodularity of the utility function, such
as greedy algorithm, can not work directly on a non-submodular function. We use
$\epsilon$ to measure the difference between our function and its closest
submodular function and propose a novel approximate algorithm with
approximation ratio $\beta(\epsilon)$ with $\lim_{\epsilon\rightarrow
0}\beta(\epsilon)=1-1/e$. This is the best approximation guarantee for
approximate submodular maximization subject to a partition matroid and knapsack
constraints, our results apply to a broad range of optimization problems that
can be formulated as an approximate submodular maximization problem.
",1,0,0,0,0,0
696,Dynamics of the multi-soliton waves in the sine-Gordon model with two identical point impurities,"  The particular type of four-kink multi-solitons (or quadrons) adiabatic
dynamics of the sine-Gordon equation in a model with two identical point
attracting impurities has been studied. This model can be used for describing
magnetization localized waves in multilayer ferromagnet. The quadrons structure
and properties has been numerically investigated. The cases of both large and
small distances between impurities has been viewed. The dependence of the
localized in impurity region nonlinear high-amplitude waves frequencies on the
distance between the impurities has been found. For an analytical description
of two bound localized on impurities nonlinear waves dynamics, using
perturbation theory, the system of differential equations for harmonic
oscillators with elastic link has been found. The analytical model
qualitatively describes the results of the sine-Gordon equation numerical
simulation.
",0,1,0,0,0,0
13254,Continuity of Utility Maximization under Weak Convergence,"  In this paper we find sufficient conditions for the continuity of the value
of the utility maximization problem from terminal wealth with respect to the
convergence in distribution of the underlying processes. We provide several
examples which illustrate that without these conditions, we cannot generally
expect continuity to hold. Finally, we apply our results to the computation of
the minimum shortfall in the Heston model by building an appropriate lattice
approximation.
",0,0,0,0,0,1
259,Performance of Energy Harvesting Receivers with Power Optimization,"  The difficulty of modeling energy consumption in communication systems leads
to challenges in energy harvesting (EH) systems, in which nodes scavenge energy
from their environment. An EH receiver must harvest enough energy for
demodulating and decoding. The energy required depends upon factors, like code
rate and signal-to-noise ratio, which can be adjusted dynamically. We consider
a receiver which harvests energy from ambient sources and the transmitter,
meaning the received signal is used for both EH and information decoding.
Assuming a generalized function for energy consumption, we maximize the total
number of information bits decoded, under both average and peak power
constraints at the transmitter, by carefully optimizing the power used for EH,
power used for information transmission, fraction of time for EH, and code
rate. For transmission over a single block, we find there exist problem
parameters for which either maximizing power for information transmission or
maximizing power for EH is optimal. In the general case, the optimal solution
is a tradeoff of the two. For transmission over multiple blocks, we give an
upper bound on performance and give sufficient and necessary conditions to
achieve this bound. Finally, we give some numerical results to illustrate our
results and analysis.
",1,0,0,0,0,0
4683,Opinion Dynamics via Search Engines (and other Algorithmic Gatekeepers),"  Ranking algorithms are the information gatekeepers of the Internet era. We
develop a stylized model to study the effects of ranking algorithms on opinion
dynamics. We consider a search engine that uses an algorithm based on
popularity and on personalization. We find that popularity-based rankings
generate an advantage of the fewer effect: fewer websites reporting a given
signal attract relatively more traffic overall. This highlights a novel,
ranking-driven channel that explains the diffusion of misinformation, as
websites reporting incorrect information may attract an amplified amount of
traffic precisely because they are few. Furthermore, when individuals provide
sufficiently positive feedback to the ranking algorithm, popularity-based
rankings tend to aggregate information while personalization acts in the
opposite direction.
",1,0,0,0,0,1
590,Semiblind subgraph reconstruction in Gaussian graphical models,"  Consider a social network where only a few nodes (agents) have meaningful
interactions in the sense that the conditional dependency graph over node
attribute variables (behaviors) is sparse. A company that can only observe the
interactions between its own customers will generally not be able to accurately
estimate its customers' dependency subgraph: it is blinded to any external
interactions of its customers and this blindness creates false edges in its
subgraph. In this paper we address the semiblind scenario where the company has
access to a noisy summary of the complementary subgraph connecting external
agents, e.g., provided by a consolidator. The proposed framework applies to
other applications as well, including field estimation from a network of awake
and sleeping sensors and privacy-constrained information sharing over social
subnetworks. We propose a penalized likelihood approach in the context of a
graph signal obeying a Gaussian graphical models (GGM). We use a convex-concave
iterative optimization algorithm to maximize the penalized likelihood.
",1,0,0,1,0,0
880,A Topologist's View of Kinematic Maps and Manipulation Complexity,"  In this paper we combine a survey of the most important topological
properties of kinematic maps that appear in robotics, with the exposition of
some basic results regarding the topological complexity of a map. In
particular, we discuss mechanical devices that consist of rigid parts connected
by joints and show how the geometry of the joints determines the forward
kinematic map that relates the configuration of joints with the pose of the
end-effector of the device. We explain how to compute the dimension of the
joint space and describe topological obstructions for a kinematic map to be a
fibration or to admit a continuous section. In the second part of the paper we
define the complexity of a continuous map and show how the concept can be
viewed as a measure of the difficulty to find a robust manipulation plan for a
given mechanical device. We also derive some basic estimates for the complexity
and relate it to the degree of instability of a manipulation plan.
",1,0,1,0,0,0
308,Seismic fragility curves for structures using non-parametric representations,"  Fragility curves are commonly used in civil engineering to assess the
vulnerability of structures to earthquakes. The probability of failure
associated with a prescribed criterion (e.g. the maximal inter-storey drift of
a building exceeding a certain threshold) is represented as a function of the
intensity of the earthquake ground motion (e.g. peak ground acceleration or
spectral acceleration). The classical approach relies on assuming a lognormal
shape of the fragility curves; it is thus parametric. In this paper, we
introduce two non-parametric approaches to establish the fragility curves
without employing the above assumption, namely binned Monte Carlo simulation
and kernel density estimation. As an illustration, we compute the fragility
curves for a three-storey steel frame using a large number of synthetic ground
motions. The curves obtained with the non-parametric approaches are compared
with respective curves based on the lognormal assumption. A similar comparison
is presented for a case when a limited number of recorded ground motions is
available. It is found that the accuracy of the lognormal curves depends on the
ground motion intensity measure, the failure criterion and most importantly, on
the employed method for estimating the parameters of the lognormal shape.
",0,0,0,1,0,0
3574,Spatially-resolved Brillouin spectroscopy reveals biomechanical changes in early ectatic corneal disease and post-crosslinking in vivo,"  Mounting evidence connects the biomechanical properties of tissues to the
development of eye diseases such as keratoconus, a common disease in which the
cornea thins and bulges into a conical shape. However, measuring biomechanical
changes in vivo with sufficient sensitivity for disease detection has proved
challenging. Here, we present a first large-scale study (~200 subjects,
including normal and keratoconus patients) using Brillouin light-scattering
microscopy to measure longitudinal modulus in corneal tissues with high
sensitivity and spatial resolution. Our results in vivo provide evidence of
biomechanical inhomogeneity at the onset of keratoconus and suggest that
biomechanical asymmetry between the left and right eyes may presage disease
development. We additionally measure the stiffening effect of corneal
crosslinking treatment in vivo for the first time. Our results demonstrate the
promise of Brillouin microscopy for diagnosis and treatment of keratoconus, and
potentially other diseases.
",0,0,0,0,1,0
6497,Portfolio Optimization under Fast Mean-reverting and Rough Fractional Stochastic Environment,"  Fractional stochastic volatility models have been widely used to capture the
non-Markovian structure revealed from financial time series of realized
volatility. On the other hand, empirical studies have identified scales in
stock price volatility: both fast-time scale on the order of days and
slow-scale on the order of months. So, it is natural to study the portfolio
optimization problem under the effects of dependence behavior which we will
model by fractional Brownian motions with Hurst index $H$, and in the fast or
slow regimes characterized by small parameters $\eps$ or $\delta$. For the
slowly varying volatility with $H \in (0,1)$, it was shown that the first order
correction to the problem value contains two terms of order $\delta^H$, one
random component and one deterministic function of state processes, while for
the fast varying case with $H > \half$, the same form holds at order
$\eps^{1-H}$. This paper is dedicated to the remaining case of a fast-varying
rough environment ($H < \half$) which exhibits a different behavior. We show
that, in the expansion, only one deterministic term of order $\sqrt{\eps}$
appears in the first order correction.
",0,0,0,0,0,1
438,Mass and moment of inertia govern the transition in the dynamics and wakes of freely rising and falling cylinders,"  In this Letter, we study the motion and wake-patterns of freely rising and
falling cylinders in quiescent fluid. We show that the amplitude of oscillation
and the overall system-dynamics are intricately linked to two parameters: the
particle's mass-density relative to the fluid $m^* \equiv \rho_p/\rho_f$ and
its relative moment-of-inertia $I^* \equiv {I}_p/{I}_f$. This supersedes the
current understanding that a critical mass density ($m^*\approx$ 0.54) alone
triggers the sudden onset of vigorous vibrations. Using over 144 combinations
of ${m}^*$ and $I^*$, we comprehensively map out the parameter space covering
very heavy ($m^* > 10$) to very buoyant ($m^* < 0.1$) particles. The entire
data collapses into two scaling regimes demarcated by a transitional Strouhal
number, $St_t \approx 0.17$. $St_t$ separates a mass-dominated regime from a
regime dominated by the particle's moment of inertia. A shift from one regime
to the other also marks a gradual transition in the wake-shedding pattern: from
the classical $2S$~(2-Single) vortex mode to a $2P$~(2-Pairs) vortex mode.
Thus, auto-rotation can have a significant influence on the trajectories and
wakes of freely rising isotropic bodies.
",0,1,0,0,0,0
349,Ensemble Estimation of Mutual Information,"  We derive the mean squared error convergence rates of kernel density-based
plug-in estimators of mutual information measures between two multidimensional
random variables $\mathbf{X}$ and $\mathbf{Y}$ for two cases: 1) $\mathbf{X}$
and $\mathbf{Y}$ are both continuous; 2) $\mathbf{X}$ is continuous and
$\mathbf{Y}$ is discrete. Using the derived rates, we propose an ensemble
estimator of these information measures for the second case by taking a
weighted sum of the plug-in estimators with varied bandwidths. The resulting
ensemble estimator achieves the $1/N$ parametric convergence rate when the
conditional densities of the continuous variables are sufficiently smooth. To
the best of our knowledge, this is the first nonparametric mutual information
estimator known to achieve the parametric convergence rate for this case, which
frequently arises in applications (e.g. variable selection in classification).
The estimator is simple to implement as it uses the solution to an offline
convex optimization problem and simple plug-in estimators. A central limit
theorem is also derived for the ensemble estimator. Ensemble estimators that
achieve the parametric rate are also derived for the first case ($\mathbf{X}$
and $\mathbf{Y}$ are both continuous) and another case 3) $\mathbf{X}$ and
$\mathbf{Y}$ may have any mixture of discrete and continuous components.
",1,0,1,1,0,0
863,Bäcklund Transformation and Quasi-Integrable Deformation of Mixed Fermi-Pasta-Ulam and Frenkel-Kontorova Models,"  In this paper we study a non-linear partial differential equation (PDE),
proposed by N. Kudryashov [arXiv:1611.06813v1[nlin.SI]], using continuum limit
approximation of mixed Fermi-Pasta-Ulam and Frenkel-Kontorova Models. This
generalized semi-discrete equation can be considered as a model for the
description of non-linear dislocation waves in crystal lattice and the
corresponding continuous system can be called mixed generalized potential KdV
and sine-Gordon equation. We obtain the Bäcklund transformation of this
equation in Riccati form in inverse method. We further study the
quasi-integrable deformation of this model.
",0,1,1,0,0,0
684,Exploring the predictability of range-based volatility estimators using RNNs,"  We investigate the predictability of several range-based stock volatility
estimators, and compare them to the standard close-to-close estimator which is
most commonly acknowledged as the volatility. The patterns of volatility
changes are analyzed using LSTM recurrent neural networks, which are a state of
the art method of sequence learning. We implement the analysis on all current
constituents of the Dow Jones Industrial Average index, and report averaged
evaluation results. We find that changes in the values of range-based
estimators are more predictable than that of the estimator using daily closing
values only.
",0,0,0,1,0,1
6018,Implementation of Control Strategies for Sterile Insect Techniques,"  In this paper, we propose a sex-structured entomological model that serves as
a basis for design of control strategies relying on releases of sterile male
mosquitoes (Aedes spp) and aiming at elimination of the wild vector population
in some target locality. We consider different types of releases (constant and
periodic impulsive), providing necessary conditions to reach elimination.
However, the main part of the paper is focused on the study of the periodic
impulsive control in different situations. When the size of wild mosquito
population cannot be assessed in real time, we propose the so-called open-loop
control strategy that relies on periodic impulsive releases of sterile males
with constant release size. Under this control mode, global convergence towards
the mosquito-free equilibrium is proved on the grounds of sufficient condition
that relates the size and frequency of releases. If periodic assessments
(either synchronized with releases or more sparse) of the wild population size
are available in real time, we propose the so-called closed-loop control
strategy, which is adjustable in accordance with reliable estimations of the
wild population sizes. Under this control mode, global convergence to the
mosquito-free equilibrium is proved on the grounds of another sufficient
condition that relates not only the size and frequency of periodic releases but
also the frequency of sparse measurements taken on wild populations. Finally,
we propose a mixed control strategy that combines open-loop and closed-loop
strategies. This control mode renders the best result, in terms of overall time
needed to reach elimination and the number of releases to be effectively
carried out during the whole release campaign, while requiring for a reasonable
amount of released sterile insects.
",0,0,0,0,1,0
9816,Systems of ergodic BSDE arising in regime switching forward performance processes,"  We introduce and solve a new type of quadratic backward stochastic
differential equation systems defined in an infinite time horizon, called
\emph{ergodic BSDE systems}. Such systems arise naturally as candidate
solutions to characterize forward performance processes and their associated
optimal trading strategies in a regime switching market. In addition, we
develop a connection between the solution of the ergodic BSDE system and the
long-term growth rate of classical utility maximization problems, and use the
ergodic BSDE system to study the large time behavior of PDE systems with
quadratic growth Hamiltonians.
",0,0,0,0,0,1
375,Willis Theory via Graphs,"  We study the scale and tidy subgroups of an endomorphism of a totally
disconnected locally compact group using a geometric framework. This leads to
new interpretations of tidy subgroups and the scale function. Foremost, we
obtain a geometric tidying procedure which applies to endomorphisms as well as
a geometric proof of the fact that tidiness is equivalent to being minimizing
for a given endomorphism. Our framework also yields an endomorphism version of
the Baumgartner-Willis tree representation theorem. We conclude with a
construction of new endomorphisms of totally disconnected locally compact
groups from old via HNN-extensions.
",0,0,1,0,0,0
483,Susceptibility Propagation by Using Diagonal Consistency,"  A susceptibility propagation that is constructed by combining a belief
propagation and a linear response method is used for approximate computation
for Markov random fields. Herein, we formulate a new, improved susceptibility
propagation by using the concept of a diagonal matching method that is based on
mean-field approaches to inverse Ising problems. The proposed susceptibility
propagation is robust for various network structures, and it is reduced to the
ordinary susceptibility propagation and to the adaptive
Thouless-Anderson-Palmer equation in special cases.
",0,0,1,1,0,0
793,Linear complexity of Legendre-polynomial quotients,"  We continue to investigate binary sequence $(f_u)$ over $\{0,1\}$ defined by
$(-1)^{f_u}=\left(\frac{(u^w-u^{wp})/p}{p}\right)$ for integers $u\ge 0$, where
$\left(\frac{\cdot}{p}\right)$ is the Legendre symbol and we restrict
$\left(\frac{0}{p}\right)=1$. In an earlier work, the linear complexity of
$(f_u)$ was determined for $w=p-1$ under the assumption of $2^{p-1}\not\equiv 1
\pmod {p^2}$. In this work, we give possible values on the linear complexity of
$(f_u)$ for all $1\le w<p-1$ under the same conditions. We also state that the
case of larger $w(\geq p)$ can be reduced to that of $0\leq w\leq p-1$.
",1,0,1,0,0,0
263,Synchronous Observation on the Spontaneous Transformation of Liquid Metal under Free Falling Microgravity Situation,"  The unusually high surface tension of room temperature liquid metal is
molding it as unique material for diverse newly emerging areas. However, unlike
its practices on earth, such metal fluid would display very different behaviors
when working in space where gravity disappears and surface property dominates
the major physics. So far, few direct evidences are available to understand
such effect which would impede further exploration of liquid metal use for
space. Here to preliminarily probe into this intriguing issue, a low cost
experimental strategy to simulate microgravity environment on earth was
proposed through adopting bridges with high enough free falling distance as the
test platform. Then using digital cameras amounted along x, y, z directions on
outside wall of the transparent container with liquid metal and allied solution
inside, synchronous observations on the transient flow and transformational
activities of liquid metal were performed. Meanwhile, an unmanned aerial
vehicle was adopted to record the whole free falling dynamics of the test
capsule from the far end which can help justify subsequent experimental
procedures. A series of typical fundamental phenomena were thus observed as:
(a) A relatively large liquid metal object would spontaneously transform from
its original planar pool state into a sphere and float in the container if
initiating the free falling; (b) The liquid metal changes its three-dimensional
shape due to dynamic microgravity strength due to free falling and rebound of
the test capsule; and (c) A quick spatial transformation of liquid metal
immersed in the solution can easily be induced via external electrical fields.
The mechanisms of the surface tension driven liquid metal actuation in space
were interpreted. All these findings indicated that microgravity effect should
be fully treated in developing future generation liquid metal space
technologies.
",0,1,0,0,0,0
721,Measurably entire functions and their growth,"  In 1997 B. Weiss introduced the notion of measurably entire functions and
proved that they exist on every arbitrary free C- action defined on standard
probability space. In the same paper he asked about the minimal possible growth
of measurably entire functions. In this work we show that for every arbitrary
free C- action defined on a standard probability space there exists a
measurably entire function whose growth does not exceed exp (exp[log^p |z|])
for any p > 3. This complements a recent result by Buhovski, Glücksam,
Logunov, and Sodin (arXiv:1703.08101) who showed that such functions cannot
grow slower than exp (exp[log^p |z|]) for any p < 2.
",0,0,1,0,0,0
692,Recent Operation of the FNAL Magnetron $H^{-}$ Ion Source,"  This paper will detail changes in the operational paradigm of the Fermi
National Accelerator Laboratory (FNAL) magnetron $H^{-}$ ion source due to
upgrades in the accelerator system. Prior to November of 2012 the $H^{-}$ ions
for High Energy Physics (HEP) experiments were extracted at ~18 keV vertically
downward into a 90 degree bending magnet and accelerated through a
Cockcroft-Walton accelerating column to 750 keV. Following the upgrade in the
fall of 2012 the $H^{-}$ ions are now directly extracted from a magnetron at 35
keV and accelerated to 750 keV by a Radio Frequency Quadrupole (RFQ). This
change in extraction energy as well as the orientation of the ion source
required not only a redesign of the ion source, but an updated understanding of
its operation at these new values. Discussed in detail are the changes to the
ion source timing, arc discharge current, hydrogen gas pressure, and cesium
delivery system that were needed to maintain consistent operation at >99%
uptime for HEP, with an increased ion source lifetime of over 9 months.
",0,1,0,0,0,0
739,Morse geodesics in torsion groups,"  In this paper we exhibit Morse geodesics, often called ""hyperbolic
directions"", in infinite unbounded torsion groups. The groups studied are
lacunary hyperbolic groups and constructed using graded small cancellation
conditions. In all previously known examples, Morse geodesics were found in
groups which also contained Morse elements, infinite order elements whose
cyclic subgroup gives a Morse quasi-geodesic. Our result presents the first
example of a group which contains Morse geodesics but no Morse elements. In
fact, we show that there is an isometrically embedded $7$-regular tree inside
such groups where every infinite, simple path is a Morse geodesic.
",0,0,1,0,0,0
12588,Option market (in)efficiency and implied volatility dynamics after return jumps,"  In informationally efficient financial markets, option prices and this
implied volatility should immediately be adjusted to new information that
arrives along with a jump in underlying's return, whereas gradual changes in
implied volatility would indicate market inefficiency. Using minute-by-minute
data on S&P 500 index options, we provide evidence regarding delayed and
gradual movements in implied volatility after the arrival of return jumps.
These movements are directed and persistent, especially in the case of negative
return jumps. Our results are significant when the implied volatilities are
extracted from at-the-money options and out-of-the-money puts, while the
implied volatility obtained from out-of-the-money calls converges to its new
level immediately rather than gradually. Thus, our analysis reveals that the
implied volatility smile is adjusted to jumps in underlying's return
asymmetrically. Finally, it would be possible to have statistical arbitrage in
zero-transaction-cost option markets, but under actual option price spreads,
our results do not imply abnormal option returns.
",0,0,0,0,0,1
337,Learning to Adapt by Minimizing Discrepancy,"  We explore whether useful temporal neural generative models can be learned
from sequential data without back-propagation through time. We investigate the
viability of a more neurocognitively-grounded approach in the context of
unsupervised generative modeling of sequences. Specifically, we build on the
concept of predictive coding, which has gained influence in cognitive science,
in a neural framework. To do so we develop a novel architecture, the Temporal
Neural Coding Network, and its learning algorithm, Discrepancy Reduction. The
underlying directed generative model is fully recurrent, meaning that it
employs structural feedback connections and temporal feedback connections,
yielding information propagation cycles that create local learning signals.
This facilitates a unified bottom-up and top-down approach for information
transfer inside the architecture. Our proposed algorithm shows promise on the
bouncing balls generative modeling problem. Further experiments could be
conducted to explore the strengths and weaknesses of our approach.
",1,0,0,1,0,0
5048,"Fluid-Structure Interaction for the Classroom: Interpolation, Hearts, and Swimming!","  While students may find spline interpolation easily digestible, based on
their familiarity with continuity of a function and its derivatives, some of
its inherent value may be missed when students only see it applied to standard
data interpolation exercises. In this paper, we offer alternatives where
students can qualitatively and quantitatively witness the resulting dynamical
differences when objects are driven through a fluid using different spline
interpolation methods. They say, seeing is believing; here we showcase the
differences between linear and cubic spline interpolation using examples from
fluid pumping and aquatic locomotion. Moreover, students can define their own
interpolation functions and visualize the dynamics unfold. To solve the
fluid-structure interaction system, the open source software IB2d is used. In
that vein, all simulation codes, analysis scripts, and movies are provided for
streamlined use.
",0,0,0,0,1,0
744,The LCES HIRES/Keck Precision Radial Velocity Exoplanet Survey,"  We describe a 20-year survey carried out by the Lick-Carnegie Exoplanet
Survey Team (LCES), using precision radial velocities from HIRES on the Keck-I
telescope to find and characterize extrasolar planetary systems orbiting nearby
F, G, K, and M dwarf stars. We provide here 60,949 precision radial velocities
for 1,624 stars contained in that survey. We tabulate a list of 357 significant
periodic signals that are of constant period and phase, and not coincident in
period and/or phase with stellar activity indices. These signals are thus
strongly suggestive of barycentric reflex motion of the star induced by one or
more candidate exoplanets in Keplerian motion about the host star. Of these
signals, 225 have already been published as planet claims, 60 are classified as
significant unpublished planet candidates that await photometric follow-up to
rule out activity-related causes, and 54 are also unpublished, but are
classified as ""significant"" signals that require confirmation by additional
data before rising to classification as planet candidates. Of particular
interest is our detection of a candidate planet with a minimum mass of 3.9
Earth masses and an orbital period of 9.9 days orbiting Lalande 21185, the
fourth-closest main sequence star to the Sun. For each of our exoplanetary
candidate signals, we provide the period and semi-amplitude of the Keplerian
orbital fit, and a likelihood ratio estimate of its statistical significance.
We also tabulate 18 Keplerian-like signals that we classify as likely arising
from stellar activity.
",0,1,0,0,0,0
2959,Dynamic constraints on activity and connectivity during the learning of value,"  Human learning is a complex process in which future behavior is altered via
the modulation of neural activity. Yet, the degree to which brain activity and
functional connectivity during learning is constrained across subjects, for
example by conserved anatomy and physiology or by the nature of the task,
remains unknown. Here, we measured brain activity and functional connectivity
in a longitudinal experiment in which healthy adult human participants learned
the values of novel objects over the course of four days. We assessed the
presence of constraints on activity and functional connectivity using an
inter-subject correlation approach. Constraints on activity and connectivity
were greater in magnitude than expected in a non-parametric permutation-based
null model, particularly in primary sensory and motor systems, as well as in
regions associated with the learning of value. Notably, inter-subject
connectivity in activity and connectivity displayed marked temporal variations,
with inter-subject correlations in activity exceeding those in connectivity
during early learning and \emph{visa versa} in later learning. Finally,
individual differences in performance accuracy tracked the degree to which a
subject's connectivity, but not activity, tracked subject-general patterns.
Taken together, our results support the notion that brain activity and
connectivity are constrained across subjects in early learning, with
constraints on activity, but not connectivity, decreasing in later learning.
",0,0,0,0,1,0
658,Contextually Customized Video Summaries via Natural Language,"  The best summary of a long video differs among different people due to its
highly subjective nature. Even for the same person, the best summary may change
with time or mood. In this paper, we introduce the task of generating
customized video summaries through simple text. First, we train a deep
architecture to effectively learn semantic embeddings of video frames by
leveraging the abundance of image-caption data via a progressive and residual
manner. Given a user-specific text description, our algorithm is able to select
semantically relevant video segments and produce a temporally aligned video
summary. In order to evaluate our textually customized video summaries, we
conduct experimental comparison with baseline methods that utilize ground-truth
information. Despite the challenging baselines, our method still manages to
show comparable or even exceeding performance. We also show that our method is
able to generate semantically diverse video summaries by only utilizing the
learned visual embeddings.
",1,0,0,0,0,0
312,State Sum Invariants of Three Manifolds from Spherical Multi-fusion Categories,"  We define a family of quantum invariants of closed oriented $3$-manifolds
using spherical multi-fusion categories. The state sum nature of this invariant
leads directly to $(2+1)$-dimensional topological quantum field theories
($\text{TQFT}$s), which generalize the Turaev-Viro-Barrett-Westbury
($\text{TVBW}$) $\text{TQFT}$s from spherical fusion categories. The invariant
is given as a state sum over labeled triangulations, which is mostly parallel
to, but richer than the $\text{TVBW}$ approach in that here the labels live not
only on $1$-simplices but also on $0$-simplices. It is shown that a
multi-fusion category in general cannot be a spherical fusion category in the
usual sense. Thus we introduce the concept of a spherical multi-fusion category
by imposing a weakened version of sphericity. Besides containing the
$\text{TVBW}$ theory, our construction also includes the recent higher gauge
theory $(2+1)$-$\text{TQFT}$s given by Kapustin and Thorngren, which was not
known to have a categorical origin before.
",0,1,1,0,0,0
1793,Quantum Structures in Human Decision-making: Towards Quantum Expected Utility,"  {\it Ellsberg thought experiments} and empirical confirmation of Ellsberg
preferences pose serious challenges to {\it subjective expected utility theory}
(SEUT). We have recently elaborated a quantum-theoretic framework for human
decisions under uncertainty which satisfactorily copes with the Ellsberg
paradox and other puzzles of SEUT. We apply here the quantum-theoretic
framework to the {\it Ellsberg two-urn example}, showing that the paradox can
be explained by assuming a state change of the conceptual entity that is the
object of the decision ({\it decision-making}, or {\it DM}, {\it entity}) and
representing subjective probabilities by quantum probabilities. We also model
the empirical data we collected in a DM test on human participants within the
theoretic framework above. The obtained results are relevant, as they provide a
line to model real life, e.g., financial and medical, decisions that show the
same empirical patterns as the two-urn experiment.
",0,0,0,0,1,1
434,AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles,"  Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.
",1,0,0,0,0,0
505,Checklists to Support Test Charter Design in Exploratory Testing,"  During exploratory testing sessions the tester simultaneously learns, designs
and executes tests. The activity is iterative and utilizes the skills of the
tester and provides flexibility and creativity.Test charters are used as a
vehicle to support the testers during the testing. The aim of this study is to
support practitioners in the design of test charters through checklists. We
aimed to identify factors allowing practitioners to critically reflect on their
designs and contents of test charters to support practitioners in making
informed decisions of what to include in test charters. The factors and
contents have been elicited through interviews. Overall, 30 factors and 35
content elements have been elicited.
",1,0,0,0,0,0
206,Fast Multi-frame Stereo Scene Flow with Motion Segmentation,"  We propose a new multi-frame method for efficiently computing scene flow
(dense depth and optical flow) and camera ego-motion for a dynamic scene
observed from a moving stereo camera rig. Our technique also segments out
moving objects from the rigid scene. In our method, we first estimate the
disparity map and the 6-DOF camera motion using stereo matching and visual
odometry. We then identify regions inconsistent with the estimated camera
motion and compute per-pixel optical flow only at these regions. This flow
proposal is fused with the camera motion-based flow proposal using fusion moves
to obtain the final optical flow and motion segmentation. This unified
framework benefits all four tasks - stereo, optical flow, visual odometry and
motion segmentation leading to overall higher accuracy and efficiency. Our
method is currently ranked third on the KITTI 2015 scene flow benchmark.
Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3
orders of magnitude faster than the top six methods. We also report a thorough
evaluation on challenging Sintel sequences with fast camera and object motion,
where our method consistently outperforms OSF [Menze and Geiger, 2015], which
is currently ranked second on the KITTI benchmark.
",1,0,0,0,0,0
11810,Academic Engagement and Commercialization in an Institutional Transition Environment: Evidence from Shanghai Maritime University,"  Does academic engagement accelerate or crowd out the commercialization of
university knowledge? Research on this topic seldom considers the impact of the
institutional environment, especially when a formal institution for encouraging
the commercial activities of scholars has not yet been established. This study
investigates this question in the context of China, which is in the
institutional transition stage. Based on a survey of scholars from Shanghai
Maritime University, we demonstrate that academic engagement has a positive
impact on commercialization and that this impact is greater for risk-averse
scholars than for other risk-seeking scholars. Our results suggest that in an
institutional transition environment, the government should consider
encouraging academic engagement to stimulate the commercialization activities
of conservative scholars.
",0,0,0,0,0,1
216,A Closer Look at the Alpha Persei Coronal Conundrum,"  A ROSAT survey of the Alpha Per open cluster in 1993 detected its brightest
star, mid-F supergiant Alpha Persei: the X-ray luminosity and spectral hardness
were similar to coronally active late-type dwarf members. Later, in 2010, a
Hubble Cosmic Origins Spectrograph SNAPshot of Alpha Persei found
far-ultraviolet coronal proxy SiIV unexpectedly weak. This, and a suspicious
offset of the ROSAT source, suggested that a late-type companion might be
responsible for the X-rays. Recently, a multi-faceted program tested that
premise. Groundbased optical coronography, and near-UV imaging with HST Wide
Field Camera 3, searched for any close-in faint candidate coronal objects, but
without success. Then, a Chandra pointing found the X-ray source single and
coincident with the bright star. Significantly, the SiIV emissions of Alpha
Persei, in a deeper FUV spectrum collected by HST COS as part of the joint
program, aligned well with chromospheric atomic oxygen (which must be intrinsic
to the luminous star), within the context of cooler late-F and early-G
supergiants, including Cepheid variables. This pointed to the X-rays as the
fundamental anomaly. The over-luminous X-rays still support the case for a
hyperactive dwarf secondary, albeit now spatially unresolved. However, an
alternative is that Alpha Persei represents a novel class of coronal source.
Resolving the first possibility now has become more difficult, because the easy
solution -- a well separated companion -- has been eliminated. Testing the
other possibility will require a broader high-energy census of the early-F
supergiants.
",0,1,0,0,0,0
635,Magneto-inductive Passive Relaying in Arbitrarily Arranged Networks,"  We consider a wireless sensor network that uses inductive near-field coupling
for wireless powering or communication, or for both. The severely limited range
of an inductively coupled source-destination pair can be improved using
resonant relay devices, which are purely passive in nature. Utilization of such
magneto-inductive relays has only been studied for regular network topologies,
allowing simplified assumptions on the mutual antenna couplings. In this work
we present an analysis of magneto-inductive passive relaying in arbitrarily
arranged networks. We find that the resulting channel has characteristics
similar to multipath fading: the channel power gain is governed by a
non-coherent sum of phasors, resulting in increased frequency selectivity. We
propose and study two strategies to increase the channel power gain of random
relay networks: i) deactivation of individual relays by open-circuit switching
and ii) frequency tuning. The presented results show that both methods improve
the utilization of available passive relays, leading to reliable and
significant performance gains.
",1,0,0,0,0,0
15491,Adapting the CVA model to Leland's framework,"  We consider the framework proposed by Burgard and Kjaer (2011) that derives
the PDE which governs the price of an option including bilateral counterparty
risk and funding. We extend this work by relaxing the assumption of absence of
transaction costs in the hedging portfolio by proposing a cost proportional to
the amount of assets traded and the traded price. After deriving the nonlinear
PDE, we prove the existence of a solution for the corresponding
initial-boundary value problem. Moreover, we develop a numerical scheme that
allows to find the solution of the PDE by setting different values for each
parameter of the model. To understand the impact of each variable within the
model, we analyze the Greeks of the option and the sensitivity of the price to
changes in all the risk factors.
",0,0,0,0,0,1
314,Human perception in computer vision,"  Computer vision has made remarkable progress in recent years. Deep neural
network (DNN) models optimized to identify objects in images exhibit
unprecedented task-trained accuracy and, remarkably, some generalization
ability: new visual problems can now be solved more easily based on previous
learning. Biological vision (learned in life and through evolution) is also
accurate and general-purpose. Is it possible that these different learning
regimes converge to similar problem-dependent optimal computations? We
therefore asked whether the human system-level computation of visual perception
has DNN correlates and considered several anecdotal test cases. We found that
perceptual sensitivity to image changes has DNN mid-computation correlates,
while sensitivity to segmentation, crowding and shape has DNN end-computation
correlates. Our results quantify the applicability of using DNN computation to
estimate perceptual loss, and are consistent with the fascinating theoretical
view that properties of human perception are a consequence of
architecture-independent visual learning.
",1,0,0,0,0,0
662,Inter-Session Modeling for Session-Based Recommendation,"  In recent years, research has been done on applying Recurrent Neural Networks
(RNNs) as recommender systems. Results have been promising, especially in the
session-based setting where RNNs have been shown to outperform state-of-the-art
models. In many of these experiments, the RNN could potentially improve the
recommendations by utilizing information about the user's past sessions, in
addition to its own interactions in the current session. A problem for
session-based recommendation, is how to produce accurate recommendations at the
start of a session, before the system has learned much about the user's current
interests. We propose a novel approach that extends a RNN recommender to be
able to process the user's recent sessions, in order to improve
recommendations. This is done by using a second RNN to learn from recent
sessions, and predict the user's interest in the current session. By feeding
this information to the original RNN, it is able to improve its
recommendations. Our experiments on two different datasets show that the
proposed approach can significantly improve recommendations throughout the
sessions, compared to a single RNN working only on the current session. The
proposed model especially improves recommendations at the start of sessions,
and is therefore able to deal with the cold start problem within sessions.
",1,0,0,0,0,0
594,Stochastic Primal-Dual Method on Riemannian Manifolds with Bounded Sectional Curvature,"  We study a stochastic primal-dual method for constrained optimization over
Riemannian manifolds with bounded sectional curvature. We prove non-asymptotic
convergence to the optimal objective value. More precisely, for the class of
hyperbolic manifolds, we establish a convergence rate that is related to the
sectional curvature lower bound. To prove a convergence rate in terms of
sectional curvature for the elliptic manifolds, we leverage Toponogov's
comparison theorem. In addition, we provide convergence analysis for the
asymptotically elliptic manifolds, where the sectional curvature at each given
point on manifold is locally bounded from below by the distance function. We
demonstrate the performance of the primal-dual algorithm on the sphere for the
non-negative principle component analysis (PCA). In particular, under the
non-negativity constraint on the principle component and for the symmetric
spiked covariance model, we empirically show that the primal-dual approach
outperforms the spectral method. We also examine the performance of the
primal-dual method for the anchored synchronization from partial noisy
measurements of relative rotations on the Lie group SO(3). Lastly, we show that
the primal-dual algorithm can be applied to the weighted MAX-CUT problem under
constraints on the admissible cut. Specifically, we propose different
approximation algorithms for the weighted MAX-CUT problem based on optimizing a
function on the manifold of direct products of the unit spheres as well as the
manifold of direct products of the rotation groups.
",0,0,1,0,0,0
4675,When do we have the power to detect biological interactions in spatial point patterns?,"  Determining the relative importance of environmental factors, biotic
interactions and stochasticity in assembling and maintaining species-rich
communities remains a major challenge in ecology. In plant communities,
interactions between individuals of different species are expected to leave a
spatial signature in the form of positive or negative spatial correlations over
distances relating to the spatial scale of interaction. Most studies using
spatial point process tools have found relatively little evidence for
interactions between pairs of species. More interactions tend to be detected in
communities with fewer species. However, there is currently no understanding of
how the power to detect spatial interactions may change with sample size, or
the scale and intensity of interactions.
We use a simple 2-species model where the scale and intensity of interactions
are controlled to simulate point pattern data. In combination with an
approximation to the variance of the spatial summary statistics that we sample,
we investigate the power of current spatial point pattern methods to correctly
reject the null model of bivariate species independence.
We show that the power to detect interactions is positively related to the
abundances of the species tested, and the intensity and scale of interactions.
Increasing imbalance in abundances has a negative effect on the power to detect
interactions. At population sizes typically found in currently available
datasets for species-rich plant communities we find only a very low power to
detect interactions. Differences in power may explain the increased frequency
of interactions in communities with fewer species. Furthermore, the
community-wide frequency of detected interactions is very sensitive to a
minimum abundance criterion for including species in the analyses.
",0,0,0,0,1,0
5710,Correlating Cellular Features with Gene Expression using CCA,"  To understand the biology of cancer, joint analysis of multiple data
modalities, including imaging and genomics, is crucial. The involved nature of
gene-microenvironment interactions necessitates the use of algorithms which
treat both data types equally. We propose the use of canonical correlation
analysis (CCA) and a sparse variant as a preliminary discovery tool for
identifying connections across modalities, specifically between gene expression
and features describing cell and nucleus shape, texture, and stain intensity in
histopathological images. Applied to 615 breast cancer samples from The Cancer
Genome Atlas, CCA revealed significant correlation of several image features
with expression of PAM50 genes, known to be linked to outcome, while Sparse CCA
revealed associations with enrichment of pathways implicated in cancer without
leveraging prior biological understanding. These findings affirm the utility of
CCA for joint phenotype-genotype analysis of cancer.
",0,0,0,1,1,0
278,Cross-layer optimized routing with low duty cycle TDMA across multiple wireless body area networks,"  In this paper, we study the performance of two cross-layer optimized dynamic
routing techniques for radio interference mitigation across multiple coexisting
wireless body area networks (BANs), based on real-life measurements. At the
network layer, the best route is selected according to channel state
information from the physical layer, associated with low duty cycle TDMA at the
MAC layer. The routing techniques (i.e., shortest path routing (SPR), and novel
cooperative multi-path routing (CMR) incorporating 3-branch selection
combining) perform real-time and reliable data transfer across BANs operating
near the 2.4 GHz ISM band. An open-access experimental data set of 'everyday'
mixed-activities is used for analyzing the proposed cross-layer optimization.
We show that CMR gains up to 14 dB improvement with 8.3% TDMA duty cycle, and
even 10 dB improvement with 0.2% TDMA duty cycle over SPR, at 10% outage
probability at a realistic signal-to-interference-plus-noise ratio (SINR).
Acceptable packet delivery ratios (PDR) and spectral efficiencies are obtained
from SPR and CMR with reasonably sensitive receivers across a range of TDMA low
duty cycles, with up to 9 dB improvement of CMR over SPR at 90% PDR. The
distribution fits for received SINR through routing are also derived and
validated with theoretical analysis.
",1,0,0,0,0,0
340,Typed Closure Conversion for the Calculus of Constructions,"  Dependently typed languages such as Coq are used to specify and verify the
full functional correctness of source programs. Type-preserving compilation can
be used to preserve these specifications and proofs of correctness through
compilation into the generated target-language programs. Unfortunately,
type-preserving compilation of dependent types is hard. In essence, the problem
is that dependent type systems are designed around high-level compositional
abstractions to decide type checking, but compilation interferes with the
type-system rules for reasoning about run-time terms.
We develop a type-preserving closure-conversion translation from the Calculus
of Constructions (CC) with strong dependent pairs ($\Sigma$ types)---a subset
of the core language of Coq---to a type-safe, dependently typed compiler
intermediate language named CC-CC. The central challenge in this work is how to
translate the source type-system rules for reasoning about functions into
target type-system rules for reasoning about closures. To justify these rules,
we prove soundness of CC-CC by giving a model in CC. In addition to type
preservation, we prove correctness of separate compilation.
",1,0,0,0,0,0
764,A supernova at 50 pc: Effects on the Earth's atmosphere and biota,"  Recent 60Fe results have suggested that the estimated distances of supernovae
in the last few million years should be reduced from 100 pc to 50 pc. Two
events or series of events are suggested, one about 2.7 million years to 1.7
million years ago, and another may at 6.5 to 8.7 million years ago. We ask what
effects such supernovae are expected to have on the terrestrial atmosphere and
biota. Assuming that the Local Bubble was formed before the event being
considered, and that the supernova and the Earth were both inside a weak,
disordered magnetic field at that time, TeV-PeV cosmic rays at Earth will
increase by a factor of a few hundred. Tropospheric ionization will increase
proportionately, and the overall muon radiation load on terrestrial organisms
will increase by a factor of 150. All return to pre-burst levels within 10kyr.
In the case of an ordered magnetic field, effects depend strongly on the field
orientation. The upper bound in this case is with a largely coherent field
aligned along the line of sight to the supernova, in which case TeV-PeV cosmic
ray flux increases are 10^4; in the case of a transverse field they are below
current levels. We suggest a substantial increase in the extended effects of
supernovae on Earth and in the lethal distance estimate; more work is
needed.This paper is an explicit followup to Thomas et al. (2016). We also here
provide more detail on the computational procedures used in both works.
",0,1,0,0,0,0
356,A Combinatorial Approach to the Opposite Bi-Free Partial $S$-Transform,"  In this paper, we present a combinatorial approach to the opposite 2-variable
bi-free partial $S$-transforms where the opposite multiplication is used on the
right. In addition, extensions of this partial $S$-transforms to the
conditional bi-free and operator-valued bi-free settings are discussed.
",0,0,1,0,0,0
810,A Coherent vorticity preserving eddy viscosity correction for Large-Eddy Simulation,"  This paper introduces a new approach to Large-Eddy Simulation (LES) where
subgrid-scale (SGS) dissipation is applied proportionally to the degree of
local spectral broadening, hence mitigated or deactivated in regions dominated
by large-scale and/or laminar vortical motion. The proposed Coherent vorticity
preserving (CvP) LES methodology is based on the evaluation of the ratio of the
test-filtered to resolved (or grid-filtered) enstrophy $\sigma$. Values of
$\sigma$ close to 1 indicate low sub-test-filter turbulent activity, justifying
local deactivation of the SGS dissipation. The intensity of the SGS dissipation
is progressively increased for $\sigma < 1$ which corresponds to a small-scale
spectral broadening. The SGS dissipation is then fully activated in developed
turbulence characterized by $\sigma \le \sigma_{eq}$, where the value
$\sigma_{eq}$ is derived assuming a Kolmogorov spectrum. The proposed approach
can be applied to any eddy-viscosity model, is algorithmically simple and
computationally inexpensive. LES of Taylor-Green vortex breakdown demonstrates
that the CvP methodology improves the performance of traditional, non-dynamic
dissipative SGS models, capturing the peak of total turbulent kinetic energy
dissipation during transition. Similar accuracy is obtained by adopting
Germano's dynamic procedure albeit at more than twice the computational
overhead. A CvP-LES of a pair of unstable periodic helical vortices is shown to
predict accurately the experimentally observed growth rate using coarse
resolutions. The ability of the CvP methodology to dynamically sort the
coherent, large-scale motion from the smaller, broadband scales during
transition is demonstrated via flow visualizations. LES of compressible channel
are carried out and show a good match with a reference DNS.
",1,1,0,0,0,0
657,A global model for predicting the arrival of imported dengue infections,"  With approximately half of the world's population at risk of contracting
dengue, this mosquito-borne disease is of global concern. International
travellers significantly contribute to dengue's rapid and large-scale spread by
importing the disease from endemic into non-endemic countries. To prevent
future outbreaks and dengue from establishing in non-endemic countries,
knowledge about the arrival time and location of infected travellers is
crucial. We propose a network model that predicts the monthly number of dengue
infected air passengers arriving at any given airport. We consider
international air travel volumes, monthly dengue incidence rates and temporal
infection dynamics. Our findings shed light onto dengue importation routes and
reveal country-specific reporting rates that have been until now largely
unknown.
",1,0,0,0,1,0
5848,Approximate Analytical Solution of a Cancer Immunotherapy Model by the Application of Differential Transform and Adomian Decomposition Methods,"  Immunotherapy plays a major role in tumour treatment, in comparison with
other methods of dealing with cancer. The Kirschner-Panetta (KP) model of
cancer immunotherapy describes the interaction between tumour cells, effector
cells and interleukin-2 which are clinically utilized as medical treatment. The
model selects a rich concept of immune-tumour dynamics. In this paper,
approximate analytical solutions to KP model are represented by using the
differential transform and Adomian decomposition. The complicated nonlinearity
of the KP system causes the application of these two methods to require more
involved calculations. The approximate analytical solutions to the model are
compared with the results obtained by numerical fourth order Runge-Kutta
method.
",0,0,0,0,1,0
869,Enhanced version of AdaBoostM1 with J48 Tree learning method,"  Machine Learning focuses on the construction and study of systems that can
learn from data. This is connected with the classification problem, which
usually is what Machine Learning algorithms are designed to solve. When a
machine learning method is used by people with no special expertise in machine
learning, it is important that the method be robust in classification, in the
sense that reasonable performance is obtained with minimal tuning of the
problem at hand. Algorithms are evaluated based on how robust they can classify
the given data. In this paper, we propose a quantifiable measure of robustness,
and describe a particular learning method that is robust according to this
measure in the context of classification problem. We proposed Adaptive Boosting
(AdaBoostM1) with J48(C4.5 tree) as a base learner with tuning weight threshold
(P) and number of iterations (I) for boosting algorithm. To benchmark the
performance, we used the baseline classifier, AdaBoostM1 with Decision Stump as
base learner without tuning parameters. By tuning parameters and using J48 as
base learner, we are able to reduce the overall average error rate ratio
(errorC/errorNB) from 2.4 to 0.9 for development sets of data and 2.1 to 1.2
for evaluation sets of data.
",0,0,0,1,0,0
3421,A closed formula for illiquid corporate bonds and an application to the European market,"  We deduce a simple closed formula for illiquid corporate coupon bond prices
when liquid bonds with similar characteristics (e.g. maturity) are present in
the market for the same issuer. The key model parameter is the
time-to-liquidate a position, i.e. the time that an experienced bond trader
takes to liquidate a given position on a corporate coupon bond.
The option approach we propose for pricing bonds' illiquidity is reminiscent
of the celebrated work of Longstaff (1995) on the non-marketability of some
non-dividend-paying shares in IPOs. This approach describes a quite common
situation in the fixed income market: it is rather usual to find issuers that,
besides liquid benchmark bonds, issue some other bonds that either are placed
to a small number of investors in private placements or have a limited issue
size.
The model considers interest rate and credit spread term structures and their
dynamics. We show that illiquid bonds present an additional liquidity spread
that depends on the time-to-liquidate aside from credit and interest rate
parameters. We provide a detailed application for two issuers in the European
market.
",0,0,0,0,0,1
737,Stability of the Poincaré bundle,"  Let X be an irreducible smooth projective curve, of genus at least two, over
an algebraically closed field k. Let $\mathcal{M}^d_G$ denote the moduli stack
of principal G-bundles over X of fixed topological type $d \in \pi_1(G)$, where
G is any almost simple affine algebraic group over k. We prove that the
universal bundle over $X \times \mathcal{M}^d_G$ is stable with respect to any
polarization on $X \times \mathcal{M}^d_G$. A similar result is proved for the
Poincaré adjoint bundle over $X \times M_G^{d, rs}$, where $M_G^{d, rs}$ is
the coarse moduli space of regularly stable principal G-bundles over X of fixed
topological type d.
",0,0,1,0,0,0
8204,On the optimal investment-consumption and life insurance selection problem with an external stochastic factor,"  In this paper, we study a stochastic optimal control problem with stochastic
volatility. We prove the sufficient and necessary maximum principle for the
proposed problem. Then we apply the results to solve an investment, consumption
and life insurance problem with stochastic volatility, that is, we consider a
wage earner investing in one risk-free asset and one risky asset described by a
jump-diffusion process and has to decide concerning consumption and life
insurance purchase. We assume that the life insurance for the wage earner is
bought from a market composed of $M>1$ life insurance companies offering
pairwise distinct life insurance contracts. The goal is to maximize the
expected utilities derived from the consumption, the legacy in the case of a
premature death and the investor's terminal wealth.
",0,0,0,0,0,1
578,On M-functions associated with modular forms,"  Let $f$ be a primitive cusp form of weight $k$ and level $N,$ let $\chi$ be a
Dirichlet character of conductor coprime with $N,$ and let
$\mathfrak{L}(f\otimes \chi, s)$ denote either $\log L(f\otimes \chi, s)$ or
$(L'/L)(f\otimes \chi, s).$ In this article we study the distribution of the
values of $\mathfrak{L}$ when either $\chi$ or $f$ vary. First, for a
quasi-character $\psi\colon \mathbb{C} \to \mathbb{C}^\times$ we find the limit
for the average $\mathrm{Avg}\_\chi \psi(L(f\otimes\chi, s)),$ when $f$ is
fixed and $\chi$ varies through the set of characters with prime conductor that
tends to infinity. Second, we prove an equidistribution result for the values
of $\mathfrak{L}(f\otimes \chi,s)$ by establishing analytic properties of the
above limit function. Third, we study the limit of the harmonic average
$\mathrm{Avg}^h\_f \psi(L(f, s)),$ when $f$ runs through the set of primitive
cusp forms of given weight $k$ and level $N\to \infty.$ Most of the results are
obtained conditionally on the Generalized Riemann Hypothesis for
$L(f\otimes\chi, s).$
",0,0,1,0,0,0
303,Framing U-Net via Deep Convolutional Framelets: Application to Sparse-view CT,"  X-ray computed tomography (CT) using sparse projection views is a recent
approach to reduce the radiation dose. However, due to the insufficient
projection views, an analytic reconstruction approach using the filtered back
projection (FBP) produces severe streaking artifacts. Recently, deep learning
approaches using large receptive field neural networks such as U-Net have
demonstrated impressive performance for sparse- view CT reconstruction.
However, theoretical justification is still lacking. Inspired by the recent
theory of deep convolutional framelets, the main goal of this paper is,
therefore, to reveal the limitation of U-Net and propose new multi-resolution
deep learning schemes. In particular, we show that the alternative U- Net
variants such as dual frame and the tight frame U-Nets satisfy the so-called
frame condition which make them better for effective recovery of high frequency
edges in sparse view- CT. Using extensive experiments with real patient data
set, we demonstrate that the new network architectures provide better
reconstruction performance.
",1,0,0,1,0,0
672,Stability of Valuations: Higher Rational Rank,"  Given a klt singularity $x\in (X, D)$, we show that a quasi-monomial
valuation $v$ with a finitely generated associated graded ring is the minimizer
of the normalized volume function $\widehat{\rm vol}_{(X,D),x}$, if and only if
$v$ induces a degeneration to a K-semistable log Fano cone singularity.
Moreover, such a minimizer is unique among all quasi-monomial valuations up to
rescaling. As a consequence, we prove that for a klt singularity $x\in X$ on
the Gromov-Hausdorff limit of Kähler-Einstein Fano manifolds, the
intermediate K-semistable cone associated to its metric tangent cone is
uniquely determined by the algebraic structure of $x\in X$, hence confirming a
conjecture by Donaldson-Sun.
",0,0,1,0,0,0
344,Characterizing Exoplanet Habitability,"  A habitable exoplanet is a world that can maintain stable liquid water on its
surface. Techniques and approaches to characterizing such worlds are essential,
as performing a census of Earth-like planets that may or may not have life will
inform our understanding of how frequently life originates and is sustained on
worlds other than our own. Observational techniques like high contrast imaging
and transit spectroscopy can reveal key indicators of habitability for
exoplanets. Both polarization measurements and specular reflectance from oceans
(also known as ""glint"") can provide direct evidence for surface liquid water,
while constraining surface pressure and temperature (from moderate resolution
spectra) can indicate liquid water stability. Indirect evidence for
habitability can come from a variety of sources, including observations of
variability due to weather, surface mapping studies, and/or measurements of
water vapor or cloud profiles that indicate condensation near a surface.
Approaches to making the types of measurements that indicate habitability are
diverse, and have different considerations for the required wavelength range,
spectral resolution, maximum noise levels, stellar host temperature, and
observing geometry.
",0,1,0,0,0,0
482,Sequential testing for structural stability in approximate factor models,"  We develop an on-line monitoring procedure to detect a change in a large
approximate factor model. Our statistics are based on a well-known property of
the $% \left( r+1\right) $-th eigenvalue of the sample covariance matrix of the
data (having defined $r$ as the number of common factors): whilst under the
null the $\left( r+1\right) $-th eigenvalue is bounded, under the alternative
of a change (either in the loadings, or in the number of factors itself) it
becomes spiked. Given that the sample eigenvalue cannot be estimated
consistently under the null, we regularise the problem by randomising the test
statistic in conjunction with sample conditioning, obtaining a sequence of
\textit{i.i.d.}, asymptotically chi-square statistics which are then employed
to build the monitoring scheme. Numerical evidence shows that our procedure
works very well in finite samples, with a very small probability of false
detections and tight detection times in presence of a genuine change-point.
",0,0,0,1,0,0
675,High Dimensional Robust Estimation of Sparse Models via Trimmed Hard Thresholding,"  We study the problem of sparsity constrained $M$-estimation with arbitrary
corruptions to both {\em explanatory and response} variables in the
high-dimensional regime, where the number of variables $d$ is larger than the
sample size $n$. Our main contribution is a highly efficient gradient-based
optimization algorithm that we call Trimmed Hard Thresholding -- a robust
variant of Iterative Hard Thresholding (IHT) by using trimmed mean in gradient
computations. Our algorithm can deal with a wide class of sparsity constrained
$M$-estimation problems, and we can tolerate a nearly dimension independent
fraction of arbitrarily corrupted samples. More specifically, when the
corrupted fraction satisfies $\epsilon \lesssim {1} /\left({\sqrt{k} \log
(nd)}\right)$, where $k$ is the sparsity of the parameter, we obtain accurate
estimation and model selection guarantees with optimal sample complexity.
Furthermore, we extend our algorithm to sparse Gaussian graphical model
(precision matrix) estimation via a neighborhood selection approach. We
demonstrate the effectiveness of robust estimation in sparse linear, logistic
regression, and sparse precision matrix estimation on synthetic and real-world
US equities data.
",1,0,1,1,0,0
7137,Optimal investment-consumption problem post-retirement with a minimum guarantee,"  We study the optimal investment-consumption problem for a member of defined
contribution plan during the decumulation phase. For a fixed annuitization
time, to achieve higher final annuity, we consider a variable consumption rate.
Moreover, to eliminate the ruin possibilities and having a minimum guarantee
for the final annuity, we consider a safety level for the wealth process which
consequently yields a Hamilton-Jacobi-Bellman (HJB) equation on a bounded
domain. We apply the policy iteration method to find approximations of solution
of the HJB equation. Finally, we give the simulation results for the optimal
investment-consumption strategies, optimal wealth process and the final annuity
for different ranges of admissible consumptions. Furthermore, by calculating
the present market value of the future cash flows before and after the
annuitization, we compare the results for different consumption policies.
",0,0,0,0,0,1
329,Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?,"  Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors.
",1,0,0,0,0,0
6585,From jamming to collective cell migration through a boundary induced transition,"  Cell monolayers provide an interesting example of active matter, exhibiting a
phase transition from a flowing to jammed state as they age. Here we report
experiments and numerical simulations illustrating how a jammed cellular layer
rapidly reverts to a flowing state after a wound. Quantitative comparison
between experiments and simulations shows that cells change their
self-propulsion and alignement strength so that the system crosses a phase
transition line, which we characterize by finite-size scaling in an active
particle model. This wound-induced unjamming transition is found to occur
generically in epithelial, endothelial and cancer cells.
",0,0,0,0,1,0
911,CMB anisotropies at all orders: the non-linear Sachs-Wolfe formula,"  We obtain the non-linear generalization of the Sachs-Wolfe + integrated
Sachs-Wolfe (ISW) formula describing the CMB temperature anisotropies. Our
formula is valid at all orders in perturbation theory, is also valid in all
gauges and includes scalar, vector and tensor modes. A direct consequence of
our results is that the maps of the logarithmic temperature anisotropies are
much cleaner than the usual CMB maps, because they automatically remove many
secondary anisotropies. This can for instance, facilitate the search for
primordial non-Gaussianity in future works. It also disentangles the non-linear
ISW from other effects. Finally, we provide a method which can iteratively be
used to obtain the lensing solution at the desired order.
",0,1,0,0,0,0
3908,The Moon Illusion explained by the Projective Consciousness Model,"  The Moon often appears larger near the perceptual horizon and smaller high in
the sky though the visual angle subtended is invariant. We show how this
illusion results from the optimization of a projective geometrical frame for
conscious perception through free energy minimization, as articulated in the
Projective Consciousness Model. The model accounts for all documented
modulations of the illusion without anomalies (e.g., the size-distance
paradox), surpasses other theories in explanatory power, makes sense of inter-
and intra-subjective variability vis-a-vis the illusion, and yields new
quantitative and qualitative predictions.
",0,0,0,0,1,0
388,Belyi map for the sporadic group J1,"  We compute the genus 0 Belyi map for the sporadic Janko group J1 of degree
266 and describe the applied method. This yields explicit polynomials having J1
as a Galois group over K(t), [K:Q] = 7.
",0,0,1,0,0,0
4718,CLEAR: Coverage-based Limiting-cell Experiment Analysis for RNA-seq,"  Direct cDNA preamplification protocols developed for single-cell RNA-seq
(scRNA-seq) have enabled transcriptome profiling of rare cells without having
to pool multiple samples or to perform RNA extraction. We term this approach
limiting-cell RNA-seq (lcRNA-seq). Unlike scRNA-seq, which focuses on
'cell-atlasing', lcRNA-seq focuses on identifying differentially expressed
genes (DEGs) between experimental groups. This requires accounting for systems
noise which can obscure biological differences. We present CLEAR, a workflow
that identifies robust transcripts in lcRNA-seq data for between-group
comparisons. To develop CLEAR, we compared DEGs from RNA extracted from
FACS-derived CD5+ and CD5- cells from a single chronic lymphocytic leukemia
patient diluted to input RNA levels of 10-, 100- and 1,000pg. Data quality at
ultralow input levels are known to be noisy. When using CLEAR transcripts vs.
using all available transcripts, downstream analyses reveal more shared DEGs,
improved Principal Component Analysis separation of cell type, and increased
similarity between results across different input RNA amounts. CLEAR was
applied to two publicly available ultralow input RNA-seq data and an in-house
murine neural cell lcRNA-seq dataset. CLEAR provides a novel way to visualize
the public datasets while validates cell phenotype markers for astrocytes,
neural stem and progenitor cells.
",0,0,0,0,1,0
623,Failure of Smooth Pasting Principle and Nonexistence of Equilibrium Stopping Rules under Time-Inconsistency,"  This paper considers a time-inconsistent stopping problem in which the
inconsistency arises from non-constant time preference rates. We show that the
smooth pasting principle, the main approach that has been used to construct
explicit solutions for conventional time-consistent optimal stopping problems,
may fail under time-inconsistency. Specifically, we prove that the smooth
pasting principle solves a time-inconsistent problem within the intra-personal
game theoretic framework if and only if a certain inequality on the model
primitives is satisfied. We show that the violation of this inequality can
happen even for very simple non-exponential discount functions. Moreover, we
demonstrate that the stopping problem does not admit any intra-personal
equilibrium whenever the smooth pasting principle fails. The ""negative"" results
in this paper caution blindly extending the classical approaches for
time-consistent stopping problems to their time-inconsistent counterparts.
",0,0,0,0,0,1
461,Kinetic modelling of competition and depletion of shared miRNAs by competing endogenous RNAs,"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
",0,0,0,0,1,0
449,Experimental Evidence on a Refined Conjecture of the BSD type,"  Let $E/\mathbb{Q}$ be an elliptic curve of level $N$ and rank equal to $1$.
Let $p$ be a prime of ordinary reduction. We experimentally study conjecture
$4$ of B. Mazur and J. Tate in his article ""Refined Conjectures of the Birch
and Swinnerton-Dyer Type"". We report the computational evidence.
",0,0,1,0,0,0
604,Asymptotics of ABC,"  We present an informal review of recent work on the asymptotics of
Approximate Bayesian Computation (ABC). In particular we focus on how does the
ABC posterior, or point estimates obtained by ABC, behave in the limit as we
have more data? The results we review show that ABC can perform well in terms
of point estimation, but standard implementations will over-estimate the
uncertainty about the parameters. If we use the regression correction of
Beaumont et al. then ABC can also accurately quantify this uncertainty. The
theoretical results also have practical implications for how to implement ABC.
",0,0,1,1,0,0
568,An Extended Low Fat Allocator API and Applications,"  The primary function of memory allocators is to allocate and deallocate
chunks of memory primarily through the malloc API. Many memory allocators also
implement other API extensions, such as deriving the size of an allocated
object from the object's pointer, or calculating the base address of an
allocation from an interior pointer. In this paper, we propose a general
purpose extended allocator API built around these common extensions. We argue
that such extended APIs have many applications and demonstrate several use
cases, such as (manual) memory error detection, meta data storage, typed
pointers and compact data-structures. Because most existing allocators were not
designed for the extended API, traditional implementations are expensive or not
possible.
Recently, the LowFat allocator for heap and stack objects has been developed.
The LowFat allocator is an implementation of the idea of low-fat pointers,
where object bounds information (size and base) are encoded into the native
machine pointer representation itself. The ""killer app"" for low-fat pointers is
automated bounds check instrumentation for program hardening and bug detection.
However, the LowFat allocator can also be used to implement highly optimized
version of the extended allocator API, which makes the new applications (listed
above) possible. In this paper, we implement and evaluate several applications
based efficient memory allocator API extensions using low-fat pointers. We also
extend the LowFat allocator to cover global objects for the first time.
",1,0,0,0,0,0
6159,Modelling thermo-electro-mechanical effects in orthotropic cardiac tissue,"  In this paper we introduce a new mathematical model for the active
contraction of cardiac muscle, featuring different thermo-electric and
nonlinear conductivity properties. The passive hyperelastic response of the
tissue is described by an orthotropic exponential model, whereas the ionic
activity dictates active contraction incorporated through the concept of
orthotropic active strain. We use a fully incompressible formulation, and the
generated strain modifies directly the conductivity mechanisms in the medium
through the pull-back transformation. We also investigate the influence of
thermo-electric effects in the onset of multiphysics emergent spatiotemporal
dynamics, using nonlinear diffusion. It turns out that these ingredients have a
key role in reproducing pathological chaotic dynamics such as ventricular
fibrillation during inflammatory events, for instance. The specific structure
of the governing equations suggests to cast the problem in mixed-primal form
and we write it in terms of Kirchhoff stress, displacements, solid pressure,
electric potential, activation generation, and ionic variables. We also propose
a new mixed-primal finite element method for its numerical approximation, and
we use it to explore the properties of the model and to assess the importance
of coupling terms, by means of a few computational experiments in 3D.
",0,0,0,0,1,0
370,When the Universe Expands Too Fast: Relentless Dark Matter,"  We consider a modification to the standard cosmological history consisting of
introducing a new species $\phi$ whose energy density red-shifts with the scale
factor $a$ like $\rho_\phi \propto a^{-(4+n)}$. For $n>0$, such a red-shift is
faster than radiation, hence the new species dominates the energy budget of the
universe at early times while it is completely negligible at late times. If
equality with the radiation energy density is achieved at low enough
temperatures, dark matter can be produced as a thermal relic during the new
cosmological phase. Dark matter freeze-out then occurs at higher temperatures
compared to the standard case, implying that reproducing the observed abundance
requires significantly larger annihilation rates. Here, we point out a
completely new phenomenon, which we refer to as $\textit{relentless}$ dark
matter: for large enough $n$, unlike the standard case where annihilation ends
shortly after the departure from thermal equilibrium, dark matter particles
keep annihilating long after leaving chemical equilibrium, with a significant
depletion of the final relic abundance. Relentless annihilation occurs for $n
\geq 2$ and $n \geq 4$ for s-wave and p-wave annihilation, respectively, and it
thus occurs in well motivated scenarios such as a quintessence with a kination
phase. We discuss a few microscopic realizations for the new cosmological
component and highlight the phenomenological consequences of our calculations
for dark matter searches.
",0,1,0,0,0,0
844,Stochastic and Chance-Constrained Conic Distribution System Expansion Planning Using Bilinear Benders Decomposition,"  Second order conic programming (SOCP) has been used to model various
applications in power systems, such as operation and expansion planning. In
this paper, we present a two-stage stochastic mixed integer SOCP (MISOCP) model
for the distribution system expansion planning problem that considers
uncertainty and also captures the nonlinear AC power flow. To avoid costly
investment plans due to some extreme scenarios, we further present a
chance-constrained variant that could lead to cost-effective solutions. To
address the computational challenge, we extend the basic Benders decomposition
method and develop a bilinear variant to compute stochastic and
chance-constrained MISOCP formulations. A set of numerical experiments is
performed to illustrate the performance of our models and computational
methods. In particular, results show that our Benders decomposition algorithms
drastically outperform a professional MISOCP solver in handling stochastic
scenarios by orders of magnitude.
",0,0,1,0,0,0
16684,Robust XVA,"  We introduce an arbitrage-free framework for robust valuation adjustments. An
investor trades a credit default swap portfolio with a risky counterparty, and
hedges credit risk by taking a position in the counterparty bond. The investor
does not know the expected rate of return of the counterparty bond, but he is
confident that it lies within an uncertainty interval. We derive both upper and
lower bounds for the XVA process of the portfolio, and show that these bounds
may be recovered as solutions of nonlinear ordinary differential equations. The
presence of collateralization and closeout payoffs leads to fundamental
differences with respect to classical credit risk valuation. The value of the
super-replicating portfolio cannot be directly obtained by plugging one of the
extremes of the uncertainty interval in the valuation equation, but rather
depends on the relation between the XVA replicating portfolio and the close-out
value throughout the life of the transaction.
",0,0,0,0,0,1
2631,Computational and informatics advances for reproducible data analysis in neuroimaging,"  The reproducibility of scientific research has become a point of critical
concern. We argue that openness and transparency are critical for
reproducibility, and we outline an ecosystem for open and transparent science
that has emerged within the human neuroimaging community. We discuss the range
of open data sharing resources that have been developed for neuroimaging data,
and the role of data standards (particularly the Brain Imaging Data Structure)
in enabling the automated sharing, processing, and reuse of large neuroimaging
datasets. We outline how the open-source Python language has provided the basis
for a data science platform that enables reproducible data analysis and
visualization. We also discuss how new advances in software engineering, such
as containerization, provide the basis for greater reproducibility in data
analysis. The emergence of this new ecosystem provides an example for many
areas of science that are currently struggling with reproducibility.
",0,0,0,0,1,0
513,Handling state space explosion in verification of component-based systems: A review,"  Component-based design is a different way of constructing systems which
offers numerous benefits, in particular, decreasing the complexity of system
design. However, deploying components into a system is a challenging and
error-prone task. Model checking is one of the reliable methods that
automatically and systematically analyse the correctness of a given system. Its
brute-force check of the state space significantly expands the level of
confidence in the system. Nevertheless, model checking is limited by a critical
problem so-called State Space Explosion (SSE). To benefit from model checking,
appropriate methods to reduce SSE, is required. In two last decades, a great
number of methods to mitigate the state space explosion have been proposed
which have many similarities, dissimilarities, and unclear concepts in some
cases. This research, firstly, aims at present a review and brief discussion of
the methods of handling SSE problem and classify them based on their
similarities, principle and characteristics. Second, it investigates the
methods for handling SSE problem in verifying Component-based system (CBS) and
provides insight into CBS verification limitations that have not been addressed
yet. The analysis in this research has revealed the patterns, specific
features, and gaps in the state-of-the-art methods. In addition, we identified
and discussed suitable methods to soften SSE problem in CBS and underlined the
key challenges for future research efforts.
",1,0,1,0,0,0
282,Cyclotron resonant scattering feature simulations. II. Description of the CRSF simulation process,"  Cyclotron resonant scattering features (CRSFs) are formed by scattering of
X-ray photons off quantized plasma electrons in the strong magnetic field (of
the order 10^12 G) close to the surface of an accreting X-ray pulsar. The line
profiles of CRSFs cannot be described by an analytic expression. Numerical
methods such as Monte Carlo (MC) simulations of the scattering processes are
required in order to predict precise line shapes for a given physical setup,
which can be compared to observations to gain information about the underlying
physics in these systems.
A versatile simulation code is needed for the generation of synthetic
cyclotron lines. Sophisticated geometries should be investigatable by making
their simulation possible for the first time.
The simulation utilizes the mean free path tables described in the first
paper of this series for the fast interpolation of propagation lengths. The
code is parallelized to make the very time consuming simulations possible on
convenient time scales. Furthermore, it can generate responses to
mono-energetic photon injections, producing Green's functions, which can be
used later to generate spectra for arbitrary continua.
We develop a new simulation code to generate synthetic cyclotron lines for
complex scenarios, allowing for unprecedented physical interpretation of the
observed data. An associated XSPEC model implementation is used to fit
synthetic line profiles to NuSTAR data of Cep X-4. The code has been developed
with the main goal of overcoming previous geometrical constraints in MC
simulations of CRSFs. By applying this code also to more simple, classic
geometries used in previous works, we furthermore address issues of code
verification and cross-comparison of various models. The XSPEC model and the
Green's function tables are available online at
this http URL .
",0,1,0,0,0,0
307,On the economics of knowledge creation and sharing,"  This work bridges the technical concepts underlying distributed computing and
blockchain technologies with their profound socioeconomic and sociopolitical
implications, particularly on academic research and the healthcare industry.
Several examples from academia, industry, and healthcare are explored
throughout this paper. The limiting factor in contemporary life sciences
research is often funding: for example, to purchase expensive laboratory
equipment and materials, to hire skilled researchers and technicians, and to
acquire and disseminate data through established academic channels. In the case
of the U.S. healthcare system, hospitals generate massive amounts of data, only
a small minority of which is utilized to inform current and future medical
practice. Similarly, corporations too expend large amounts of money to collect,
secure and transmit data from one centralized source to another. In all three
scenarios, data moves under the traditional paradigm of centralization, in
which data is hosted and curated by individuals and organizations and of
benefit to only a small subset of people.
",1,0,0,0,0,0
950,Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try After Deep Learning,"  We found an easy and quick post-learning method named ""Icing on the Cake"" to
enhance a classification performance in deep learning. The method is that we
train only the final classifier again after an ordinary training is done.
",0,0,0,1,0,0
864,Data-Mining Textual Responses to Uncover Misconception Patterns,"  An important, yet largely unstudied, problem in student data analysis is to
detect misconceptions from students' responses to open-response questions.
Misconception detection enables instructors to deliver more targeted feedback
on the misconceptions exhibited by many students in their class, thus improving
the quality of instruction. In this paper, we propose a new natural language
processing-based framework to detect the common misconceptions among students'
textual responses to short-answer questions. We propose a probabilistic model
for students' textual responses involving misconceptions and experimentally
validate it on a real-world student-response dataset. Experimental results show
that our proposed framework excels at classifying whether a response exhibits
one or more misconceptions. More importantly, it can also automatically detect
the common misconceptions exhibited across responses from multiple students to
multiple questions; this property is especially important at large scale, since
instructors will no longer need to manually specify all possible misconceptions
that students might exhibit.
",1,0,0,1,0,0
1332,BOLD5000: A public fMRI dataset of 5000 images,"  Vision science, particularly machine vision, has been revolutionized by
introducing large-scale image datasets and statistical learning approaches.
Yet, human neuroimaging studies of visual perception still rely on small
numbers of images (around 100) due to time-constrained experimental procedures.
To apply statistical learning approaches that integrate neuroscience, the
number of images used in neuroimaging must be significantly increased. We
present BOLD5000, a human functional MRI (fMRI) study that includes almost
5,000 distinct images depicting real-world scenes. Beyond dramatically
increasing image dataset size relative to prior fMRI studies, BOLD5000 also
accounts for image diversity, overlapping with standard computer vision
datasets by incorporating images from the Scene UNderstanding (SUN), Common
Objects in Context (COCO), and ImageNet datasets. The scale and diversity of
these image datasets, combined with a slow event-related fMRI design, enable
fine-grained exploration into the neural representation of a wide range of
visual features, categories, and semantics. Concurrently, BOLD5000 brings us
closer to realizing Marr's dream of a singular vision science - the intertwined
study of biological and computer vision.
",0,0,0,0,1,0
626,Core2Vec: A core-preserving feature learning framework for networks,"  Recent advances in the field of network representation learning are mostly
attributed to the application of the skip-gram model in the context of graphs.
State-of-the-art analogues of skip-gram model in graphs define a notion of
neighbourhood and aim to find the vector representation for a node, which
maximizes the likelihood of preserving this neighborhood.
In this paper, we take a drastic departure from the existing notion of
neighbourhood of a node by utilizing the idea of coreness. More specifically,
we utilize the well-established idea that nodes with similar core numbers play
equivalent roles in the network and hence induce a novel and an organic notion
of neighbourhood. Based on this idea, we propose core2vec, a new algorithmic
framework for learning low dimensional continuous feature mapping for a node.
Consequently, the nodes having similar core numbers are relatively closer in
the vector space that we learn.
We further demonstrate the effectiveness of core2vec by comparing word
similarity scores obtained by our method where the node representations are
drawn from standard word association graphs against scores computed by other
state-of-the-art network representation techniques like node2vec, DeepWalk and
LINE. Our results always outperform these existing methods
",1,0,0,0,0,0
819,On the K-theory of C*-algebras for substitution tilings (a pedestrian version),"  Under suitable conditions, a substitution tiling gives rise to a Smale space,
from which three equivalence relations can be constructed, namely the stable,
unstable, and asymptotic equivalence relations. We denote with $S$, $U$, and
$A$ their corresponding $C^*$-algebras in the sense of Renault. In this article
we show that the $K$-theories of $S$ and $U$ can be computed from the
cohomology and homology of a single cochain complex with connecting maps for
tilings of the line and of the plane. Moreover, we provide formulas to compute
the $K$-theory for these three $C^*$-algebras. Furthermore, we show that the
$K$-theory groups for tilings of dimension 1 are always torsion free. For
tilings of dimension 2, only $K_0(U)$ and $K_1(S)$ can contain torsion.
",0,0,1,0,0,0
6374,WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models,"  Learning sparse linear models with two-way interactions is desirable in many
application domains such as genomics. l1-regularised linear models are popular
to estimate sparse models, yet standard implementations fail to address
specifically the quadratic explosion of candidate two-way interactions in high
dimensions, and typically do not scale to genetic data with hundreds of
thousands of features. Here we present WHInter, a working set algorithm to
solve large l1-regularised problems with two-way interactions for binary design
matrices. The novelty of WHInter stems from a new bound to efficiently identify
working sets while avoiding to scan all features, and on fast computations
inspired from solutions to the maximum inner product search problem. We apply
WHInter to simulated and real genetic data and show that it is more scalable
and two orders of magnitude faster than the state of the art.
",0,0,0,1,1,0
3758,A six-factor asset pricing model,"  The present study introduce the human capital component to the Fama and
French five-factor model proposing an equilibrium six-factor asset pricing
model. The study employs an aggregate of four sets of portfolios mimicking size
and industry with varying dimensions. The first set consists of three set of
six portfolios each sorted on size to B/M, size to investment, and size to
momentum. The second set comprises of five index portfolios, third, a four-set
of twenty-five portfolios each sorted on size to B/M, size to investment, size
to profitability, and size to momentum, and the final set constitute thirty
industry portfolios. To estimate the parameters of six-factor asset pricing
model for the four sets of variant portfolios, we use OLS and Generalized
method of moments based robust instrumental variables technique (IVGMM). The
results obtained from the relevance, endogeneity, overidentifying restrictions,
and the Hausman's specification, tests indicate that the parameter estimates of
the six-factor model using IVGMM are robust and performs better than the OLS
approach. The human capital component shares equally the predictive power
alongside the factors in the framework in explaining the variations in return
on portfolios. Furthermore, we assess the t-ratio of the human capital
component of each IVGMM estimates of the six-factor asset pricing model for the
four sets of variant portfolios. The t-ratio of the human capital of the
eighty-three IVGMM estimates are more than 3.00 with reference to the standard
proposed by Harvey et al. (2016). This indicates the empirical success of the
six-factor asset-pricing model in explaining the variation in asset returns.
",0,0,0,0,0,1
628,Randomly coloring simple hypergraphs with fewer colors,"  We study the problem of constructing a (near) uniform random proper
$q$-coloring of a simple $k$-uniform hypergraph with $n$ vertices and maximum
degree $\Delta$. (Proper in that no edge is mono-colored and simple in that two
edges have maximum intersection of size one). We show that if $q\geq
\max\{C_k\log n,500k^3\Delta^{1/(k-1)}\}$ then the Glauber Dynamics will become
close to uniform in $O(n\log n)$ time, given a random (improper) start. This
improves on the results in Frieze and Melsted [5].
",1,0,0,0,0,0
8674,Towards equation of state for a market: A thermodynamical paradigm of economics,"  Foundations of equilibrium thermodynamics are the equation of state (EoS) and
four postulated laws of thermodynamics. We use equilibrium thermodynamics
paradigms in constructing the EoS for microeconomics system that is a market.
This speculation is hoped to be first step towards whole pictures of
thermodynamical paradigm of economics.
",0,0,0,0,0,1
3464,Species tree inference from genomic sequences using the log-det distance,"  The log-det distance between two aligned DNA sequences was introduced as a
tool for statistically consistent inference of a gene tree under simple
non-mixture models of sequence evolution. Here we prove that the log-det
distance, coupled with a distance-based tree construction method, also permits
consistent inference of species trees under mixture models appropriate to
aligned genomic-scale sequences data. Data may include sites from many genetic
loci, which evolved on different gene trees due to incomplete lineage sorting
on an ultrametric species tree, with different time-reversible substitution
processes. The simplicity and speed of distance-based inference suggests
log-det based methods should serve as benchmarks for judging more elaborate and
computationally-intensive species trees inference methods.
",0,0,0,0,1,0
495,Software correlator for Radioastron mission,"  In this paper we discuss the characteristics and operation of Astro Space
Center (ASC) software FX correlator that is an important component of
space-ground interferometer for Radioastron project. This project performs
joint observations of compact radio sources using 10 meter space radio
telescope (SRT) together with ground radio telescopes at 92, 18, 6 and 1.3 cm
wavelengths. In this paper we describe the main features of space-ground VLBI
data processing of Radioastron project using ASC correlator. Quality of
implemented fringe search procedure provides positive results without
significant losses in correlated amplitude. ASC Correlator has a computational
power close to real time operation. The correlator has a number of processing
modes: ""Continuum"", ""Spectral Line"", ""Pulsars"", ""Giant Pulses"",""Coherent"".
Special attention is paid to peculiarities of Radioastron space-ground VLBI
data processing. The algorithms of time delay and delay rate calculation are
also discussed, which is a matter of principle for data correlation of
space-ground interferometers. During 5 years of Radioastron space radio
telescope (SRT) successful operation, ASC correlator showed high potential of
satisfying steady growing needs of current and future ground and space VLBI
science. Results of ASC software correlator operation are demonstrated.
",0,1,0,0,0,0
3111,Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise,"  We introduce coroICA, confounding-robust independent component analysis, a
novel ICA algorithm which decomposes linearly mixed multivariate observations
into independent components that are corrupted (and rendered dependent) by
hidden group-wise stationary confounding. It extends the ordinary ICA model in
a theoretically sound and explicit way to incorporate group-wise (or
environment-wise) confounding. We show that our general noise model allows to
perform ICA in settings where other noisy ICA procedures fail. Additionally, it
can be used for applications with grouped data by adjusting for different
stationary noise within each group. We show that the noise model has a natural
relation to causality and explain how it can be applied in the context of
causal inference. In addition to our theoretical framework, we provide an
efficient estimation procedure and prove identifiability of the unmixing matrix
under mild assumptions. Finally, we illustrate the performance and robustness
of our method on simulated data, provide audible and visual examples, and
demonstrate the applicability to real-world scenarios by experiments on
publicly available Antarctic ice core data as well as two EEG data sets. We
provide a scikit-learn compatible pip-installable Python package coroICA as
well as R and Matlab implementations accompanied by a documentation at
this https URL.
",0,0,0,1,1,0
1587,International crop trade networks: The impact of shocks and cascades,"  Analyzing available FAO data from 176 countries over 21 years, we observe an
increase of complexity in the international trade of maize, rice, soy, and
wheat. A larger number of countries play a role as producers or intermediaries,
either for trade or food processing. In consequence, we find that the trade
networks become more prone to failure cascades caused by exogenous shocks. In
our model, countries compensate for demand deficits by imposing export
restrictions. To capture these, we construct higher-order trade dependency
networks for the different crops and years. These networks reveal hidden
dependencies between countries and allow to discuss policy implications.
",0,0,0,0,0,1
579,Learning Graph Representations by Dendrograms,"  Hierarchical graph clustering is a common technique to reveal the multi-scale
structure of complex networks. We propose a novel metric for assessing the
quality of a hierarchical clustering. This metric reflects the ability to
reconstruct the graph from the dendrogram, which encodes the hierarchy. The
optimal representation of the graph defines a class of reducible linkages
leading to regular dendrograms by greedy agglomerative clustering.
",1,0,0,1,0,0
6036,PaccMann: Prediction of anticancer compound sensitivity with multi-modal attention-based neural networks,"  We present a novel approach for the prediction of anticancer compound
sensitivity by means of multi-modal attention-based neural networks (PaccMann).
In our approach, we integrate three key pillars of drug sensitivity, namely,
the molecular structure of compounds, transcriptomic profiles of cancer cells
as well as prior knowledge about interactions among proteins within cells. Our
models ingest a drug-cell pair consisting of SMILES encoding of a compound and
the gene expression profile of a cancer cell and predicts an IC50 sensitivity
value. Gene expression profiles are encoded using an attention-based encoding
mechanism that assigns high weights to the most informative genes. We present
and study three encoders for SMILES string of compounds: 1) bidirectional
recurrent 2) convolutional 3) attention-based encoders. We compare our devised
models against a baseline model that ingests engineered fingerprints to
represent the molecular structure. We demonstrate that using our
attention-based encoders, we can surpass the baseline model. The use of
attention-based encoders enhance interpretability and enable us to identify
genes, bonds and atoms that were used by the network to make a prediction.
",0,0,0,0,1,0
938,Human peripheral blur is optimal for object recognition,"  Our eyes sample a disproportionately large amount of information at the
centre of gaze with increasingly sparse sampling into the periphery. This
sampling scheme is widely believed to be a wiring constraint whereby high
resolution at the centre is achieved by sacrificing spatial acuity in the
periphery. Here we propose that this sampling scheme may be optimal for object
recognition because the relevant spatial content is dense near an object and
sparse in the surrounding vicinity. We tested this hypothesis by training deep
convolutional neural networks on full-resolution and foveated images. Our main
finding is that networks trained on images with foveated sampling show better
object classification compared to networks trained on full resolution images.
Importantly, blurring images according to the human blur function yielded the
best performance compared to images with shallower or steeper blurring. Taken
together our results suggest that, peripheral blurring in our eyes may have
evolved for optimal object recognition, rather than merely to satisfy wiring
constraints.
",0,0,0,0,1,0
6537,Finding influential nodes for integration in brain networks using optimal percolation theory,"  Global integration of information in the brain results from complex
interactions of segregated brain networks. Identifying the most influential
neuronal populations that efficiently bind these networks is a fundamental
problem of systems neuroscience. Here we apply optimal percolation theory and
pharmacogenetic interventions in-vivo to predict and subsequently target nodes
that are essential for global integration of a memory network in rodents. The
theory predicts that integration in the memory network is mediated by a set of
low-degree nodes located in the nucleus accumbens. This result is confirmed
with pharmacogenetic inactivation of the nucleus accumbens, which eliminates
the formation of the memory network, while inactivations of other brain areas
leave the network intact. Thus, optimal percolation theory predicts essential
nodes in brain networks. This could be used to identify targets of
interventions to modulate brain function.
",0,0,0,0,1,0
246,Many-Objective Pareto Local Search,"  We propose a new Pareto Local Search Algorithm for the many-objective
combinatorial optimization. Pareto Local Search proved to be a very effective
tool in the case of the bi-objective combinatorial optimization and it was used
in a number of the state-of-the-art algorithms for problems of this kind. On
the other hand, the standard Pareto Local Search algorithm becomes very
inefficient for problems with more than two objectives. We build an effective
Many-Objective Pareto Local Search algorithm using three new mechanisms: the
efficient update of large Pareto archives with ND-Tree data structure, a new
mechanism for the selection of the promising solutions for the neighborhood
exploration, and a partial exploration of the neighborhoods. We apply the
proposed algorithm to the instances of two different problems, i.e. the
traveling salesperson problem and the traveling salesperson problem with
profits with up to 5 objectives showing high effectiveness of the proposed
algorithm.
",1,0,0,0,0,0
7357,Dealing with the Dimensionality Curse in Dynamic Pricing Competition: Using Frequent Repricing to Compensate Imperfect Market Anticipations,"  Most sales applications are characterized by competition and limited demand
information. For successful pricing strategies, frequent price adjustments as
well as anticipation of market dynamics are crucial. Both effects are
challenging as competitive markets are complex and computations of optimized
pricing adjustments can be time-consuming. We analyze stochastic dynamic
pricing models under oligopoly competition for the sale of perishable goods. To
circumvent the curse of dimensionality, we propose a heuristic approach to
efficiently compute price adjustments. To demonstrate our strategy's
applicability even if the number of competitors is large and their strategies
are unknown, we consider different competitive settings in which competitors
frequently and strategically adjust their prices. For all settings, we verify
that our heuristic strategy yields promising results. We compare the
performance of our heuristic against upper bounds, which are obtained by
optimal strategies that take advantage of perfect price anticipations. We find
that price adjustment frequencies can have a larger impact on expected profits
than price anticipations. Finally, our approach has been applied on Amazon for
the sale of used books. We have used a seller's historical market data to
calibrate our model. Sales results show that our data-driven strategy
outperforms the rule-based strategy of an experienced seller by a profit
increase of more than 20%.
",0,0,0,0,0,1
5594,Analysis of Sequence Polymorphism of LINEs and SINEs in Entamoeba histolytica,"  The goal of this dissertation is to study the sequence polymorphism in
retrotransposable elements of Entamoeba histolytica. The Quasispecies theory, a
concept of equilibrium (stationary), has been used to understand the behaviour
of these elements. Two datasets of retrotransposons of Entamoeba histolytica
have been used. We present results from both datasets of retrotransposons
(SINE1s) of E. histolytica. We have calculated the mutation rate of EhSINE1s
for both datasets and drawn a phylogenetic tree for newly determined EhSINE1s
(dataset II). We have also discussed the variation in lengths of EhSINE1s for
both datasets. Using the quasispecies model, we have shown how sequences of
SINE1s vary within the population. The outputs of the quasispecies model are
discussed in the presence and the absence of back mutation by taking different
values of fitness. From our study of Non-long terminal repeat retrotransposons
(LINEs and their non-autonomous partner's SINEs) of Entamoeba histolytica, we
can conclude that an active EhSINE can generate very similar copies of itself
by retrotransposition. Due to this reason, it increases mutations which give
the result of sequence polymorphism. We have concluded that the mutation rate
of SINE is very high. This high mutation rate provides an idea for the
existence of SINEs, which may affect the genetic analysis of EhSINE1
ancestries, and calculation of phylogenetic distances.
",0,0,0,0,1,0
524,Static Free Space Detection with Laser Scanner using Occupancy Grid Maps,"  Drivable free space information is vital for autonomous vehicles that have to
plan evasive maneuvers in real-time. In this paper, we present a new efficient
method for environmental free space detection with laser scanner based on 2D
occupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems
(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced
inverse sensor model tailored for high-resolution laser scanners for building
OGM. It compensates the unreflected beams and deals with the ray casting to
grid cells accuracy and computational effort problems. Secondly, we introduce
the 'vehicle on a circle for grid maps' map alignment algorithm that allows
building more accurate local maps by avoiding the computationally expensive
inaccurate operations of image sub-pixel shifting and rotation. The resulted
grid map is more convenient for ADAS features than existing methods, as it
allows using less memory sizes, and hence, results into a better real-time
performance. Thirdly, we present an algorithm to detect what we call the
'in-sight edges'. These edges guarantee modeling the free space area with a
single polygon of a fixed number of vertices regardless the driving situation
and map complexity. The results from real world experiments show the
effectiveness of our approach.
",1,0,0,0,0,0
470,On the letter frequencies and entropy of written Marathi,"  We carry out a comprehensive analysis of letter frequencies in contemporary
written Marathi. We determine sets of letters which statistically predominate
any large generic Marathi text, and use these sets to estimate the entropy of
Marathi.
",1,0,0,0,0,0
6357,Finite size effects for spiking neural networks with spatially dependent coupling,"  We study finite-size fluctuations in a network of spiking deterministic
neurons coupled with non-uniform synaptic coupling. We generalize a previously
developed theory of finite size effects for uniform globally coupled neurons.
In the uniform case, mean field theory is well defined by averaging over the
network as the number of neurons in the network goes to infinity. However, for
nonuniform coupling it is no longer possible to average over the entire network
if we are interested in fluctuations at a particular location within the
network. We show that if the coupling function approaches a continuous function
in the infinite system size limit then an average over a local neighborhood can
be defined such that mean field theory is well defined for a spatially
dependent field. We then derive a perturbation expansion in the inverse system
size around the mean field limit for the covariance of the input to a neuron
(synaptic drive) and firing rate fluctuations due to dynamical deterministic
finite-size effects.
",0,0,0,0,1,0
2783,Towards Gene Expression Convolutions using Gene Interaction Graphs,"  We study the challenges of applying deep learning to gene expression data. We
find experimentally that there exists non-linear signal in the data, however is
it not discovered automatically given the noise and low numbers of samples used
in most research. We discuss how gene interaction graphs (same pathway,
protein-protein, co-expression, or research paper text association) can be used
to impose a bias on a deep model similar to the spatial bias imposed by
convolutions on an image. We explore the usage of Graph Convolutional Neural
Networks coupled with dropout and gene embeddings to utilize the graph
information. We find this approach provides an advantage for particular tasks
in a low data regime but is very dependent on the quality of the graph used. We
conclude that more work should be done in this direction. We design experiments
that show why existing methods fail to capture signal that is present in the
data when features are added which clearly isolates the problem that needs to
be addressed.
",0,0,0,1,1,0
476,Noisy Natural Gradient as Variational Inference,"  Variational Bayesian neural nets combine the flexibility of deep learning
with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff
between cheap but simple variational families (e.g.~fully factorized) or
expensive and complicated inference procedures. We show that natural gradient
ascent with adaptive weight noise implicitly fits a variational posterior to
maximize the evidence lower bound (ELBO). This insight allows us to train
full-covariance, fully factorized, or matrix-variate Gaussian variational
posteriors using noisy versions of natural gradient, Adam, and K-FAC,
respectively, making it possible to scale up to modern-size ConvNets. On
standard regression benchmarks, our noisy K-FAC algorithm makes better
predictions and matches Hamiltonian Monte Carlo's predictive variances better
than existing methods. Its improved uncertainty estimates lead to more
efficient exploration in active learning, and intrinsic motivation for
reinforcement learning.
",1,0,0,1,0,0
275,Neural Machine Translation,"  Draft of textbook chapter on neural machine translation. a comprehensive
treatment of the topic, ranging from introduction to neural networks,
computation graphs, description of the currently dominant attentional
sequence-to-sequence model, recent refinements, alternative architectures and
challenges. Written as chapter for the textbook Statistical Machine
Translation. Used in the JHU Fall 2017 class on machine translation.
",1,0,0,0,0,0
6458,CANA: A python package for quantifying control and canalization in Boolean Networks,"  Logical models offer a simple but powerful means to understand the complex
dynamics of biochemical regulation, without the need to estimate kinetic
parameters. However, even simple automata components can lead to collective
dynamics that are computationally intractable when aggregated into networks. In
previous work we demonstrated that automata network models of biochemical
regulation are highly canalizing, whereby many variable states and their
groupings are redundant (Marques-Pita and Rocha, 2013). The precise charting
and measurement of such canalization simplifies these models, making even very
large networks amenable to analysis. Moreover, canalization plays an important
role in the control, robustness, modularity and criticality of Boolean network
dynamics, especially those used to model biochemical regulation (Gates and
Rocha, 2016; Gates et al., 2016; Manicka, 2017). Here we describe a new
publicly-available Python package that provides the necessary tools to extract,
measure, and visualize canalizing redundancy present in Boolean network models.
It extracts the pathways most effective in controlling dynamics in these
models, including their effective graph and dynamics canalizing map, as well as
other tools to uncover minimum sets of control variables.
",1,0,0,0,1,0
304,Lie $\infty$-algebroids and singular foliations,"  A singular (or Hermann) foliation on a smooth manifold $M$ can be seen as a
subsheaf of the sheaf $\mathfrak{X}$ of vector fields on $M$. We show that if
this singular foliation admits a resolution (in the sense of sheaves)
consisting of sections of a graded vector bundle of finite type, then one can
lift the Lie bracket of vector fields to a Lie $\infty$-algebroid structure on
this resolution, that we call a universal Lie $\infty$-algebroid associated to
the foliation. The name is justified because it is isomorphic (up to homotopy)
to any other Lie $\infty$-algebroid structure built on any other resolution of
the given singular foliation.
",0,0,1,0,0,0
432,Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation,"  Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.
",1,0,0,0,0,0
339,Haro 11: Where is the Lyman continuum source?,"  Identifying the mechanism by which high energy Lyman continuum (LyC) photons
escaped from early galaxies is one of the most pressing questions in cosmic
evolution. Haro 11 is the best known local LyC leaking galaxy, providing an
important opportunity to test our understanding of LyC escape. The observed LyC
emission in this galaxy presumably originates from one of the three bright,
photoionizing knots known as A, B, and C. It is known that Knot C has strong
Ly$\alpha$ emission, and Knot B hosts an unusually bright ultraluminous X-ray
source, which may be a low-luminosity AGN. To clarify the LyC source, we carry
out ionization-parameter mapping (IPM) by obtaining narrow-band imaging from
the Hubble Space Telescope WFC3 and ACS cameras to construct spatially resolved
ratio maps of [OIII]/[OII] emission from the galaxy. IPM traces the ionization
structure of the interstellar medium and allows us to identify optically thin
regions. To optimize the continuum subtraction, we introduce a new method for
determining the best continuum scale factor derived from the mode of the
continuum-subtracted, image flux distribution. We find no conclusive evidence
of LyC escape from Knots B or C, but instead, we identify a high-ionization
region extending over at least 1 kpc from Knot A. Knot A shows evidence of an
extremely young age ($\lesssim 1$ Myr), perhaps containing very massive stars
($>100$ M$_\odot$). It is weak in Ly$\alpha$, so if it is confirmed as the LyC
source, our results imply that LyC emission may be independent of Ly$\alpha$
emission.
",0,1,0,0,0,0
13201,Integrating electricity markets: Impacts of increasing trade on prices and emissions in the western United States,"  This paper analyzes the market impacts of expanding California's centralized
electricity market across the western United States and provides the first
statistical assessment of this issue. Using market data from 2015-2018, I
estimate the short-term effects of increasing regional electricity trade
between California and neighboring states on prices, emissions, and generation.
Consistent with economic theory, I find negative price impacts from regional
trade, with each 1 gigawatt-hour (GWh) increase in California electricity
imports associated with an average 0.15 dollar decrease in CAISO price. The
price effect yields significant consumer savings well in excess of
implementation costs required to set up a regional market. I find a short-term
decrease in California carbon dioxide emissions associated with trading that is
partially offset by increased emissions in neighboring regions. Specifically,
each 1 GWh increase in regional trade is associated with a net 70-ton average
decrease in CO2 emissions across the western U.S. A small amount of increased
SO2 and NOx emissions are also observed in neighboring states associated with
increased exports to California. This implies a small portion (less than 10
percent) of electricity exports to California are supplied by coal generation.
This study identifies substantial short-term monetary benefits from market
regionalization for California consumers. It also shows that California's cap
and trade program is relatively effective in limiting the carbon content of
imported electricity, even absent a regional cap on CO2. The conclusions
suggest efforts to reduce trade barriers should move forward in parallel with
strong greenhouse gas policies that cap emissions levels across the market
region.
",0,0,0,0,0,1
852,Explicit evaluation of harmonic sums,"  In this paper, we obtain some formulae for harmonic sums, alternating
harmonic sums and Stirling number sums by using the method of integral
representations of series. As applications of these formulae, we give explicit
formula of several quadratic and cubic Euler sums through zeta values and
linear sums. Furthermore, some relationships between harmonic numbers and
Stirling numbers of the first kind are established.
",0,0,1,0,0,0
233,The g-Good-Neighbor Conditional Diagnosability of Locally Twisted Cubes,"  In the work of Peng et al. in 2012, a new measure was proposed for fault
diagnosis of systems: namely, g-good-neighbor conditional diagnosability, which
requires that any fault-free vertex has at least g fault-free neighbors in the
system. In this paper, we establish the g-good-neighbor conditional
diagnosability of locally twisted cubes under the PMC model and the MM^* model.
",1,0,1,0,0,0
3746,Instantaneous Arbitrage and the CAPM,"  This paper studies the concept of instantaneous arbitrage in continuous time
and its relation to the instantaneous CAPM. Absence of instantaneous arbitrage
is equivalent to the existence of a trading strategy which satisfies the CAPM
beta pricing relation in place of the market. Thus the difference between the
arbitrage argument and the CAPM argument in Black and Scholes (1973) is this:
the arbitrage argument assumes that there exists some portfolio satisfying the
capm equation, whereas the CAPM argument assumes, in addition, that this
portfolio is the market portfolio.
",0,0,0,0,0,1
547,Modular curves with infinitely many cubic points,"  In this study, we determine all modular curves $X_0(N)$ that admit infinitely
many cubic points.
",0,0,1,0,0,0
917,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,"  The recently proposed Temporal Ensembling has achieved state-of-the-art
results in several semi-supervised learning benchmarks. It maintains an
exponential moving average of label predictions on each training example, and
penalizes predictions that are inconsistent with this target. However, because
the targets change only once per epoch, Temporal Ensembling becomes unwieldy
when learning large datasets. To overcome this problem, we propose Mean
Teacher, a method that averages model weights instead of label predictions. As
an additional benefit, Mean Teacher improves test accuracy and enables training
with fewer labels than Temporal Ensembling. Without changing the network
architecture, Mean Teacher achieves an error rate of 4.35% on SVHN with 250
labels, outperforming Temporal Ensembling trained with 1000 labels. We also
show that a good network architecture is crucial to performance. Combining Mean
Teacher and Residual Networks, we improve the state of the art on CIFAR-10 with
4000 labels from 10.55% to 6.28%, and on ImageNet 2012 with 10% of the labels
from 35.24% to 9.11%.
",1,0,0,1,0,0
13516,A Stochastic Control Approach to Managed Futures Portfolios,"  We study a stochastic control approach to managed futures portfolios.
Building on the Schwartz 97 stochastic convenience yield model for commodity
prices, we formulate a utility maximization problem for dynamically trading a
single-maturity futures or multiple futures contracts over a finite horizon. By
analyzing the associated Hamilton-Jacobi-Bellman (HJB) equation, we solve the
investor's utility maximization problem explicitly and derive the optimal
dynamic trading strategies in closed form. We provide numerical examples and
illustrate the optimal trading strategies using WTI crude oil futures data.
",0,0,0,0,0,1
373,When Streams of Optofluidics Meet the Sea of Life,"  Luke P. Lee is a Tan Chin Tuan Centennial Professor at the National
University of Singapore. In this contribution he describes the power of
optofluidics as a research tool and reviews new insights within the areas of
single cell analysis, microphysiological analysis, and integrated systems.
",0,0,0,0,1,0
858,An evaluation homomorphism for quantum toroidal gl(n) algebras,"  We present an affine analog of the evaluation map for quantum groups. Namely
we introduce a surjective homomorphism from the quantum toroidal gl(n) algebra
to the quantum affine gl(n) algebra completed with respect to the homogeneous
grading. We give a brief discussion of evaluation modules.
",0,0,1,0,0,0
798,Composition Factors of Tensor Products of Symmetric Powers,"  We determine the composition factors of the tensor product $S(E)\otimes S(E)$
of two copies of the symmetric algebra of the natural module $E$ of a general
linear group over an algebraically closed field of positive characteristic. Our
main result may be regarded as a substantial generalisation of the tensor
product theorem of Krop and Sullivan, on composition factors of $S(E)$. We
earlier answered the question of which polynomially injective modules are
infinitesimally injective in terms of the ""divisibility index"". We are now able
to give an explicit description of the divisibility index for polynomial
modules for general linear groups of degree at most $3$.
",0,0,1,0,0,0
5563,Variable domain N-linked glycosylation and negative surface charge are key features of monoclonal ACPA: implications for B-cell selection,"  Autoreactive B cells have a central role in the pathogenesis of rheumatoid
arthritis (RA), and recent findings have proposed that anti-citrullinated
protein autoantibodies (ACPA) may be directly pathogenic. Herein, we
demonstrate the frequency of variable-region glycosylation in single-cell
cloned mAbs. A total of 14 ACPA mAbs were evaluated for predicted N-linked
glycosylation motifs in silico and compared to 452 highly-mutated mAbs from RA
patients and controls. Variable region N-linked motifs (N-X-S/T) were
strikingly prevalent within ACPA (100%) compared to somatically hypermutated
(SHM) RA bone marrow plasma cells (21%), and synovial plasma cells from
seropositive (39%) and seronegative RA (7%). When normalized for SHM, ACPA
still had significantly higher frequency of N-linked motifs compared to all
studied mAbs including highly-mutated HIV broadly-neutralizing and
malaria-associated mAbs. The Fab glycans of ACPA-mAbs were highly sialylated,
contributed to altered charge, but did not influence antigen binding. The
analysis revealed evidence of unusual B-cell selection pressure and
SHM-mediated decreased in surface charge and isoelectric point in ACPA. It is
still unknown how these distinct features of anti-citrulline immunity may have
an impact on pathogenesis. However, it is evident that they offer selective
advantages for ACPA+ B cells, possibly also through non-antigen driven
mechanisms.
",0,0,0,0,1,0
894,Computationally Efficient Estimation of the Spectral Gap of a Markov Chain,"  We consider the problem of estimating from sample paths the absolute spectral
gap $\gamma_*$ of a reversible, irreducible and aperiodic Markov chain
$(X_t)_{t \in \mathbb{N}}$ over a finite state $\Omega$. We propose the ${\tt
UCPI}$ (Upper Confidence Power Iteration) algorithm for this problem, a
low-complexity algorithm which estimates the spectral gap in time ${\cal O}(n)$
and memory space ${\cal O}((\ln n)^2)$ given $n$ samples. This is in stark
contrast with most known methods which require at least memory space ${\cal
O}(|\Omega|)$, so that they cannot be applied to large state spaces.
Furthermore, ${\tt UCPI}$ is amenable to parallel implementation.
",0,0,0,1,0,0
953,Likelihood ratio test for variance components in nonlinear mixed effects models,"  Mixed effects models are widely used to describe heterogeneity in a
population. A crucial issue when adjusting such a model to data consists in
identifying fixed and random effects. From a statistical point of view, it
remains to test the nullity of the variances of a given subset of random
effects. Some authors have proposed to use the likelihood ratio test and have
established its asymptotic distribution in some particular cases. Nevertheless,
to the best of our knowledge, no general variance components testing procedure
has been fully investigated yet. In this paper, we study the likelihood ratio
test properties to test that the variances of a general subset of the random
effects are equal to zero in both linear and nonlinear mixed effects model,
extending the existing results. We prove that the asymptotic distribution of
the test is a chi-bar-square distribution, that is to say a mixture of
chi-square distributions, and we identify the corresponding weights. We
highlight in particular that the limiting distribution depends on the presence
of correlations between the random effects but not on the linear or nonlinear
structure of the mixed effects model. We illustrate the finite sample size
properties of the test procedure through simulation studies and apply the test
procedure to two real datasets of dental growth and of coucal growth.
",0,0,0,1,0,0
585,Attitude Control of the Asteroid Origins Satellite 1 (AOSAT 1),"  Exploration of asteroids and small-bodies can provide valuable insight into
the origins of the solar system, into the origins of Earth and the origins of
the building blocks of life. However, the low-gravity and unknown surface
conditions of asteroids presents a daunting challenge for surface exploration,
manipulation and for resource processing. This has resulted in the loss of
several landers or shortened missions. Fundamental studies are required to
obtain better readings of the material surface properties and physical models
of these small bodies. The Asteroid Origins Satellite 1 (AOSAT 1) is a CubeSat
centrifuge laboratory that spins at up to 4 rpm to simulate the milligravity
conditions of sub 1 km asteroids. Such a laboratory will help to de-risk
development and testing of landing and resource processing technology for
asteroids. Inside the laboratory are crushed meteorites, the remains of
asteroids. The laboratory is equipped with cameras and actuators to perform a
series of science experiments to better understand material properties and
asteroid surface physics. These results will help to improve our physics models
of asteroids. The CubeSat has been designed to be low-cost and contains 3-axis
magnetorquers and a single reaction-wheel to induce spin. In our work, we first
analyze how the attitude control system will de-tumble the spacecraft after
deployment. Further analysis has been conducted to analyze the impact and
stability of the attitude control system to shifting mass (crushed meteorites)
inside the spacecraft as its spinning in its centrifuge mode. AOSAT 1 will be
the first in a series of low-cost CubeSat centrifuges that will be launched
setting the stage for a larger, permanent, on-orbit centrifuge laboratory for
experiments in planetary science, life sciences and manufacturing.
",1,1,0,0,0,0
13435,Lee-Carter method for forecasting mortality for Peruvian Population,"  In this article, we have modeled mortality rates of Peruvian female and male
populations during the period of 1950-2017 using the Lee-Carter (LC) model. The
stochastic mortality model was introduced by Lee and Carter (1992) and has been
used by many authors for fitting and forecasting the human mortality rates. The
Singular Value Decomposition (SVD) approach is used for estimation of the
parameters of the LC model. Utilizing the best fitted auto regressive
integrated moving average (ARIMA) model we forecast the values of the time
dependent parameter of the LC model for the next thirty years. The forecasted
values of life expectancy at different age group with $95\%$ confidence
intervals are also reported for the next thirty years. In this research we use
the data, obtained from the Peruvian National Institute of Statistics (INEI).
",0,0,0,0,0,1
412,Star Formation Activity in the molecular cloud G35.20$-$0.74: onset of cloud-cloud collision,"  To probe the star-formation (SF) processes, we present results of an analysis
of the molecular cloud G35.20$-$0.74 (hereafter MCG35.2) using multi-frequency
observations. The MCG35.2 is depicted in a velocity range of 30-40 km s$^{-1}$.
An almost horseshoe-like structure embedded within the MCG35.2 is evident in
the infrared and millimeter images and harbors the previously known sites,
ultra-compact/hyper-compact G35.20$-$0.74N H\,{\sc ii} region, Ap2-1, and
Mercer 14 at its base. The site, Ap2-1 is found to be excited by a radio
spectral type of B0.5V star where the distribution of 20 cm and H$\alpha$
emission is surrounded by the extended molecular hydrogen emission. Using the
{\it Herschel} 160-500 $\mu$m and photometric 1-24 $\mu$m data analysis,
several embedded clumps and clusters of young stellar objects (YSOs) are
investigated within the MCG35.2, revealing the SF activities. Majority of the
YSOs clusters and massive clumps (500-4250 M$_{\odot}$) are seen toward the
horseshoe-like structure. The position-velocity analysis of $^{13}$CO emission
shows a blue-shifted peak (at 33 km s$^{-1}$) and a red-shifted peak (at 37 km
s$^{-1}$) interconnected by lower intensity intermediated velocity emission,
tracing a broad bridge feature. The presence of such broad bridge feature
suggests the onset of a collision between molecular components in the MCG35.2.
A noticeable change in the H-band starlight mean polarization angles has also
been observed in the MCG35.2, probably tracing the interaction between
molecular components. Taken together, it seems that the cloud-cloud collision
process has influenced the birth of massive stars and YSOs clusters in the
MCG35.2.
",0,1,0,0,0,0
11146,Stochastic comparisons of the largest claim amounts from two sets of interdependent heterogeneous portfolios,"  Let $ X_{\lambda_1},\ldots,X_{\lambda_n}$ be dependent non-negative random
variables and $Y_i=I_{p_i} X_{\lambda_i}$, $i=1,\ldots,n$, where
$I_{p_1},\ldots,I_{p_n}$ are independent Bernoulli random variables independent
of $X_{\lambda_i}$'s, with ${\rm E}[I_{p_i}]=p_i$, $i=1,\ldots,n$. In actuarial
sciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In
this paper, we compare the largest claim amounts of two sets of interdependent
portfolios, in the sense of usual stochastic order, when the variables in one
set have the parameters $\lambda_1,\ldots,\lambda_n$ and $p_1,\ldots,p_n$ and
the variables in the other set have the parameters
$\lambda^{*}_1,\ldots,\lambda^{*}_n$ and $p^*_1,\ldots,p^*_n$. For
illustration, we apply the results to some important models in actuary.
",0,0,0,0,0,1
5574,Optimal portfolio selection in an Itô-Markov additive market,"  We study a portfolio selection problem in a continuous-time Itô-Markov
additive market with prices of financial assets described by Markov additive
processes which combine Lévy processes and regime switching models. Thus the
model takes into account two sources of risk: the jump diffusion risk and the
regime switching risk. For this reason the market is incomplete. We complete
the market by enlarging it with the use of a set of Markovian jump securities,
Markovian power-jump securities and impulse regime switching securities.
Moreover, we give conditions under which the market is
asymptotic-arbitrage-free. We solve the portfolio selection problem in the
Itô-Markov additive market for the power utility and the logarithmic utility.
",0,0,0,0,0,1
16304,Arbitrage-Free Interpolation in Models of Market Observable Interest Rates,"  Models which postulate lognormal dynamics for interest rates which are
compounded according to market conventions, such as forward LIBOR or forward
swap rates, can be constructed initially in a discrete tenor framework.
Interpolating interest rates between maturities in the discrete tenor structure
is equivalent to extending the model to continuous tenor. The present paper
sets forth an alternative way of performing this extension; one which preserves
the Markovian properties of the discrete tenor models and guarantees the
positivity of all interpolated rates.
",0,0,0,0,0,1
645,Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming,"  We design a new myopic strategy for a wide class of sequential design of
experiment (DOE) problems, where the goal is to collect data in order to to
fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling
(MPS), is inspired by the classical posterior (Thompson) sampling algorithm for
multi-armed bandits and leverages the flexibility of probabilistic programming
and approximate Bayesian inference to address a broad set of problems.
Empirically, this general-purpose strategy is competitive with more specialised
methods in a wide array of DOE tasks, and more importantly, enables addressing
complex DOE goals where no existing method seems applicable. On the theoretical
side, we leverage ideas from adaptive submodularity and reinforcement learning
to derive conditions under which MPS achieves sublinear regret against natural
benchmark policies.
",0,0,0,1,0,0
310,Construction of embedded periodic surfaces in $\mathbb{R}^n$,"  We construct embedded minimal surfaces which are $n$-periodic in
$\mathbb{R}^n$. They are new for codimension $n-2\ge 2$. We start with a Jordan
curve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk
which Schwarz reflection extends to a complete minimal surface. Studying the
group of Schwarz reflections, we can characterize those Jordan curves for which
the complete surface is embedded. For example, for $n=4$ exactly five such
Jordan curves generate embedded surfaces. Our results apply to surface classes
other than minimal as well, for instance polygonal surfaces.
",0,0,1,0,0,0
399,Quantum Interference of Glory Rescattering in Strong-Field Atomic Ionization,"  During the ionization of atoms irradiated by linearly polarized intense laser
fields, we find for the first time that the transverse momentum distribution of
photoelectrons can be well fitted by a squared zeroth-order Bessel function
because of the quantum interference effect of Glory rescattering. The
characteristic of the Bessel function is determined by the common angular
momentum of a bunch of semiclassical paths termed as Glory trajectories, which
are launched with different nonzero initial transverse momenta distributed on a
specific circle in the momentum plane and finally deflected to the same
asymptotic momentum, which is along the polarization direction, through
post-tunneling rescattering. Glory rescattering theory (GRT) based on the
semiclassical path-integral formalism is developed to address this effect
quantitatively. Our theory can resolve the long-standing discrepancies between
existing theories and experiments on the fringe location, predict the sudden
transition of the fringe structure in holographic patterns, and shed light on
the quantum interference aspects of low-energy structures in strong-field
atomic ionization.
",0,1,0,0,0,0
230,General notions of regression depth function,"  As a measure for the centrality of a point in a set of multivariate data,
statistical depth functions play important roles in multivariate analysis,
because one may conveniently construct descriptive as well as inferential
procedures relying on them. Many depth notions have been proposed in the
literature to fit to different applications. However, most of them are mainly
developed for the location setting. In this paper, we discuss the possibility
of extending some of them into the regression setting. A general concept of
regression depth function is also provided.
",0,0,0,1,0,0
543,An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$,"  Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to
$SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine
an inductive formula for the Schubert basis structure constants in the
torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of
ordinary (non-equivariant) $K$-theory we find an explicit closed form for the
structure constants. We also determine an inductive formula for the structure
constants in the torus-equivariant cohomology ring, and use this formula to
find closed forms for some of the structure constants.
",0,0,1,0,0,0
591,Hybrid Collaborative Recommendation via Semi-AutoEncoder,"  In this paper, we present a novel structure, Semi-AutoEncoder, based on
AutoEncoder. We generalize it into a hybrid collaborative filtering model for
rating prediction as well as personalized top-n recommendations. Experimental
results on two real-world datasets demonstrate its state-of-the-art
performances.
",1,0,0,0,0,0
320,Frank-Wolfe with Subsampling Oracle,"  We analyze two novel randomized variants of the Frank-Wolfe (FW) or
conditional gradient algorithm. While classical FW algorithms require solving a
linear minimization problem over the domain at each iteration, the proposed
method only requires to solve a linear minimization problem over a small
\emph{subset} of the original domain. The first algorithm that we propose is a
randomized variant of the original FW algorithm and achieves a
$\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic
counterpart. The second algorithm is a randomized variant of the Away-step FW
algorithm, and again as its deterministic counterpart, reaches linear (i.e.,
exponential) convergence rate making it the first provably convergent
randomized variant of Away-step FW. In both cases, while subsampling reduces
the convergence rate by a constant factor, the linear minimization step can be
a fraction of the cost of that of the deterministic versions, especially when
the data is streamed. We illustrate computational gains of the algorithms on
regression problems, involving both $\ell_1$ and latent group lasso penalties.
",0,0,0,1,0,0
6386,Model Risk Measurement under Wasserstein Distance,"  The paper proposes a new approach to model risk measurement based on the
Wasserstein distance between two probability measures. It formulates the
theoretical motivation resulting from the interpretation of fictitious
adversary of robust risk management. The proposed approach accounts for all
alternative models and incorporates the economic reality of the fictitious
adversary. It provides practically feasible results that overcome the
restriction and the integrability issue imposed by the nominal model. The
Wasserstein approach suits for all types of model risk problems, ranging from
the single-asset hedging risk problem to the multi-asset allocation problem.
The robust capital allocation line, accounting for the correlation risk, is not
achievable with other non-parametric approaches.
",0,0,0,0,0,1
293,Quantum Charge Pumps with Topological Phases in Creutz Ladder,"  Quantum charge pumping phenomenon connects band topology through the dynamics
of a one-dimensional quantum system. In terms of a microscopic model, the
Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
starting point for many considerations of topological physics. Here we present
a generalized Creutz scheme as a distinct two-band quantum pump model. By
noting that it undergoes two kinds of topological band transitions accompanying
with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
pumping schemes are studied by applying an elaborate Peierl's phase
substitution. Translating into real space, the transportation of quantized
charges is a result of cooperative quantum interference effect. In particular,
an all-flux quantum pump emerges which operates with time-varying fluxes only
and transports two charge units. This puts cold atoms with artificial gauge
fields as an unique system where this kind of phenomena can be realized.
",0,1,0,0,0,0
321,A mean value formula and a Liouville theorem for the complex Monge-Ampère equation,"  In this paper, we prove a mean value formula for bounded subharmonic
Hermitian matrix valued function on a complete Riemannian manifold with
nonnegative Ricci curvature. As its application, we obtain a Liouville type
theorem for the complex Monge-Ampère equation on product manifolds.
",0,0,1,0,0,0
566,Muon Reconstruction in the Daya Bay Water Pools,"  Muon reconstruction in the Daya Bay water pools would serve to verify the
simulated muon fluxes and offer the possibility of studying cosmic muons in
general. This reconstruction is, however, complicated by many optical obstacles
and the small coverage of photomultiplier tubes (PMTs) as compared to other
large water Cherenkov detectors. The PMTs' timing information is useful only in
the case of direct, unreflected Cherenkov light. This requires PMTs to be added
and removed as an hypothesized muon trajectory is iteratively improved, to
account for the changing effects of obstacles and direction of light.
Therefore, muon reconstruction in the Daya Bay water pools does not lend itself
to a general fitting procedure employing smoothly varying functions with
continuous derivatives. Here, an algorithm is described which overcomes these
complications. It employs the method of Least Mean Squares to determine an
hypothesized trajectory from the PMTs' charge-weighted positions. This
initially hypothesized trajectory is then iteratively refined using the PMTs'
timing information. Reconstructions with simulated data reproduce the simulated
trajectory to within about 5 degrees in direction and about 45 cm in position
at the pool surface, with a bias that tends to pull tracks away from the
vertical by about 3 degrees.
",0,1,0,0,0,0
367,Phase correction for ALMA - Investigating water vapour radiometer scaling:The long-baseline science verification data case study,"  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water
vapour radiometers (WVR), which monitor the atmospheric water vapour line at
183 GHz along the line of sight above each antenna to correct for phase delays
introduced by the wet component of the troposphere. The application of WVR
derived phase corrections improve the image quality and facilitate successful
observations in weather conditions that were classically marginal or poor. We
present work to indicate that a scaling factor applied to the WVR solutions can
act to further improve the phase stability and image quality of ALMA data. We
find reduced phase noise statistics for 62 out of 75 datasets from the
long-baseline science verification campaign after a WVR scaling factor is
applied. The improvement of phase noise translates to an expected coherence
improvement in 39 datasets. When imaging the bandpass source, we find 33 of the
39 datasets show an improvement in the signal-to-noise ratio (S/N) between a
few to ~30 percent. There are 23 datasets where the S/N of the science image is
improved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies
studied (band 6 and band 7) are those most improved, specifically datasets with
low precipitable water vapour (PWV), <1mm, where the dominance of the wet
component is reduced. Although these improvements are not profound, phase
stability improvements via the WVR scaling factor come into play for the higher
frequency (>450 GHz) and long-baseline (>5km) observations. These inherently
have poorer phase stability and are taken in low PWV (<1mm) conditions for
which we find the scaling to be most effective. A promising explanation for the
scaling factor is the mixing of dry and wet air components, although other
origins are discussed. We have produced a python code to allow ALMA users to
undertake WVR scaling tests and make improvements to their data.
",0,1,0,0,0,0
1593,Activit{é} motrice des truies en groupes dans les diff{é}rents syst{è}mes de logement,"  Assessment of the motor activity of group-housed sows in commercial farms.
The objective of this study was to specify the level of motor activity of
pregnant sows housed in groups in different housing systems. Eleven commercial
farms were selected for this study. Four housing systems were represented:
small groups of five to seven sows (SG), free access stalls (FS) with exercise
area, electronic sow feeder with a stable group (ESFsta) or a dynamic group
(ESFdyn). Ten sows in mid-gestation were observed in each farm. The
observations of motor activity were made for 6 hours at the first meal or at
the start of the feeding sequence, two consecutive days and at regular
intervals of 4 minutes. The results show that the motor activity of
group-housed sows depends on the housing system. The activity is higher with
the ESFdyn system (standing: 55.7%), sows are less active in the SG system
(standing: 26.5%), and FS system is intermediate. The distance traveled by sows
in ESF system is linked to a larger area available. Thus, sows travel an
average of 362 m $\pm$ 167 m in the ESFdyn system with an average available
surface of 446 m${}^2$ whereas sows in small groups travel 50 m $\pm$ 15 m for
15 m${}^2$ available.
",0,0,0,0,1,0
5403,Anticipation: an effective evolutionary strategy for a sub-optimal population in a cyclic environment,"  We built a two-state model of an asexually reproducing organism in a periodic
environment endowed with the capability to anticipate an upcoming environmental
change and undergo pre-emptive switching. By virtue of these anticipatory
transitions, the organism oscillates between its two states that is a time
$\theta$ out of sync with the environmental oscillation. We show that an
anticipation-capable organism increases its long-term fitness over an organism
that oscillates in-sync with the environment, provided $\theta$ does not exceed
a threshold. We also show that the long-term fitness is maximized for an
optimal anticipation time that decreases approximately as $1/n$, $n$ being the
number of cell divisions in time $T$. Furthermore, we demonstrate that optimal
""anticipators"" outperforms ""bet-hedgers"" in the range of parameters considered.
For a sub-optimal ensemble of anticipators, anticipation performs better to
bet-hedging only when the variance in anticipation is small compared to the
mean and the rate of pre-emptive transition is high. Taken together, our work
suggests that anticipation increases overall fitness of an organism in a
periodic environment and it is a viable alternative to bet-hedging provided the
error in anticipation is small.
",0,0,0,0,1,0
686,Context-Aware Pedestrian Motion Prediction In Urban Intersections,"  This paper presents a novel context-based approach for pedestrian motion
prediction in crowded, urban intersections, with the additional flexibility of
prediction in similar, but new, environments. Previously, Chen et. al. combined
Markovian-based and clustering-based approaches to learn motion primitives in a
grid-based world and subsequently predict pedestrian trajectories by modeling
the transition between learned primitives as a Gaussian Process (GP). This work
extends that prior approach by incorporating semantic features from the
environment (relative distance to curbside and status of pedestrian traffic
lights) in the GP formulation for more accurate predictions of pedestrian
trajectories over the same timescale. We evaluate the new approach on
real-world data collected using one of the vehicles in the MIT Mobility On
Demand fleet. The results show 12.5% improvement in prediction accuracy and a
2.65 times reduction in Area Under the Curve (AUC), which is used as a metric
to quantify the span of predicted set of trajectories, such that a lower AUC
corresponds to a higher level of confidence in the future direction of
pedestrian motion.
",1,0,0,1,0,0
496,Isogenies for point counting on genus two hyperelliptic curves with maximal real multiplication,"  Schoof's classic algorithm allows point-counting for elliptic curves over
finite fields in polynomial time. This algorithm was subsequently improved by
Atkin, using factorizations of modular polynomials, and by Elkies, using a
theory of explicit isogenies. Moving to Jacobians of genus-2 curves, the
current state of the art for point counting is a generalization of Schoof's
algorithm. While we are currently missing the tools we need to generalize
Elkies' methods to genus 2, recently Martindale and Milio have computed
analogues of modular polynomials for genus-2 curves whose Jacobians have real
multiplication by maximal orders of small discriminant. In this article, we
prove Atkin-style results for genus-2 Jacobians with real multiplication by
maximal orders, with a view to using these new modular polynomials to improve
the practicality of point-counting algorithms for these curves.
",1,0,1,0,0,0
2169,The Rank Effect,"  We decompose returns for portfolios of bottom-ranked, lower-priced assets
relative to the market into rank crossovers and changes in the relative price
of those bottom-ranked assets. This decomposition is general and consistent
with virtually any asset pricing model. Crossovers measure changes in rank and
are smoothly increasing over time, while return fluctuations are driven by
volatile relative price changes. Our results imply that in a closed,
dividend-free market in which the relative price of bottom-ranked assets is
approximately constant, a portfolio of those bottom-ranked assets will
outperform the market portfolio over time. We show that bottom-ranked relative
commodity futures prices have increased only slightly, and confirm the
existence of substantial excess returns predicted by our theory. If these
excess returns did not exist, then top-ranked relative prices would have had to
be much higher in 2018 than those actually observed -- this would imply a
radically different commodity price distribution.
",0,0,0,0,0,1
389,"Convolution Semigroups of Probability Measures on Gelfand Pairs, Revisited","  Our goal is to find classes of convolution semigroups on Lie groups $G$ that
give rise to interesting processes in symmetric spaces $G/K$. The
$K$-bi-invariant convolution semigroups are a well-studied example. An
appealing direction for the next step is to generalise to right $K$-invariant
convolution semigroups, but recent work of Liao has shown that these are in
one-to-one correspondence with $K$-bi-invariant convolution semigroups. We
investigate a weaker notion of right $K$-invariance, but show that this is, in
fact, the same as the usual notion. Another possible approach is to use
generalised notions of negative definite functions, but this also leads to
nothing new. We finally find an interesting class of convolution semigroups
that are obtained by making use of the Cartan decomposition of a semisimple Lie
group, and the solution of certain stochastic differential equations. Examples
suggest that these are well-suited for generating random motion along geodesics
in symmetric spaces.
",0,0,1,0,0,0
956,Solving $\ell^p\!$-norm regularization with tensor kernels,"  In this paper, we discuss how a suitable family of tensor kernels can be used
to efficiently solve nonparametric extensions of $\ell^p$ regularized learning
methods. Our main contribution is proposing a fast dual algorithm, and showing
that it allows to solve the problem efficiently. Our results contrast recent
findings suggesting kernel methods cannot be extended beyond Hilbert setting.
Numerical experiments confirm the effectiveness of the method.
",0,0,1,1,0,0
850,Opinion dynamics model based on cognitive biases,"  We present an introduction to a novel model of an individual and group
opinion dynamics, taking into account different ways in which different sources
of information are filtered due to cognitive biases. The agent based model,
using Bayesian updating of the individual belief distribution, is based on the
recent psychology work by Dan Kahan. Open nature of the model allows to study
the effects of both static and time-dependent biases and information processing
filters. In particular, the paper compares the effects of two important
psychological mechanisms: the confirmation bias and the politically motivated
reasoning. Depending on the effectiveness of the information filtering (agent
bias), the agents confronted with an objective information source may either
reach a consensus based on the truth, or remain divided despite the evidence.
In general, the model might provide an understanding into the increasingly
polarized modern societies, especially as it allows mixing of different types
of filters: psychological, social, and algorithmic.
",1,1,0,0,0,0
588,Optimum Decoder for Multiplicative Spread Spectrum Image Watermarking with Laplacian Modeling,"  This paper investigates the multiplicative spread spectrum watermarking
method for the image. The information bit is spreaded into middle-frequency
Discrete Cosine Transform (DCT) coefficients of each block of an image using a
generated pseudo-random sequence. Unlike the conventional signal modeling, we
suppose that both signal and noise are distributed with Laplacian distribution
because the sample loss of digital media can be better modeled with this
distribution than the Gaussian one. We derive the optimum decoder for the
proposed embedding method thanks to the maximum likelihood decoding scheme. We
also analyze our watermarking system in the presence of noise and provide
analytical evaluations and several simulations. The results show that it has
the suitable performance and transparency required for watermarking
applications.
",1,0,0,0,0,0
261,Closing the loop on multisensory interactions: A neural architecture for multisensory causal inference and recalibration,"  When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
",0,0,0,0,1,0
7928,Multi-channel discourse as an indicator for Bitcoin price and volume movements,"  This research aims to identify how Bitcoin-related news publications and
online discourse are expressed in Bitcoin exchange movements of price and
volume. Being inherently digital, all Bitcoin-related fundamental data (from
exchanges, as well as transactional data directly from the blockchain) is
available online, something that is not true for traditional businesses or
currencies traded on exchanges. This makes Bitcoin an interesting subject for
such research, as it enables the mapping of sentiment to fundamental events
that might otherwise be inaccessible. Furthermore, Bitcoin discussion largely
takes place on online forums and chat channels. In stock trading, the value of
sentiment data in trading decisions has been demonstrated numerous times [1]
[2] [3], and this research aims to determine whether there is value in such
data for Bitcoin trading models. To achieve this, data over the year 2015 has
been collected from Bitcointalk.org, (the biggest Bitcoin forum in post
volume), established news sources such as Bloomberg and the Wall Street
Journal, the complete /r/btc and /r/Bitcoin subreddits, and the bitcoin-otc and
bitcoin-dev IRC channels. By analyzing this data on sentiment and volume, we
find weak to moderate correlations between forum, news, and Reddit sentiment
and movements in price and volume from 1 to 5 days after the sentiment was
expressed. A Granger causality test confirms the predictive causality of the
sentiment on the daily percentage price and volume movements, and at the same
time underscores the predictive causality of market movements on sentiment
expressions in online communities
",0,0,0,0,0,1
3251,Deep Learning for micro-Electrocorticographic (μECoG) Data,"  Machine learning can extract information from neural recordings, e.g.,
surface EEG, ECoG and {\mu}ECoG, and therefore plays an important role in many
research and clinical applications. Deep learning with artificial neural
networks has recently seen increasing attention as a new approach in brain
signal decoding. Here, we apply a deep learning approach using convolutional
neural networks to {\mu}ECoG data obtained with a wireless, chronically
implanted system in an ovine animal model. Regularized linear discriminant
analysis (rLDA), a filter bank component spatial pattern (FBCSP) algorithm and
convolutional neural networks (ConvNets) were applied to auditory evoked
responses captured by {\mu}ECoG. We show that compared with rLDA and FBCSP,
significantly higher decoding accuracy can be obtained by ConvNets trained in
an end-to-end manner, i.e., without any predefined signal features. Deep
learning thus proves a promising technique for {\mu}ECoG-based brain-machine
interfacing applications.
",0,0,0,0,1,0
315,Analyses and estimation of certain design parameters of micro-grooved heat pipes,"  A numerical analysis of heat conduction through the cover plate of a heat
pipe is carried out to determine the temperature of the working substance,
average temperature of heating and cooling surfaces, heat spread in the
transmitter, and the heat bypass through the cover plate. Analysis has been
extended for the estimation of heat transfer requirements at the outer surface
of the con- denser under different heat load conditions using Genetic
Algorithm. This paper also presents the estimation of an average heat transfer
coefficient for the boiling and condensation of the working substance inside
the microgrooves corresponding to a known temperature of the heat source. The
equation of motion of the working fluid in the meniscus of an equilateral
triangular groove has been presented from which a new term called the minimum
surface tension required for avoiding the dry out condition is defined.
Quantitative results showing the effect of thickness of cover plate, heat load,
angle of inclination and viscosity of the working fluid on the different
aspects of the heat transfer, minimum surface tension required to avoid dry
out, velocity distribution of the liquid, and radius of liquid meniscus inside
the micro-grooves have been presented and discussed.
",0,1,0,0,0,0
381,Adaptive Model Predictive Control for High-Accuracy Trajectory Tracking in Changing Conditions,"  Robots and automated systems are increasingly being introduced to unknown and
dynamic environments where they are required to handle disturbances, unmodeled
dynamics, and parametric uncertainties. Robust and adaptive control strategies
are required to achieve high performance in these dynamic environments. In this
paper, we propose a novel adaptive model predictive controller that combines
model predictive control (MPC) with an underlying $\mathcal{L}_1$ adaptive
controller to improve trajectory tracking of a system subject to unknown and
changing disturbances. The $\mathcal{L}_1$ adaptive controller forces the
system to behave in a predefined way, as specified by a reference model. A
higher-level model predictive controller then uses this reference model to
calculate the optimal reference input based on a cost function, while taking
into account input and state constraints. We focus on the experimental
validation of the proposed approach and demonstrate its effectiveness in
experiments on a quadrotor. We show that the proposed approach has a lower
trajectory tracking error compared to non-predictive, adaptive approaches and a
predictive, non-adaptive approach, even when external wind disturbances are
applied.
",1,0,0,0,0,0
14082,Closed-form approximations in derivatives pricing: The Kristensen-Mele approach,"  Kristensen and Mele (2011) developed a new approach to obtain closed-form
approximations to continuous-time derivatives pricing models. The approach uses
a power series expansion of the pricing bias between an intractable model and
some known auxiliary model. Since the resulting approximation formula has
closed-form it is straightforward to obtain approximations of greeks. In this
thesis I will introduce Kristensen and Mele's methods and apply it to a variety
of stochastic volatility models of European style options as well as a model
for commodity futures. The focus of this thesis is the effect of different
model choices and different model parameter values on the numerical stability
of Kristensen and Mele's approximation.
",0,0,0,0,0,1
752,Molecular Simulation of Caloric Properties of Fluids Modelled by Force Fields with Intramolecular Contributions: Application to Heat Capacities,"  The calculation of caloric properties such as heat capacity, Joule-Thomson
coefficients and the speed of sound by classical force-field-based molecular
simulation methodology has received scant attention in the literature,
particularly for systems composed of complex molecules whose force fields (FFs)
are characterized by a combination of intramolecular and intermolecular terms
(referred to herein as ""flexible FFs""). The calculation of a thermodynamic
property for a system whose molecules are described by such a FF involves the
calculation of the residual property prior to its addition to the corresponding
ideal-gas (IG) property, the latter of which is separately calculated, either
using thermochemical compilations or nowadays accurate quantum mechanical
calculations. Although the simulation of a volumetric residual property
proceeds by simply replacing the intermolecular FF in the rigid molecule case
by the total (intramolecular plus intermolecular) FF, this is not the case for
a caloric property. We discuss the methodology required in performing such
calculations, and focus on the example of the molar heat capacity at constant
pressure, $c_P$, one of the most important caloric properties. We also consider
three approximations for the calculation procedure, and illustrate their
consequences for the examples of the relatively simple molecule 2-propanol,
${\rm CH_3CH(OH)CH_3}$, and for monoethanolamine, ${\rm HO(CH_2)_2NH_2}$, an
important fluid used in carbon capture.
",0,1,0,0,0,0
795,Excited states of defect lines in silicon: A first-principles study based on hydrogen cluster analogues,"  Excited states of a single donor in bulk silicon have previously been studied
extensively based on effective mass theory. However, a proper theoretical
description of the excited states of a donor cluster is still scarce. Here we
study the excitations of lines of defects within a single-valley spherical band
approximation, thus mapping the problem to a scaled hydrogen atom array. A
series of detailed full configuration-interaction and time-dependent hybrid
density-functional theory calculations have been performed to understand linear
clusters of up to 10 donors. Our studies illustrate the generic features of
their excited states, addressing the competition between formation of
inter-donor ionic states and intra-donor atomic excited states. At short
inter-donor distances, excited states of donor molecules are dominant, at
intermediate distances ionic states play an important role, and at long
distances the intra-donor excitations are predominant as expected. The
calculations presented here emphasise the importance of correlations between
donor electrons, and are thus complementary to other recent approaches that
include effective mass anisotropy and multi-valley effects. The exchange
splittings between relevant excited states have also been estimated for a donor
pair and for a three-donor arrays; the splittings are much larger than those in
the ground state in the range of donor separations between 10 and 20 nm. This
establishes a solid theoretical basis for the use of excited-state exchange
interactions for controllable quantum gate operations in silicon.
",0,1,0,0,0,0
485,Learning to Drive in a Day,"  We demonstrate the first application of deep reinforcement learning to
autonomous driving. From randomly initialised parameters, our model is able to
learn a policy for lane following in a handful of training episodes using a
single monocular image as input. We provide a general and easy to obtain
reward: the distance travelled by the vehicle without the safety driver taking
control. We use a continuous, model-free deep reinforcement learning algorithm,
with all exploration and optimisation performed on-vehicle. This demonstrates a
new framework for autonomous driving which moves away from reliance on defined
logical rules, mapping, and direct supervision. We discuss the challenges and
opportunities to scale this approach to a broader range of autonomous driving
tasks.
",1,0,0,1,0,0
814,New type integral inequalities for convex functions with applications II,"  We have recently established some integral inequalities for convex functions
via the Hermite-Hadamard's inequalities. In continuation here, we also
establish some interesting new integral inequalities for convex functions via
the Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful
applications involving special means are also included.
",0,0,1,0,0,0
2070,"Neurofeedback: principles, appraisal and outstanding issues","  Neurofeedback is a form of brain training in which subjects are fed back
information about some measure of their brain activity which they are
instructed to modify in a way thought to be functionally advantageous. Over the
last twenty years, NF has been used to treat various neurological and
psychiatric conditions, and to improve cognitive function in various contexts.
However, despite its growing popularity, each of the main steps in NF comes
with its own set of often covert assumptions. Here we critically examine some
conceptual and methodological issues associated with the way general objectives
and neural targets of NF are defined, and review the neural mechanisms through
which NF may act, and the way its efficacy is gauged. The NF process is
characterised in terms of functional dynamics, and possible ways in which it
may be controlled are discussed. Finally, it is proposed that improving NF will
require better understanding of various fundamental aspects of brain dynamics
and a more precise definition of functional brain activity and brain-behaviour
relationships.
",0,0,0,0,1,0
488,BiHom-Lie colour algebras structures,"  BiHom-Lie Colour algebra is a generalized Hom-Lie Colour algebra endowed with
two commuting multiplicative linear maps. The main purpose of this paper is to
define representations and a cohomology of BiHom-Lie colour algebras and to
study some key constructions and properties.
Moreover, we discuss $\alpha^{k}\beta^l$-generalized derivations,
$\alpha^{k}\beta^l$-quasi-derivations and $\alpha^{k}\beta^l$-quasi-centroid.
We provide some properties and their relationships with BiHom-Jordan colour
algebra.
",0,0,1,0,0,0
1352,Community structure detection and evaluation during the pre- and post-ictal hippocampal depth recordings,"  Detecting and evaluating regions of brain under various circumstances is one
of the most interesting topics in computational neuroscience. However, the
majority of the studies on detecting communities of a functional connectivity
network of the brain is done on networks obtained from coherency attributes,
and not from correlation. This lack of studies, in part, is due to the fact
that many common methods for clustering graphs require the nodes of the network
to be `positively' linked together, a property that is guaranteed by a
coherency matrix, by definition. However, correlation matrices reveal more
information regarding how each pair of nodes are linked together. In this
study, for the first time we simultaneously examine four inherently different
network clustering methods (spectral, heuristic, and optimization methods)
applied to the functional connectivity networks of the CA1 region of the
hippocampus of an anaesthetized rat during pre-ictal and post-ictal states. The
networks are obtained from correlation matrices, and its results are compared
with the ones obtained by applying the same methods to coherency matrices. The
correlation matrices show a much finer community structure compared to the
coherency matrices. Furthermore, we examine the potential smoothing effect of
choosing various window sizes for computing the correlation/coherency matrices.
",1,0,0,0,1,0
743,Simulating and Reconstructing Neurodynamics with Epsilon-Automata Applied to Electroencephalography (EEG) Microstate Sequences,"  We introduce new techniques to the analysis of neural spatiotemporal dynamics
via applying $\epsilon$-machine reconstruction to electroencephalography (EEG)
microstate sequences. Microstates are short duration quasi-stable states of the
dynamically changing electrical field topographies recorded via an array of
electrodes from the human scalp, and cluster into four canonical classes. The
sequence of microstates observed under particular conditions can be considered
an information source with unknown underlying structure. $\epsilon$-machines
are discrete dynamical system automata with state-dependent probabilities on
different future observations (in this case the next measured EEG microstate).
They artificially reproduce underlying structure in an optimally predictive
manner as generative models exhibiting dynamics emulating the behaviour of the
source. Here we present experiments using both simulations and empirical data
supporting the value of associating these discrete dynamical systems with
mental states (e.g. mind-wandering, focused attention, etc.) and with clinical
populations. The neurodynamics of mental states and clinical populations can
then be further characterized by properties of these dynamical systems,
including: i) statistical complexity (determined by the number of states of the
corresponding $\epsilon$-automaton); ii) entropy rate; iii) characteristic
sequence patterning (syntax, probabilistic grammars); iv) duration, persistence
and stability of dynamical patterns; and v) algebraic measures such as
Krohn-Rhodes complexity or holonomy length of the decompositions of these. The
potential applications include the characterization of mental states in
neurodynamic terms for mental health diagnostics, well-being interventions,
human-machine interface, and others on both subject-specific and
group/population-level.
",1,1,0,0,0,0
3720,General multilevel Monte Carlo methods for pricing discretely monitored Asian options,"  We describe general multilevel Monte Carlo methods that estimate the price of
an Asian option monitored at $m$ fixed dates. Our approach yields unbiased
estimators with standard deviation $O(\epsilon)$ in $O(m + (1/\epsilon)^{2})$
expected time for a variety of processes including the Black-Scholes model,
Merton's jump-diffusion model, the Square-Root diffusion model, Kou's double
exponential jump-diffusion model, the variance gamma and NIG exponential Levy
processes and, via the Milstein scheme, processes driven by scalar stochastic
differential equations. Using the Euler scheme, our approach estimates the
Asian option price with root mean square error $O(\epsilon)$ in
$O(m+(\ln(\epsilon)/\epsilon)^{2})$ expected time for processes driven by
multidimensional stochastic differential equations. Numerical experiments
confirm that our approach outperforms the conventional Monte Carlo method by a
factor of order $m$.
",0,0,0,0,0,1
898,Cosmological perturbation effects on gravitational-wave luminosity distance estimates,"  Waveforms of gravitational waves provide information about a variety of
parameters for the binary system merging. However, standard calculations have
been performed assuming a FLRW universe with no perturbations. In reality this
assumption should be dropped: we show that the inclusion of cosmological
perturbations translates into corrections to the estimate of astrophysical
parameters derived for the merging binary systems. We compute corrections to
the estimate of the luminosity distance due to velocity, volume, lensing and
gravitational potential effects. Our results show that the amplitude of the
corrections will be negligible for current instruments, mildly important for
experiments like the planned DECIGO, and very important for future ones such as
the Big Bang Observer.
",0,1,0,0,0,0
727,Photoelectron Yields of Scintillation Counters with Embedded Wavelength-Shifting Fibers Read Out With Silicon Photomultipliers,"  Photoelectron yields of extruded scintillation counters with titanium dioxide
coating and embedded wavelength shifting fibers read out by silicon
photomultipliers have been measured at the Fermilab Test Beam Facility using
120\,GeV protons. The yields were measured as a function of transverse,
longitudinal, and angular positions for a variety of scintillator compositions
and reflective coating mixtures, fiber diameters, and photosensor sizes. Timing
performance was also studied. These studies were carried out by the Cosmic Ray
Veto Group of the Mu2e collaboration as part of their R\&D program.
",0,1,0,0,0,0
873,Multi-Antenna Coded Caching,"  In this paper we consider a single-cell downlink scenario where a
multiple-antenna base station delivers contents to multiple cache-enabled user
terminals. Based on the multicasting opportunities provided by the so-called
Coded Caching technique, we investigate three delivery approaches. Our baseline
scheme employs the coded caching technique on top of max-min fair multicasting.
The second one consists of a joint design of Zero-Forcing (ZF) and coded
caching, where the coded chunks are formed in the signal domain (complex
field). The third scheme is similar to the second one with the difference that
the coded chunks are formed in the data domain (finite field). We derive
closed-form rate expressions where our results suggest that the latter two
schemes surpass the first one in terms of Degrees of Freedom (DoF). However, at
the intermediate SNR regime forming coded chunks in the signal domain results
in power loss, and will deteriorate throughput of the second scheme. The main
message of our paper is that the schemes performing well in terms of DoF may
not be directly appropriate for intermediate SNR regimes, and modified schemes
should be employed.
",1,0,1,0,0,0
8565,"Exploring the nuances in the relationship ""culture-strategy"" for the business world","  The current article explores interesting, significant and recently identified
nuances in the relationship ""culture-strategy"". The shared views of leading
scholars at the University of National and World Economy in relation with the
essence, direction, structure, role and hierarchy of ""culture-strategy""
relation are defined as a starting point of the analysis. The research emphasis
is directed on recent developments in interpreting the observed realizations of
the aforementioned link among the community of international scholars and
consultants, publishing in selected electronic scientific databases. In this
way a contemporary notion of the nature of ""culture-strategy"" relationship for
the entities from the world of business is outlined.
",0,0,0,0,0,1
285,Stochastic Gradient Monomial Gamma Sampler,"  Recent advances in stochastic gradient techniques have made it possible to
estimate posterior distributions from large datasets via Markov Chain Monte
Carlo (MCMC). However, when the target posterior is multimodal, mixing
performance is often poor. This results in inadequate exploration of the
posterior distribution. A framework is proposed to improve the sampling
efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A
generalized kinetic function is leveraged, delivering superior stationary
mixing, especially for multimodal distributions. Techniques are also discussed
to overcome the practical issues introduced by this generalization. It is shown
that the proposed approach is better at exploring complex multimodal posterior
distributions, as demonstrated on multiple applications and in comparison with
other stochastic gradient MCMC methods.
",1,0,0,1,0,0
226,Over Recurrence for Mixing Transformations,"  We show that every invertible strong mixing transformation on a Lebesgue
space has strictly over-recurrent sets. Also, we give an explicit procedure for
constructing strong mixing transformations with no under-recurrent sets. This
answers both parts of a question of V. Bergelson.
We define $\epsilon$-over-recurrence and show that given $\epsilon > 0$, any
ergodic measure preserving invertible transformation (including discrete
spectrum) has $\epsilon$-over-recurrent sets of arbitrarily small measure.
Discrete spectrum transformations and rotations do not have over-recurrent
sets, but we construct a weak mixing rigid transformation with strictly
over-recurrent sets.
",0,0,1,0,0,0
6741,Bivariate Causal Discovery and its Applications to Gene Expression and Imaging Data Analysis,"  The mainstream of research in genetics, epigenetics and imaging data analysis
focuses on statistical association or exploring statistical dependence between
variables. Despite their significant progresses in genetic research,
understanding the etiology and mechanism of complex phenotypes remains elusive.
Using association analysis as a major analytical platform for the complex data
analysis is a key issue that hampers the theoretic development of genomic
science and its application in practice. Causal inference is an essential
component for the discovery of mechanical relationships among complex
phenotypes. Many researchers suggest making the transition from association to
causation. Despite its fundamental role in science, engineering and
biomedicine, the traditional methods for causal inference require at least
three variables. However, quantitative genetic analysis such as QTL, eQTL,
mQTL, and genomic-imaging data analysis requires exploring the causal
relationships between two variables. This paper will focus on bivariate causal
discovery. We will introduce independence of cause and mechanism (ICM) as a
basic principle for causal inference, algorithmic information theory and
additive noise model (ANM) as major tools for bivariate causal discovery.
Large-scale simulations will be performed to evaluate the feasibility of the
ANM for bivariate causal discovery. To further evaluate their performance for
causal inference, the ANM will be applied to the construction of gene
regulatory networks. Also, the ANM will be applied to trait-imaging data
analysis to illustrate three scenarios: presence of both causation and
association, presence of association while absence of causation, and presence
of causation, while lack of association between two variables.
",0,0,0,0,1,0
2268,Estimation of Covariance Matrices for Portfolio Optimization using Gaussian Processes,"  Estimating covariances between financial assets plays an important role in
risk management and optimal portfolio allocation. In practice, when the sample
size is small compared to the number of variables, i.e. when considering a wide
universe of assets over just a few years, this poses considerable challenges
and the empirical estimate is known to be very unstable.
Here, we propose a novel covariance estimator based on the Gaussian Process
Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear
extension of standard factor models with readily interpretable parameters
reminiscent of market betas. Furthermore, our Bayesian treatment naturally
shrinks the sample covariance matrix towards a more structured matrix given by
the prior and thereby systematically reduces estimation errors.
",0,0,0,0,0,1
6644,Measuring Systematic Risk with Neural Network Factor Model,"  In this paper, we measure systematic risk with a new nonparametric factor
model, the neural network factor model. The suitable factors for systematic
risk can be naturally found by inserting daily returns on a wide range of
assets into the bottleneck network. The network-based model does not stick to a
probabilistic structure unlike parametric factor models, and it does not need
feature engineering because it selects notable features by itself. In addition,
we compare performance between our model and the existing models using 20-year
data of S&P 100 components. Although the new model can not outperform the best
ones among the parametric factor models due to limitations of the variational
inference, the estimation method used for this study, it is still noteworthy in
that it achieves the performance as best the comparable models could without
any prior knowledge.
",0,0,0,0,0,1
683,An Unsupervised Learning Classifier with Competitive Error Performance,"  An unsupervised learning classification model is described. It achieves
classification error probability competitive with that of popular supervised
learning classifiers such as SVM or kNN. The model is based on the incremental
execution of small step shift and rotation operations upon selected
discriminative hyperplanes at the arrival of input samples. When applied, in
conjunction with a selected feature extractor, to a subset of the ImageNet
dataset benchmark, it yields 6.2 % Top 3 probability of error; this exceeds by
merely about 2 % the result achieved by (supervised) k-Nearest Neighbor, both
using same feature extractor. This result may also be contrasted with popular
unsupervised learning schemes such as k-Means which is shown to be practically
useless on same dataset.
",0,0,0,1,0,0
1400,Estimation of Relationship between Stimulation Current and Force Exerted during Isometric Contraction,"  In this study, we developed a method to estimate the relationship between
stimulation current and volatility during isometric contraction. In functional
electrical stimulation (FES), joints are driven by applying voltage to muscles.
This technology has been used for a long time in the field of rehabilitation,
and recently application oriented research has been reported. However,
estimation of the relationship between stimulus value and exercise capacity has
not been discussed to a great extent. Therefore, in this study, a human muscle
model was estimated using the transfer function estimation method with fast
Fourier transform. It was found that the relationship between stimulation
current and force exerted could be expressed by a first-order lag system. In
verification of the force estimate, the ability of the proposed model to
estimate the exerted force under steady state response was found to be good.
",0,0,0,0,1,0
832,OSIRIS-REx Contamination Control Strategy and Implementation,"  OSIRIS-REx will return pristine samples of carbonaceous asteroid Bennu. This
article describes how pristine was defined based on expectations of Bennu and
on a realistic understanding of what is achievable with a constrained schedule
and budget, and how that definition flowed to requirements and implementation.
To return a pristine sample, the OSIRIS- REx spacecraft sampling hardware was
maintained at level 100 A/2 and <180 ng/cm2 of amino acids and hydrazine on the
sampler head through precision cleaning, control of materials, and vigilance.
Contamination is further characterized via witness material exposed to the
spacecraft assembly and testing environment as well as in space. This
characterization provided knowledge of the expected background and will be used
in conjunction with archived spacecraft components for comparison with the
samples when they are delivered to Earth for analysis. Most of all, the
cleanliness of the OSIRIS-REx spacecraft was achieved through communication
among scientists, engineers, managers, and technicians.
",0,1,0,0,0,0
740,Global and local thermometry schemes in coupled quantum systems,"  We study the ultimate bounds on the estimation of temperature for an
interacting quantum system. We consider two coupled bosonic modes that are
assumed to be thermal and using quantum estimation theory establish the role
the Hamiltonian parameters play in thermometry. We show that in the case of a
conserved particle number the interaction between the modes leads to a decrease
in the overall sensitivity to temperature, while interestingly, if particle
exchange is allowed with the thermal bath the converse is true. We explain this
dichotomy by examining the energy spectra. Finally, we devise experimentally
implementable thermometry schemes that rely only on locally accessible
information from the total system, showing that almost Heisenberg limited
precision can still be achieved, and we address the (im)possibility for
multiparameter estimation in the system.
",0,1,0,0,0,0
602,Subset Labeled LDA for Large-Scale Multi-Label Classification,"  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard
unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address
multi-label learning tasks. Previous work has shown it to perform in par with
other state-of-the-art multi-label methods. Nonetheless, with increasing label
sets sizes LLDA encounters scalability issues. In this work, we introduce
Subset LLDA, a simple variant of the standard LLDA algorithm, that not only can
effectively scale up to problems with hundreds of thousands of labels but also
improves over the LLDA state-of-the-art. We conduct extensive experiments on
eight data sets, with label sets sizes ranging from hundreds to hundreds of
thousands, comparing our proposed algorithm with the previously proposed LLDA
algorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme
multi-label classification. The results show a steady advantage of our method
over the other LLDA algorithms and competitive results compared to the extreme
multi-label classification algorithms.
",0,0,0,1,0,0
4504,Large-scale validation of an automatic EEG arousal detection algorithm using different heterogeneous databases,"  $\textbf{Objective}$: To assess the validity of an automatic EEG arousal
detection algorithm using large patient samples and different heterogeneous
databases
$\textbf{Methods}$: Automatic scorings were confronted with results from
human expert scorers on a total of 2768 full-night PSG recordings obtained from
two different databases. Of them, 472 recordings were obtained during clinical
routine at our sleep center and were subdivided into two subgroups of 220
(HMC-S) and 252 (HMC-M) recordings each, attending to the procedure followed by
the clinical expert during the visual review (semi-automatic or purely manual,
respectively). In addition, 2296 recordings from the public SHHS-2 database
were evaluated against the respective manual expert scorings.
$\textbf{Results}$: Event-by-event epoch-based validation resulted in an
overall Cohen kappa agreement K = 0.600 (HMC-S), 0.559 (HMC-M), and 0.573
(SHHS-2). Estimated inter-scorer variability on the datasets was, respectively,
K = 0.594, 0.561 and 0.543. Analyses of the corresponding Arousal Index scores
showed associated automatic-human repeatability indices ranging in 0.693-0.771
(HMC-S), 0.646-0.791 (HMC-M), and 0.759-0.791 (SHHS-2).
$\textbf{Conclusions}$: Large-scale validation of our automatic EEG arousal
detector on different databases has shown robust performance and good
generalization results comparable to the expected levels of human agreement.
Special emphasis has been put on allowing reproducibility of the results and
implementation of our method has been made accessible online as open source
code
",0,0,0,0,1,0
14236,Erratum: Higher Order Elicitability and Osband's Principle,"  This note corrects conditions in Proposition 3.4 and Theorem 5.2(ii) and
comments on imprecisions in Propositions 4.2 and 4.4 in Fissler and Ziegel
(2016).
",0,0,1,1,0,1
616,Efficient sampling of conditioned Markov jump processes,"  We consider the task of generating draws from a Markov jump process (MJP)
between two time points at which the process is known. Resulting draws are
typically termed bridges and the generation of such bridges plays a key role in
simulation-based inference algorithms for MJPs. The problem is challenging due
to the intractability of the conditioned process, necessitating the use of
computationally intensive methods such as weighted resampling or Markov chain
Monte Carlo. An efficient implementation of such schemes requires an
approximation of the intractable conditioned hazard/propensity function that is
both cheap and accurate. In this paper, we review some existing approaches to
this problem before outlining our novel contribution. Essentially, we leverage
the tractability of a Gaussian approximation of the MJP and suggest a
computationally efficient implementation of the resulting conditioned hazard
approximation. We compare and contrast our approach with existing methods using
three examples.
",0,0,0,1,0,0
802,Attention-Based Guided Structured Sparsity of Deep Neural Networks,"  Network pruning is aimed at imposing sparsity in a neural network
architecture by increasing the portion of zero-valued weights for reducing its
size regarding energy-efficiency consideration and increasing evaluation speed.
In most of the conducted research efforts, the sparsity is enforced for network
pruning without any attention to the internal network characteristics such as
unbalanced outputs of the neurons or more specifically the distribution of the
weights and outputs of the neurons. That may cause severe accuracy drop due to
uncontrolled sparsity. In this work, we propose an attention mechanism that
simultaneously controls the sparsity intensity and supervised network pruning
by keeping important information bottlenecks of the network to be active. On
CIFAR-10, the proposed method outperforms the best baseline method by 6% and
reduced the accuracy drop by 2.6x at the same level of sparsity.
",0,0,0,1,0,0
780,Learning Combinations of Sigmoids Through Gradient Estimation,"  We develop a new approach to learn the parameters of regression models with
hidden variables. In a nutshell, we estimate the gradient of the regression
function at a set of random points, and cluster the estimated gradients. The
centers of the clusters are used as estimates for the parameters of hidden
units. We justify this approach by studying a toy model, whereby the regression
function is a linear combination of sigmoids. We prove that indeed the
estimated gradients concentrate around the parameter vectors of the hidden
units, and provide non-asymptotic bounds on the number of required samples. To
the best of our knowledge, no comparable guarantees have been proven for linear
combinations of sigmoids.
",1,0,0,1,0,0
574,Improved Quantile Regression Estimators when the Errors are Independently and Non-identically Distributed,"  In a classical regression model, it is usually assumed that the explanatory
variables are independent of each other and error terms are normally
distributed. But when these assumptions are not met, situations like the error
terms are not independent or they are not identically distributed or both of
these, LSE will not be robust. Hence, quantile regression has been used to
complement this deficiency of classical regression analysis and to improve the
least square estimation (LSE). In this study, we consider preliminary test and
shrinkage estimation strategies for quantile regression models with
independently and non-identically distributed (i.ni.d.) errors. A Monte Carlo
simulation study is conducted to assess the relative performance of the
estimators. Also, we numerically compare their performance with Ridge, Lasso,
Elastic Net penalty estimation strategies. A real data example is presented to
illustrate the usefulness of the suggested methods. Finally, we obtain the
asymptotic results of suggested estimators
",0,0,1,1,0,0
2055,Bounds on the expected size of the maximum agreement subtree for a given tree shape,"  We show that the expected size of the maximum agreement subtree of two
$n$-leaf trees, uniformly random among all trees with the shape, is
$\Theta(\sqrt{n})$. To derive the lower bound, we prove a global structural
result on a decomposition of rooted binary trees into subgroups of leaves
called blobs. To obtain the upper bound, we generalize a first moment argument
for random tree distributions that are exchangeable and not necessarily
sampling consistent.
",0,0,0,0,1,0
404,Towards exascale real-time RFI mitigation,"  We describe the design and implementation of an extremely scalable real-time
RFI mitigation method, based on the offline AOFlagger. All algorithms scale
linearly in the number of samples. We describe how we implemented the flagger
in the LOFAR real-time pipeline, on both CPUs and GPUs. Additionally, we
introduce a novel simple history-based flagger that helps reduce the impact of
our small window on the data.
By examining an observation of a known pulsar, we demonstrate that our
flagger can achieve much higher quality than a simple thresholder, even when
running in real time, on a distributed system. The flagger works on visibility
data, but also on raw voltages, and beam formed data. The algorithms are
scale-invariant, and work on microsecond to second time scales. We are
currently implementing a prototype for the time domain pipeline of the SKA
central signal processor.
",0,1,0,0,0,0
893,On perpetuities with gamma-like tails,"  An infinite convergent sum of independent and identically distributed random
variables discounted by a multiplicative random walk is called perpetuity,
because of a possible actuarial application. We give three disjoint groups of
sufficient conditions which ensure that the distribution right tail of a
perpetuity $\mathbb{P}\{X>x\}$ is asymptotic to $ax^ce^{-bx}$ as $x\to\infty$
for some $a,b>0$ and $c\in\mathbb{R}$. Our results complement those of Denisov
and Zwart [J. Appl. Probab. 44 (2007), 1031--1046]. As an auxiliary tool we
provide criteria for the finiteness of the one-sided exponential moments of
perpetuities. Several examples are given in which the distributions of
perpetuities are explicitly identified.
",0,0,1,0,0,0
221,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,"  Generative Adversarial Networks (GANs) excel at creating realistic images
with complex models for which maximum likelihood is infeasible. However, the
convergence of GAN training has still not been proved. We propose a two
time-scale update rule (TTUR) for training GANs with stochastic gradient
descent on arbitrary GAN loss functions. TTUR has an individual learning rate
for both the discriminator and the generator. Using the theory of stochastic
approximation, we prove that the TTUR converges under mild assumptions to a
stationary local Nash equilibrium. The convergence carries over to the popular
Adam optimization, for which we prove that it follows the dynamics of a heavy
ball with friction and thus prefers flat minima in the objective landscape. For
the evaluation of the performance of GANs at image generation, we introduce the
""Fréchet Inception Distance"" (FID) which captures the similarity of generated
images to real ones better than the Inception Score. In experiments, TTUR
improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
Bedrooms, and the One Billion Word Benchmark.
",1,0,0,1,0,0
489,Clustering of Gamma-Ray bursts through kernel principal component analysis,"  We consider the problem related to clustering of gamma-ray bursts (from
""BATSE"" catalogue) through kernel principal component analysis in which our
proposed kernel outperforms results of other competent kernels in terms of
clustering accuracy and we obtain three physically interpretable groups of
gamma-ray bursts. The effectivity of the suggested kernel in combination with
kernel principal component analysis in revealing natural clusters in noisy and
nonlinear data while reducing the dimension of the data is also explored in two
simulated data sets.
",0,1,0,1,0,0
508,Dynamic Shrinkage Processes,"  We propose a novel class of dynamic shrinkage processes for Bayesian time
series and regression analysis. Building upon a global-local framework of prior
construction, in which continuous scale mixtures of Gaussian distributions are
employed for both desirable shrinkage properties and computational
tractability, we model dependence among the local scale parameters. The
resulting processes inherit the desirable shrinkage behavior of popular
global-local priors, such as the horseshoe prior, but provide additional
localized adaptivity, which is important for modeling time series data or
regression functions with local features. We construct a computationally
efficient Gibbs sampling algorithm based on a Pólya-Gamma scale mixture
representation of the proposed process. Using dynamic shrinkage processes, we
develop a Bayesian trend filtering model that produces more accurate estimates
and tighter posterior credible intervals than competing methods, and apply the
model for irregular curve-fitting of minute-by-minute Twitter CPU usage data.
In addition, we develop an adaptive time-varying parameter regression model to
assess the efficacy of the Fama-French five-factor asset pricing model with
momentum added as a sixth factor. Our dynamic analysis of manufacturing and
healthcare industry data shows that with the exception of the market risk, no
other risk factors are significant except for brief periods.
",0,0,0,1,0,0
477,A Game of Life on Penrose tilings,"  We define rules for cellular automata played on quasiperiodic tilings of the
plane arising from the multigrid method in such a way that these cellular
automata are isomorphic to Conway's Game of Life. Although these tilings are
nonperiodic, determining the next state of each tile is a local computation,
requiring only knowledge of the local structure of the tiling and the states of
finitely many nearby tiles. As an example, we show a version of a ""glider""
moving through a region of a Penrose tiling. This constitutes a potential
theoretical framework for a method of executing computations in
non-periodically structured substrates such as quasicrystals.
",0,1,1,0,0,0
211,An Effective Way to Improve YouTube-8M Classification Accuracy in Google Cloud Platform,"  Large-scale datasets have played a significant role in progress of neural
network and deep learning areas. YouTube-8M is such a benchmark dataset for
general multi-label video classification. It was created from over 7 million
YouTube videos (450,000 hours of video) and includes video labels from a
vocabulary of 4716 classes (3.4 labels/video on average). It also comes with
pre-extracted audio & visual features from every second of video (3.2 billion
feature vectors in total). Google cloud recently released the datasets and
organized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.
Competitors are challenged to develop classification algorithms that assign
video-level labels using the new and improved Youtube-8M V2 dataset. Inspired
by the competition, we started exploration of audio understanding and
classification using deep learning algorithms and ensemble methods. We built
several baseline predictions according to the benchmark paper and public github
tensorflow code. Furthermore, we improved global prediction accuracy (GAP) from
base level 77% to 80.7% through approaches of ensemble.
",1,0,0,1,0,0
751,Analytic heating rate of neutron star merger ejecta derived from Fermi's theory of beta decay,"  Macronovae (kilonovae) that arise in binary neutron star mergers are powered
by radioactive beta decay of hundreds of $r$-process nuclides. We derive, using
Fermi's theory of beta decay, an analytic estimate of the nuclear heating rate.
We show that the heating rate evolves as a power law ranging between $t^{-6/5}$
to $t^{-4/3}$. The overall magnitude of the heating rate is determined by the
mean values of nuclear quantities, e.g., the nuclear matrix elements of beta
decay. These values are specified by using nuclear experimental data. We
discuss the role of higher order beta transitions and the robustness of the
power law. The robust and simple form of the heating rate suggests that
observations of the late-time bolometric light curve $\propto t^{-\frac{4}{3}}$
would be a direct evidence of a $r$-process driven macronova. Such observations
could also enable us to estimate the total amount of $r$-process nuclei
produced in the merger.
",0,1,0,0,0,0
625,"Coarse-grained simulation of auxetic, two-dimensional crystal dynamics","  The increasing number of protein-based metamaterials demands reliable and
efficient methods to study the physicochemical properties they may display. In
this regard, we develop a simulation strategy based on Molecular Dynamics (MD)
that addresses the geometric degrees of freedom of an auxetic two-dimensional
protein crystal. This model consists of a network of impenetrable rigid squares
linked through massless rigid rods, thus featuring a large number of both
holonomic and nonholonomic constraints. Our MD methodology is optimized to
study highly constrained systems and allows for the simulation of long-time
dynamics with reasonably large timesteps. The data extracted from the
simulations shows a persistent motional interdependence among the protein
subunits in the crystal. We characterize the dynamical correlations featured by
these subunits and identify two regimes characterized by their locality or
nonlocality, depending on the geometric parameters of the crystal. From the
same data, we also calculate the Poisson\rq{}s (longitudinal to axial strain)
ratio of the crystal, and learn that, due to holonomic constraints (rigidness
of the rod links), the crystal remains auxetic even after significant changes
in the original geometry. The nonholonomic ones (collisions between subunits)
increase the number of inhomogeneous deformations of the crystal, thus driving
it away from an isotropic response. Our work provides the first simulation of
the dynamics of protein crystals and offers insights into promising mechanical
properties afforded by these materials.
",0,1,0,0,0,0
3062,Interactions mediated by a public good transiently increase cooperativity in growing Pseudomonas putida metapopulations,"  Bacterial communities have rich social lives. A well-established interaction
involves the exchange of a public good in Pseudomonas populations, where the
iron-scavenging compound pyoverdine, synthesized by some cells, is shared with
the rest. Pyoverdine thus mediates interactions between producers and
non-producers and can constitute a public good. This interaction is often used
to test game theoretical predictions on the ""social dilemma"" of producers. Such
an approach, however, underestimates the impact of specific properties of the
public good, for example consequences of its accumulation in the environment.
Here, we experimentally quantify costs and benefits of pyoverdine production in
a specific environment, and build a model of population dynamics that
explicitly accounts for the changing significance of accumulating pyoverdine as
chemical mediator of social interactions. The model predicts that, in an
ensemble of growing populations (metapopulation) with different initial
producer fractions (and consequently pyoverdine contents), the global producer
fraction initially increases. Because the benefit of pyoverdine declines at
saturating concentrations, the increase need only be transient. Confirmed by
experiments on metapopulations, our results show how a changing benefit of a
public good can shape social interactions in a bacterial population.
",0,0,0,0,1,0
361,XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification,"  We propose two multimodal deep learning architectures that allow for
cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
more interpretable features and obtaining a better representation than through
unimodal learning, for the same amount of training data. These models can
usefully exploit correlations between audio and visual data, which have a
different dimensionality and are therefore nontrivially exchangeable. Our work
improves on existing multimodal deep learning metholodogies in two essential
ways: (1) it presents a novel method for performing cross-modality (before
features are learned from individual modalities) and (2) extends the previously
proposed cross-connections, which only transfer information between streams
that process compatible data. Both cross-modal architectures outperformed their
baselines (by up to 7.5%) when evaluated on the AVletters dataset.
",1,0,0,1,0,0
6820,Quantification of market efficiency based on informational-entropy,"  Since the 1960s, the question whether markets are efficient or not is
controversially discussed. One reason for the difficulty to overcome the
controversy is the lack of a universal, but also precise, quantitative
definition of efficiency that is able to graduate between different states of
efficiency. The main purpose of this article is to fill this gap by developing
a measure for the efficiency of markets that fulfill all the stated
requirements. It is shown that the new definition of efficiency, based on
informational-entropy, is equivalent to the two most used definitions of
efficiency from Fama and Jensen. The new measure therefore enables steps to
settle the dispute over the state of efficiency in markets. Moreover, it is
shown that inefficiency in a market can either arise from the possibility to
use information to predict an event with higher than chance level, or can
emerge from wrong pricing/ quotes that do not reflect the right probabilities
of possible events. Finally, the calculation of efficiency is demonstrated on a
simple game (of coin tossing), to show how one could exactly quantify the
efficiency in any market-like system, if all probabilities are known.
",0,0,0,0,0,1
313,Sparse Neural Networks Topologies,"  We propose Sparse Neural Network architectures that are based on random or
structured bipartite graph topologies. Sparse architectures provide compression
of the models learned and speed-ups of computations, they can also surpass
their unstructured or fully connected counterparts. As we show, even more
compact topologies of the so-called SNN (Sparse Neural Network) can be achieved
with the use of structured graphs of connections between consecutive layers of
neurons. In this paper, we investigate how the accuracy and training speed of
the models depend on the topology and sparsity of the neural network. Previous
approaches using sparcity are all based on fully connected neural network
models and create sparcity during training phase, instead we explicitly define
a sparse architectures of connections before the training. Building compact
neural network models is coherent with empirical observations showing that
there is much redundancy in learned neural network models. We show
experimentally that the accuracy of the models learned with neural networks
depends on expander-like properties of the underlying topologies such as the
spectral gap and algebraic connectivity rather than the density of the graphs
of connections.
",1,0,0,1,0,0
366,From mindless mathematics to thinking meat?,"  Deconstruction of the theme of the 2017 FQXi essay contest is already an
interesting exercise in its own right: Teleology is rarely useful in physics
--- the only known mainstream physics example (black hole event horizons) has a
very mixed score-card --- so the ""goals"" and ""aims and intentions"" alluded to
in the theme of the 2017 FQXi essay contest are already somewhat pushing the
limits. Furthermore, ""aims and intentions"" certainly carries the implication of
consciousness, and opens up a whole can of worms related to the mind-body
problem. As for ""mindless mathematical laws"", that allusion is certainly in
tension with at least some versions of the ""mathematical universe hypothesis"".
Finally ""wandering towards a goal"" again carries the implication of
consciousness, with all its attendant problems.
In this essay I will argue, simply because we do not yet have any really good
mathematical or physical theory of consciousness, that the theme of this essay
contest is premature, and unlikely to lead to any resolution that would be
widely accepted in the mathematics or physics communities.
",0,1,0,0,0,0
6426,Image-derived generative modeling of pseudo-macromolecular structures - towards the statistical assessment of Electron CryoTomography template matching,"  Cellular Electron CryoTomography (CECT) is a 3D imaging technique that
captures information about the structure and spatial organization of
macromolecular complexes within single cells, in near-native state and at
sub-molecular resolution. Although template matching is often used to locate
macromolecules in a CECT image, it is insufficient as it only measures the
relative structural similarity. Therefore, it is preferable to assess the
statistical credibility of the decision through hypothesis testing, requiring
many templates derived from a diverse population of macromolecular structures.
Due to the very limited number of known structures, we need a generative model
to efficiently and reliably sample pseudo-structures from the complex
distribution of macromolecular structures. To address this challenge, we
propose a novel image-derived approach for performing hypothesis testing for
template matching by constructing generative models using the generative
adversarial network. Finally, we conducted hypothesis testing experiments for
template matching on both simulated and experimental subtomograms, allowing us
to conclude the identity of subtomograms with high statistical credibility and
significantly reducing false positives.
",0,0,0,1,1,0
410,J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation,"  In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model.
",1,0,0,0,0,0
837,When flux standards go wild: white dwarfs in the age of Kepler,"  White dwarf stars have been used as flux standards for decades, thanks to
their staid simplicity. We have empirically tested their photometric stability
by analyzing the light curves of 398 high-probability candidates and
spectroscopically confirmed white dwarfs observed during the original Kepler
mission and later with K2 Campaigns 0-8. We find that the vast majority (>97
per cent) of non-pulsating and apparently isolated white dwarfs are stable to
better than 1 per cent in the Kepler bandpass on 1-hr to 10-d timescales,
confirming that these stellar remnants are useful flux standards. From the
cases that do exhibit significant variability, we caution that binarity,
magnetism, and pulsations are three important attributes to rule out when
establishing white dwarfs as flux standards, especially those hotter than
30,000 K.
",0,1,0,0,0,0
353,Qualification Conditions in Semi-algebraic Programming,"  For an arbitrary finite family of semi-algebraic/definable functions, we
consider the corresponding inequality constraint set and we study qualification
conditions for perturbations of this set. In particular we prove that all
positive diagonal perturbations, save perhaps a finite number of them, ensure
that any point within the feasible set satisfies Mangasarian-Fromovitz
constraint qualification. Using the Milnor-Thom theorem, we provide a bound for
the number of singular perturbations when the constraints are polynomial
functions. Examples show that the order of magnitude of our exponential bound
is relevant. Our perturbation approach provides a simple protocol to build
sequences of ""regular"" problems approximating an arbitrary
semi-algebraic/definable problem. Applications to sequential quadratic
programming methods and sum of squares relaxation are provided.
",0,0,1,0,0,0
271,Multilevel maximum likelihood estimation with application to covariance matrices,"  The asymptotic variance of the maximum likelihood estimate is proved to
decrease when the maximization is restricted to a subspace that contains the
true parameter value. Maximum likelihood estimation allows a systematic fitting
of covariance models to the sample, which is important in data assimilation.
The hierarchical maximum likelihood approach is applied to the spectral
diagonal covariance model with different parameterizations of eigenvalue decay,
and to the sparse inverse covariance model with specified parameter values on
different sets of nonzero entries. It is shown computationally that using
smaller sets of parameters can decrease the sampling noise in high dimension
substantially.
",0,0,1,1,0,0
723,Completely $p$-primitive binary quadratic forms,"  Let $f(x,y)=ax^2+bxy+cy^2$ be a binary quadratic form with integer
coefficients. For a prime $p$ not dividing the discriminant of $f$, we say $f$
is completely $p$-primitive if for any non-zero integer $N$, the diophantine
equation $f(x,y)=N$ has always an integer solution $(x,y)=(m,n)$ with
$(m,n,p)=1$ whenever it has an integer solution. In this article, we study
various properties of completely $p$-primitive binary quadratic forms. In
particular, we give a necessary and sufficient condition for a definite binary
quadratic form $f$ to be completely $p$-primitive.
",0,0,1,0,0,0
789,Perturbation theory for cosmologies with non-linear structure,"  The next generation of cosmological surveys will operate over unprecedented
scales, and will therefore provide exciting new opportunities for testing
general relativity. The standard method for modelling the structures that these
surveys will observe is to use cosmological perturbation theory for linear
structures on horizon-sized scales, and Newtonian gravity for non-linear
structures on much smaller scales. We propose a two-parameter formalism that
generalizes this approach, thereby allowing interactions between large and
small scales to be studied in a self-consistent and well-defined way. This uses
both post-Newtonian gravity and cosmological perturbation theory, and can be
used to model realistic cosmological scenarios including matter, radiation and
a cosmological constant. We find that the resulting field equations can be
written as a hierarchical set of perturbation equations. At leading-order,
these equations allow us to recover a standard set of Friedmann equations, as
well as a Newton-Poisson equation for the inhomogeneous part of the Newtonian
energy density in an expanding background. For the perturbations in the
large-scale cosmology, however, we find that the field equations are sourced by
both non-linear and mode-mixing terms, due to the existence of small-scale
structures. These extra terms should be expected to give rise to new
gravitational effects, through the mixing of gravitational modes on small and
large scales - effects that are beyond the scope of standard linear
cosmological perturbation theory. We expect our formalism to be useful for
accurately modelling gravitational physics in universes that contain non-linear
structures, and for investigating the effects of non-linear gravity in the era
of ultra-large-scale surveys.
",0,1,0,0,0,0
3114,Towards Neural Co-Processors for the Brain: Combining Decoding and Encoding in Brain-Computer Interfaces,"  The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a ""co-processor"" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These ""neural co-processors"" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.
",0,0,0,0,1,0
351,The vortex method for 2D ideal flows in the exterior of a disk,"  The vortex method is a common numerical and theoretical approach used to
implement the motion of an ideal flow, in which the vorticity is approximated
by a sum of point vortices, so that the Euler equations read as a system of
ordinary differential equations. Such a method is well justified in the full
plane, thanks to the explicit representation formulas of Biot and Savart. In an
exterior domain, we also replace the impermeable boundary by a collection of
point vortices generating the circulation around the obstacle. The density of
these point vortices is chosen in order that the flow remains tangent at
midpoints between adjacent vortices. In this work, we provide a rigorous
justification for this method in exterior domains. One of the main mathematical
difficulties being that the Biot-Savart kernel defines a singular integral
operator when restricted to a curve. For simplicity and clarity, we only treat
the case of the unit disk in the plane approximated by a uniformly distributed
mesh of point vortices. The complete and general version of our work is
available in [arXiv:1707.01458].
",0,0,1,0,0,0
778,Recursive Multikernel Filters Exploiting Nonlinear Temporal Structure,"  In kernel methods, temporal information on the data is commonly included by
using time-delayed embeddings as inputs. Recently, an alternative formulation
was proposed by defining a gamma-filter explicitly in a reproducing kernel
Hilbert space, giving rise to a complex model where multiple kernels operate on
different temporal combinations of the input signal. In the original
formulation, the kernels are then simply combined to obtain a single kernel
matrix (for instance by averaging), which provides computational benefits but
discards important information on the temporal structure of the signal.
Inspired by works on multiple kernel learning, we overcome this drawback by
considering the different kernels separately. We propose an efficient strategy
to adaptively combine and select these kernels during the training phase. The
resulting batch and online algorithms automatically learn to process highly
nonlinear temporal information extracted from the input signal, which is
implicitly encoded in the kernel values. We evaluate our proposal on several
artificial and real tasks, showing that it can outperform classical approaches
both in batch and online settings.
",1,0,0,1,0,0
4431,A Brain-Inspired Trust Management Model to Assure Security in a Cloud based IoT Framework for Neuroscience Applications,"  Rapid popularity of Internet of Things (IoT) and cloud computing permits
neuroscientists to collect multilevel and multichannel brain data to better
understand brain functions, diagnose diseases, and devise treatments. To ensure
secure and reliable data communication between end-to-end (E2E) devices
supported by current IoT and cloud infrastructure, trust management is needed
at the IoT and user ends. This paper introduces a Neuro-Fuzzy based
Brain-inspired trust management model (TMM) to secure IoT devices and relay
nodes, and to ensure data reliability. The proposed TMM utilizes node
behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference
System and weighted-additive methods respectively to assess the nodes
trustworthiness. In contrast to the existing fuzzy based TMMs, the NS2
simulation results confirm the robustness and accuracy of the proposed TMM in
identifying malicious nodes in the communication network. With the growing
usage of cloud based IoT frameworks in Neuroscience research, integrating the
proposed TMM into the existing infrastructure will assure secure and reliable
data communication among the E2E devices.
",0,0,0,0,1,0
16315,Pricing of debt and equity in a financial network with comonotonic endowments,"  In this paper we present formulas for the valuation of debt and equity of
firms in a financial network under comonotonic endowments. We demonstrate that
the comonotonic setting provides a lower bound to the price of debt under
Eisenberg-Noe financial networks with consistent marginal endowments. Such
financial networks encode the interconnection of firms through debt claims. The
proposed pricing formulas consider the realized, endogenous, recovery rate on
debt claims. Special consideration will be given to the setting in which firms
only invest in a risk-free bond and a common risky asset following a geometric
Brownian motion.
",0,0,0,0,0,1
656,Evolutionary dynamics of N-person Hawk-Dove games,"  In the animal world, the competition between individuals belonging to
different species for a resource often requires the cooperation of several
individuals in groups. This paper proposes a generalization of the Hawk-Dove
Game for an arbitrary number of agents: the N-person Hawk-Dove Game. In this
model, doves exemplify the cooperative behavior without intraspecies conflict,
while hawks represent the aggressive behavior. In the absence of hawks, doves
share the resource equally and avoid conflict, but having hawks around lead to
doves escaping without fighting. Conversely, hawks fight for the resource at
the cost of getting injured. Nevertheless, if doves are present in sufficient
number to expel the hawks, they can aggregate to protect the resource, and thus
avoid being plundered by hawks. We derive and numerically solve an exact
equation for the evolution of the system in both finite and infinite well-mixed
populations, finding the conditions for stable coexistence between both
species. Furthermore, by varying the different parameters, we found a scenario
of bifurcations that leads the system from dominating hawks and coexistence to
bi-stability, multiple interior equilibria and dominating doves.
",0,1,0,0,0,0
2696,Early Salient Region Selection Does Not Drive Rapid Visual Categorization,"  The current dominant visual processing paradigm in both human and machine
research is the feedforward, layered hierarchy of neural-like processing
elements. Within this paradigm, visual saliency is seen by many to have a
specific role, namely that of early selection. Early selection is thought to
enable very fast visual performance by limiting processing to only the most
relevant candidate portions of an image. Though this strategy has indeed led to
improved processing time efficiency in machine algorithms, at least one set of
critical tests of this idea has never been performed with respect to the role
of early selection in human vision. How would the best of the current saliency
models perform on the stimuli used by experimentalists who first provided
evidence for this visual processing paradigm? Would the algorithms really
provide correct candidate sub-images to enable fast categorization on those
same images? Here, we report on a new series of tests of these questions whose
results suggest that it is quite unlikely that such an early selection process
has any role in human rapid visual categorization.
",0,0,0,0,1,0
3123,Tonic activation of extrasynaptic NMDA receptors decreases intrinsic excitability and promotes bistability in a model of neuronal activity,"  NMDA receptors (NMDA-R) typically contribute to excitatory synaptic
transmission in the central nervous system. While calcium influx through NMDA-R
plays a critical role in synaptic plasticity, indirect experimental evidence
also exists demonstrating actions of NMDAR-mediated calcium influx on neuronal
excitability through the activation of calcium-activated potassium channels.
But, so far, this mechanism has not been studied theoretically. Our theoretical
model provide a simple description of neuronal electrical activity including
the tonic activity of NMDA receptors and a cytosolic calcium compartment. We
show that calcium influx through NMDA-R can directly be coupled to activation
of calcium-activated potassium channels providing an overall inhibitory effect
on neuronal excitability. Furthermore, the presence of tonic NMDA-R activity
promotes bistability in electrical activity by dramatically increasing the
stimulus interval where both a stable steady state and repetitive firing can
exist. This results could provide an intrinsic mechanism for the constitution
of memory traces in neuronal circuits. They also shed light on the way by which
beta-amyloids can decrease neuronal activity when interfering with NMDA-R in
Alzheimer's disease.
",0,0,0,0,1,0
945,Total-positivity preservers,"  We prove that the only entrywise transforms of rectangular matrices which
preserve total positivity or total non-negativity are either constant or
linear. This follows from an extended classification of preservers of these two
properties for matrices of fixed dimension. We also prove that the same
assertions hold upon working only with symmetric matrices; for total-positivity
preservers our proofs proceed through solving two totally positive completion
problems.
",0,0,1,0,0,0
2410,Global Sensitivity Analysis of High Dimensional Neuroscience Models: An Example of Neurovascular Coupling,"  The complexity and size of state-of-the-art cell models have significantly
increased in part due to the requirement that these models possess complex
cellular functions which are thought--but not necessarily proven--to be
important. Modern cell models often involve hundreds of parameters; the values
of these parameters come, more often than not, from animal experiments whose
relationship to the human physiology is weak with very little information on
the errors in these measurements. The concomitant uncertainties in parameter
values result in uncertainties in the model outputs or Quantities of Interest
(QoIs). Global Sensitivity Analysis (GSA) aims at apportioning to individual
parameters (or sets of parameters) their relative contribution to output
uncertainty thereby introducing a measure of influence or importance of said
parameters. New GSA approaches are required to deal with increased model size
and complexity; a three stage methodology consisting of screening (dimension
reduction), surrogate modeling, and computing Sobol' indices, is presented. The
methodology is used to analyze a physiologically validated numerical model of
neurovascular coupling which possess 160 uncertain parameters. The sensitivity
analysis investigates three quantities of interest (QoIs), the average value of
$K^+$ in the extracellular space, the average volumetric flow rate through the
perfusing vessel, and the minimum value of the actin/myosin complex in the
smooth muscle cell. GSA provides a measure of the influence of each parameter,
for each of the three QoIs, giving insight into areas of possible physiological
dysfunction and areas of further investigation.
",0,0,0,0,1,0
203,An Integrated Decision and Control Theoretic Solution to Multi-Agent Co-Operative Search Problems,"  This paper considers the problem of autonomous multi-agent cooperative target
search in an unknown environment using a decentralized framework under a
no-communication scenario. The targets are considered as static targets and the
agents are considered to be homogeneous. The no-communication scenario
translates as the agents do not exchange either the information about the
environment or their actions among themselves. We propose an integrated
decision and control theoretic solution for a search problem which generates
feasible agent trajectories. In particular, a perception based algorithm is
proposed which allows an agent to estimate the probable strategies of other
agents' and to choose a decision based on such estimation. The algorithm shows
robustness with respect to the estimation accuracy to a certain degree. The
performance of the algorithm is compared with random strategies and numerical
simulation shows considerable advantages.
",1,0,0,0,0,0
641,Development of probabilistic dam breach model using Bayesian inference,"  Dam breach models are commonly used to predict outflow hydrographs of
potentially failing dams and are key ingredients for evaluating flood risk. In
this paper a new dam breach modeling framework is introduced that shall improve
the reliability of hydrograph predictions of homogeneous earthen embankment
dams. Striving for a small number of parameters, the simplified physics-based
model describes the processes of failing embankment dams by breach enlargement,
driven by progressive surface erosion. Therein the erosion rate of dam material
is modeled by empirical sediment transport formulations. Embedding the model
into a Bayesian multilevel framework allows for quantitative analysis of
different categories of uncertainties. To this end, data available in
literature of observed peak discharge and final breach width of historical dam
failures was used to perform model inversion by applying Markov Chain Monte
Carlo simulation. Prior knowledge is mainly based on non-informative
distribution functions. The resulting posterior distribution shows that the
main source of uncertainty is a correlated subset of parameters, consisting of
the residual error term and the epistemic term quantifying the breach erosion
rate. The prediction intervals of peak discharge and final breach width are
congruent with values known from literature. To finally predict the outflow
hydrograph for real case applications, an alternative residual model was
formulated that assumes perfect data and a perfect model. The fully
probabilistic fashion of hydrograph prediction has the potential to improve the
adequate risk management of downstream flooding.
",0,0,0,1,0,0
249,On some polynomials and series of Bloch-Polya Type,"  We will show that $(1-q)(1-q^2)\dots (1-q^m)$ is a polynomial in $q$ with
coefficients from $\{-1,0,1\}$ iff $m=1,\ 2,\ 3,$ or $5$ and explore some
interesting consequences of this result. We find explicit formulas for the
$q$-series coefficients of $(1-q^2)(1-q^3)(1-q^4)(1-q^5)\dots$ and
$(1-q^3)(1-q^4)(1-q^5)(1-q^6)\dots$. In doing so, we extend certain
observations made by Sudler in 1964. We also discuss the classification of the
products $(1-q)(1-q^2)\dots (1-q^m)$ and some related series with respect to
their absolute largest coefficients.
",0,0,1,0,0,0
1365,Pricing options and computing implied volatilities using neural networks,"  This paper proposes a data-driven approach, by means of an Artificial Neural
Network (ANN), to value financial options and to calculate implied volatilities
with the aim of accelerating the corresponding numerical methods. With ANNs
being universal function approximators, this method trains an optimized ANN on
a data set generated by a sophisticated financial model, and runs the trained
ANN as an agent of the original solver in a fast and efficient way. We test
this approach on three different types of solvers, including the analytic
solution for the Black-Scholes equation, the COS method for the Heston
stochastic volatility model and Brent's iterative root-finding method for the
calculation of implied volatilities. The numerical results show that the ANN
solver can reduce the computing time significantly.
",1,0,0,0,0,1
6071,Statistical mechanics of low-rank tensor decomposition,"  Often, large, high dimensional datasets collected across multiple modalities
can be organized as a higher order tensor. Low-rank tensor decomposition then
arises as a powerful and widely used tool to discover simple low dimensional
structures underlying such data. However, we currently lack a theoretical
understanding of the algorithmic behavior of low-rank tensor decompositions. We
derive Bayesian approximate message passing (AMP) algorithms for recovering
arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic
mean field theory to precisely characterize their performance. Our theory
reveals the existence of phase transitions between easy, hard and impossible
inference regimes, and displays an excellent match with simulations. Moreover,
it reveals several qualitative surprises compared to the behavior of symmetric,
cubic tensor decomposition. Finally, we compare our AMP algorithm to the most
commonly used algorithm, alternating least squares (ALS), and demonstrate that
AMP significantly outperforms ALS in the presence of noise.
",0,0,0,0,1,0
437,City-Scale Road Audit System using Deep Learning,"  Road networks in cities are massive and is a critical component of mobility.
Fast response to defects, that can occur not only due to regular wear and tear
but also because of extreme events like storms, is essential. Hence there is a
need for an automated system that is quick, scalable and cost-effective for
gathering information about defects. We propose a system for city-scale road
audit, using some of the most recent developments in deep learning and semantic
segmentation. For building and benchmarking the system, we curated a dataset
which has annotations required for road defects. However, many of the labels
required for road audit have high ambiguity which we overcome by proposing a
label hierarchy. We also propose a multi-step deep learning model that segments
the road, subdivide the road further into defects, tags the frame for each
defect and finally localizes the defects on a map gathered using GPS. We
analyze and evaluate the models on image tagging as well as segmentation at
different levels of the label hierarchy.
",1,0,0,0,0,0
621,Birth of a subaqueous barchan dune,"  Barchan dunes are crescentic shape dunes with horns pointing downstream. The
present paper reports the formation of subaqueous barchan dunes from initially
conical heaps in a rectangular channel. Because the most unique feature of a
barchan dune is its horns, we associate the timescale for the appearance of
horns to the formation of a barchan dune. A granular heap initially conical was
placed on the bottom wall of a closed conduit and it was entrained by a water
flow in turbulent regime. After a certain time, horns appear and grow, until an
equilibrium length is reached. Our results show the existence of the timescales
$0.5t_c$ and $2.5t_c$ for the appearance and equilibrium of horns,
respectively, where $t_c$ is a characteristic time that scales with the grains
diameter, gravity acceleration, densities of the fluid and grains, and shear
and threshold velocities.
",0,1,0,0,0,0
2100,Eigensolutions and spectral analysis of a model for vertical gene transfer of plasmids,"  Plasmids are autonomously replicating genetic elements in bacteria. At cell
division plasmids are distributed among the two daughter cells. This gene
transfer from one generation to the next is called vertical gene transfer. We
study the dynamics of a bacterial population carrying plasmids and are in
particular interested in the long-time distribution of plasmids. Starting with
a model for a bacterial population structured by the discrete number of
plasmids, we proceed to the continuum limit in order to derive a continuous
model. The model incorporates plasmid reproduction, division and death of
bacteria, and distribution of plasmids at cell division. It is a hyperbolic
integro-differential equation and a so-called growth-fragmentation-death model.
As we are interested in the long-time distribution of plasmids we study the
associated eigenproblem and show existence of eigensolutions. The stability of
this solution is studied by analyzing the spectrum of the integro-differential
operator given by the eigenproblem. By relating the spectrum with the spectrum
of an integral operator we find a simple real dominating eigenvalue with a
non-negative corresponding eigenfunction. Moreover, we describe an iterative
method for the numerical construction of the eigenfunction.
",0,0,0,0,1,0
6871,"Modelling diverse sources of Clostridium difficile in the community: importance of animals, infants and asymptomatic carriers","  Clostridium difficile infections (CDIs) affect patients in hospitals and in
the community, but the relative importance of transmission in each setting is
unknown. We developed a mathematical model of C. difficile transmission in a
hospital and surrounding community that included infants, adults, and
transmission from animal reservoirs. We assessed the role of these transmission
routes in maintaining disease and evaluated the recommended classification
system for hospital and community-acquired CDIs. The reproduction number in the
hospital was <1 (range: 0.16-0.46) for all scenarios. Outside the hospital, the
reproduction number was >1 for nearly all scenarios without transmission from
animal reservoirs (range: 1.0-1.34). However, the reproduction number for the
human population was <1 if a minority (>3.5-26.0%) of human exposures
originated from animal reservoirs. Symptomatic adults accounted for <10%
transmission in the community. Under conservative assumptions, infants
accounted for 17% of community transmission. An estimated 33-40% of
community-acquired cases were reported but 28-39% of these reported cases were
misclassified as hospital-acquired by recommended definitions. Transmission
could be plausibly sustained by asymptomatically colonized adults and infants
in the community or exposure to animal reservoirs, but not hospital
transmission alone. Underreporting of community-onset cases and systematic
misclassification underplays the role of community transmission.
",0,0,0,0,1,0
937,Toward Controlled Generation of Text,"  Generic generation and manipulation of text is challenging and has limited
success compared to recent deep generative modeling in visual domain. This
paper aims at generating plausible natural language sentences, whose attributes
are dynamically controlled by learning disentangled latent representations with
designated semantics. We propose a new neural generative model which combines
variational auto-encoders and holistic attribute discriminators for effective
imposition of semantic structures. With differentiable approximation to
discrete text samples, explicit constraints on independent attribute controls,
and efficient collaborative learning of generator and discriminators, our model
learns highly interpretable representations from even only word annotations,
and produces realistic sentences with desired attributes. Quantitative
evaluation validates the accuracy of sentence and attribute generation.
",1,0,0,1,0,0
918,WOMBAT: A Scalable and High Performance Astrophysical MHD Code,"  We present a new code for astrophysical magneto-hydrodynamics specifically
designed and optimized for high performance and scaling on modern and future
supercomputers. We describe a novel hybrid OpenMP/MPI programming model that
emerged from a collaboration between Cray, Inc. and the University of
Minnesota. This design utilizes MPI-RMA optimized for thread scaling, which
allows the code to run extremely efficiently at very high thread counts ideal
for the latest generation of the multi-core and many-core architectures. Such
performance characteristics are needed in the era of ""exascale"" computing. We
describe and demonstrate our high-performance design in detail with the intent
that it may be used as a model for other, future astrophysical codes intended
for applications demanding exceptional performance.
",0,1,0,0,0,0
981,Smooth Neighbors on Teacher Graphs for Semi-supervised Learning,"  The recently proposed self-ensembling methods have achieved promising results
in deep semi-supervised learning, which penalize inconsistent predictions of
unlabeled data under different perturbations. However, they only consider
adding perturbations to each single data point, while ignoring the connections
between data samples. In this paper, we propose a novel method, called Smooth
Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on
the predictions of the teacher model, i.e., the implicit self-ensemble of
models. Then the graph serves as a similarity measure with respect to which the
representations of ""similar"" neighboring points are learned to be smooth on the
low-dimensional manifold. We achieve state-of-the-art results on
semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for
CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,
the improvements are significant when the labels are fewer. For the
non-augmented MNIST with only 20 labels, the error rate is reduced from
previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.
",1,0,0,1,0,0
690,High-$T_\textrm {C}$ superconductivity in Cs$_3$C$_{60}$ compounds governed by local Cs-C$_{60}$ Coulomb interactions,"  Unique among alkali-doped $\textit {A}$$_3$C$_{60}$ fullerene compounds, the
A15 and fcc forms of Cs$_3$C$_{60}$ exhibit superconducting states varying
under hydrostatic pressure with highest transition temperatures at $T_\textrm
{C}$$^\textrm {meas}$ = 38.3 and 35.2 K, respectively. Herein it is argued that
these two compounds under pressure represent the optimal materials of the
$\textit {A}$$_3$C$_{60}$ family, and that the C$_{60}$-associated
superconductivity is mediated through Coulombic interactions with charges on
the alkalis. A derivation of the interlayer Coulombic pairing model of
high-$T_\textrm {C}$ superconductivity employing non-planar geometry is
introduced, generalizing the picture of two interacting layers to an
interaction between charge reservoirs located on the C$_{60}$ and alkali ions.
The optimal transition temperature follows the algebraic expression, $T_\textrm
{C0}$ = (12.474 nm$^2$ K)/$\ell$${\zeta}$, where $\ell$ relates to the mean
spacing between interacting surface charges on the C$_{60}$ and ${\zeta}$ is
the average radial distance between the C$_{60}$ surface and the neighboring Cs
ions. Values of $T_\textrm {C0}$ for the measured cation stoichiometries of
Cs$_{3-\textrm{x}}$C$_{60}$ with x $\approx$ 0 are found to be 38.19 and 36.88
K for the A15 and fcc forms, respectively, with the dichotomy in transition
temperature reflecting the larger ${\zeta}$ and structural disorder in the fcc
form. In the A15 form, modeled interacting charges and Coulomb potential
e$^2$/${\zeta}$ are shown to agree quantitatively with findings from
nuclear-spin relaxation and mid-infrared optical conductivity. In the fcc form,
suppression of $T_\textrm {C}$$^\textrm {meas}$ below $T_\textrm {C0}$ is
ascribed to native structural disorder. Phononic effects in conjunction with
Coulombic pairing are discussed.
",0,1,0,0,0,0
584,TED Talk Recommender Using Speech Transcripts,"  Nowadays, online video platforms mostly recommend related videos by analyzing
user-driven data such as viewing patterns, rather than the content of the
videos. However, content is more important than any other element when videos
aim to deliver knowledge. Therefore, we have developed a web application which
recommends related TED lecture videos to the users, considering the content of
the videos from the transcripts. TED Talk Recommender constructs a network for
recommending videos that are similar content-wise and providing a user
interface.
",1,0,0,0,0,0
848,NeuroNER: an easy-to-use program for named-entity recognition based on neural networks,"  Named-entity recognition (NER) aims at identifying entities of interest in a
text. Artificial neural networks (ANNs) have recently been shown to outperform
existing NER systems. However, ANNs remain challenging to use for non-expert
users. In this paper, we present NeuroNER, an easy-to-use named-entity
recognition tool based on ANNs. Users can annotate entities using a graphical
web-based user interface (BRAT): the annotations are then used to train an ANN,
which in turn predict entities' locations and categories in new texts. NeuroNER
makes this annotation-training-prediction flow smooth and accessible to anyone.
",1,0,0,1,0,0
369,Scatteract: Automated extraction of data from scatter plots,"  Charts are an excellent way to convey patterns and trends in data, but they
do not facilitate further modeling of the data or close inspection of
individual data points. We present a fully automated system for extracting the
numerical values of data points from images of scatter plots. We use deep
learning techniques to identify the key components of the chart, and optical
character recognition together with robust regression to map from pixels to the
coordinate system of the chart. We focus on scatter plots with linear scales,
which already have several interesting challenges. Previous work has done fully
automatic extraction for other types of charts, but to our knowledge this is
the first approach that is fully automatic for scatter plots. Our method
performs well, achieving successful data extraction on 89% of the plots in our
test set.
",1,0,0,1,0,0
709,The solitary g-mode frequencies in early B-type stars,"  We present possible explanations of pulsations in early B-type main sequence
stars which arise purely from the excitation of gravity modes. There are three
stars with this type of oscillations detected from the BRITE light curves:
$\kappa$ Cen, a Car, $\kappa$ Vel. We show that by changing metallicity or the
opacity profile it is possible in some models to dump pressure modes keeping
gravity modes unstable. Other possible scenario involves pulsations of a lower
mass companion.
",0,1,0,0,0,0
4985,Strong deformations of DNA: Effect on the persistence length,"  Extreme deformations of the DNA double helix attracted a lot of attention
during the past decades. Particularly, the determination of the persistence
length of DNA with extreme local disruptions, or kinks, has become a crucial
problem in the studies of many important biological processes. In this paper we
review an approach to calculate the persistence length of the double helix by
taking into account the formation of kinks of arbitrary configuration. The
reviewed approach improves the Kratky--Porod model to determine the type and
nature of kinks that occur in the double helix, by measuring a reduction of the
persistence length of the kinkable DNA.
",0,0,0,0,1,0
323,Yu-Shiba-Rusinov bands in superconductors in contact with a magnetic insulator,"  Superconductor-Ferromagnet (SF) heterostructures are of interest due to
numerous phenomena related to the spin-dependent interaction of Cooper pairs
with the magnetization. Here we address the effects of a magnetic insulator on
the density of states of a superconductor based on a recently developed
boundary condition for strongly spin-dependent interfaces. We show that the
boundary to a magnetic insulator has a similar effect like the presence of
magnetic impurities. In particular we find that the impurity effects of
strongly scattering localized spins leading to the formation of Shiba bands can
be mapped onto the boundary problem.
",0,1,0,0,0,0
245,Growing length scale accompanying the vitrification: A perspective based on non-singular density fluctuations,"  In glass forming liquids close to the glass transition point, even a very
slight increase in the macroscopic density results in a dramatic slowing down
of the macroscopic relaxation. Concomitantly, the local density itself
fluctuates in space. Therefore, one can imagine that even very small local
density variations control the local glassy nature. Based on this perspective,
a model for describing growing length scale accompanying the vitrification is
introduced, in which we assume that in a subsystem whose density is above a
certain threshold value, $\rho_{\rm c}$, owing to steric constraints, particle
rearrangements are highly suppressed for a sufficiently long time period
($\sim$ structural relaxation time). We regard such a subsystem as a glassy
cluster. Then, based on the statistics of the subsystem-density, we predict
that with compression (increasing average density $\rho$) at a fixed
temperature $T$ in supercooled states, the characteristic length of the
clusters, $\xi$, diverges as $\xi\sim(\rho_{\rm c}-\rho)^{-2/d}$, where $d$ is
the spatial dimensionality. This $\xi$ measures the average persistence length
of the steric constraints in blocking the rearrangement motions and is
determined by the subsystem density. Additionally, with decreasing $T$ at a
fixed $\rho$, the length scale diverges in the same manner as $\xi\sim(T-T_{\rm
c})^{-2/d}$, for which $\rho$ is identical to $\rho_{\rm c}$ at $T=T_{\rm c}$.
The exponent describing the diverging length scale is the same as the one
predicted by some theoretical models and indeed has been observed in some
simulations and experiments. However, the basic mechanism for this divergence
is different; that is, we do not invoke thermodynamic anomalies associated with
the thermodynamic phase transition as the origin of the growing length scale.
We further present arguements for the cooperative properties based on the
clusters.
",0,1,0,0,0,0
395,Attitude Control of a 2U Cubesat by Magnetic and Air Drag Torques,"  This paper describes the development of a magnetic attitude control subsystem
for a 2U cubesat. Due to the presence of gravity gradient torques, the
satellite dynamics are open-loop unstable near the desired pointing
configuration. Nevertheless the linearized time-varying system is completely
controllable, under easily verifiable conditions, and the system's disturbance
rejection capabilities can be enhanced by adding air drag panels exemplifying a
beneficial interplay between hardware design and control. In the paper,
conditions for the complete controllability for the case of a magnetically
controlled satellite with passive air drag panels are developed, and simulation
case studies with the LQR and MPC control designs applied in combination with a
nonlinear time-varying input transformation are presented to demonstrate the
ability of the closed-loop system to satisfy mission objectives despite
disturbance torques.
",0,0,1,0,0,0
3012,Kinky DNA in solution: Small angle scattering study of a nucleosome positioning sequence,"  DNA is a flexible molecule, but the degree of its flexibility is subject to
debate. The commonly-accepted persistence length of $l_p \approx 500\,$\AA\ is
inconsistent with recent studies on short-chain DNA that show much greater
flexibility but do not probe its origin. We have performed X-ray and neutron
small-angle scattering on a short DNA sequence containing a strong nucleosome
positioning element, and analyzed the results using a modified Kratky-Porod
model to determine possible conformations. Our results support a hypothesis
from Crick and Klug in 1975 that some DNA sequences in solution can have sharp
kinks, potentially resolving the discrepancy. Our conclusions are supported by
measurements on a radiation-damaged sample, where single-strand breaks lead to
increased flexibility and by an analysis of data from another sequence, which
does not have kinks, but where our method can detect a locally enhanced
flexibility due to an $AT$-domain.
",0,0,0,0,1,0
565,Non-negative Matrix Factorization via Archetypal Analysis,"  Given a collection of data points, non-negative matrix factorization (NMF)
suggests to express them as convex combinations of a small set of `archetypes'
with non-negative entries. This decomposition is unique only if the true
archetypes are non-negative and sufficiently sparse (or the weights are
sufficiently sparse), a regime that is captured by the separability condition
and its generalizations.
In this paper, we study an approach to NMF that can be traced back to the
work of Cutler and Breiman (1994) and does not require the data to be
separable, while providing a generally unique decomposition. We optimize the
trade-off between two objectives: we minimize the distance of the data points
from the convex envelope of the archetypes (which can be interpreted as an
empirical risk), while minimizing the distance of the archetypes from the
convex envelope of the data (which can be interpreted as a data-dependent
regularization). The archetypal analysis method of (Cutler, Breiman, 1994) is
recovered as the limiting case in which the last term is given infinite weight.
We introduce a `uniqueness condition' on the data which is necessary for
exactly recovering the archetypes from noiseless data. We prove that, under
uniqueness (plus additional regularity conditions on the geometry of the
archetypes), our estimator is robust. While our approach requires solving a
non-convex optimization problem, we find that standard optimization methods
succeed in finding good solutions both for real and synthetic data.
",1,0,0,0,0,0
357,Roche-lobe overflow in eccentric planet-star systems,"  Many giant exoplanets are found near their Roche limit and in mildly
eccentric orbits. In this study we examine the fate of such planets through
Roche-lobe overflow as a function of the physical properties of the binary
components, including the eccentricity and the asynchronicity of the rotating
planet. We use a direct three-body integrator to compute the trajectories of
the lost mass in the ballistic limit and investigate the possible outcomes. We
find three different outcomes for the mass transferred through the Lagrangian
point $L_{1}$: (i) self-accretion by the planet, (ii) direct impact on the
stellar surface, (iii) disk formation around the star. We explore the parameter
space of the three different regimes and find that at low eccentricities,
$e\lesssim 0.2$, mass overflow leads to disk formation for most systems, while
for higher eccentricities or retrograde orbits self-accretion is the only
possible outcome. We conclude that the assumption often made in previous work
that when a planet overflows its Roche lobe it is quickly disrupted and
accreted by the star is not always valid.
",0,1,0,0,0,0
771,On Diamond's $L^1$ criterion for asymptotic density of Beurling generalized integers,"  We give a short proof of the $L^{1}$ criterion for Beurling generalized
integers to have a positive asymptotic density. We actually prove the existence
of density under a weaker hypothesis. We also discuss related sufficient
conditions for the estimate $m(x)=\sum_{n_{k}\leq x} \mu(n_k)/n_k=o(1)$, with
$\mu$ the Beurling analog of the Moebius function.
",0,0,1,0,0,0
12352,Effects of a Price limit Change on Market Stability at the Intraday Horizon in the Korean Stock Market,"  This paper investigates the effects of a price limit change on the volatility
of the Korean stock market's (KRX) intraday stock price process. Based on the
most recent transaction data from the KRX, which experienced a change in the
price limit on June 15, 2015, we examine the change in realized variance after
the price limit change to investigate the overall effects of the change on the
intraday market volatility. We then analyze the effects in more detail by
applying the discrete Fourier transform (DFT) to the data set. We find evidence
that the market becomes more volatile in the intraday horizon because of the
increase in the amplitudes of the low-frequency components of the price
processes after the price limit change. Therefore, liquidity providers are in a
worse situation than they were prior to the change.
",0,0,0,0,0,1
520,Perfect phylogenies via branchings in acyclic digraphs and a generalization of Dilworth's theorem,"  Motivated by applications in cancer genomics and following the work of
Hajirasouliha and Raphael (WABI 2014), Hujdurović et al. (IEEE TCBB, to
appear) introduced the minimum conflict-free row split (MCRS) problem: split
each row of a given binary matrix into a bitwise OR of a set of rows so that
the resulting matrix corresponds to a perfect phylogeny and has the minimum
possible number of rows among all matrices with this property. Hajirasouliha
and Raphael also proposed the study of a similar problem, in which the task is
to minimize the number of distinct rows of the resulting matrix. Hujdurović
et al. proved that both problems are NP-hard, gave a related characterization
of transitively orientable graphs, and proposed a polynomial-time heuristic
algorithm for the MCRS problem based on coloring cocomparability graphs.
We give new, more transparent formulations of the two problems, showing that
the problems are equivalent to two optimization problems on branchings in a
derived directed acyclic graph. Building on these formulations, we obtain new
results on the two problems, including: (i) a strengthening of the heuristic by
Hujdurović et al. via a new min-max result in digraphs generalizing
Dilworth's theorem, which may be of independent interest, (ii) APX-hardness
results for both problems, (iii) approximation algorithms, and (iv)
exponential-time algorithms solving the two problems to optimality faster than
the naïve brute-force approach. Our work relates to several well studied
notions in combinatorial optimization: chain partitions in partially ordered
sets, laminar hypergraphs, and (classical and weighted) colorings of graphs.
",1,0,1,0,0,0
372,An Asymptotically Efficient Metropolis-Hastings Sampler for Bayesian Inference in Large-Scale Educational Measuremen,"  This paper discusses a Metropolis-Hastings algorithm developed by
\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is
proven that the algorithm becomes more efficient with more data and meets the
growing demands of large scale educational measurement.
",0,0,0,1,0,0
4367,"The same strain of Piscine orthoreovirus (PRV-1) is involved with the development of different, but related, diseases in Atlantic and Pacific Salmon in British Columbia","  Piscine orthoreovirus Strain PRV-1 is the causative agent of heart and
skeletal muscle inflammation (HSMI) in Atlantic salmon (Salmo salar). Given its
high prevalence in net pen salmon, debate has arisen on whether PRV poses a
risk to migratory salmon, especially in British Columbia (BC) where
commercially important wild Pacific salmon are in decline. Various strains of
PRV have been associated with diseases in Pacific salmon, including
erythrocytic inclusion body syndrome (EIBS), HSMI-like disease, and
jaundice/anemia in Japan, Norway, Chile and Canada. We examine the
developmental pathway of HSMI and jaundice/anemia associated with PRV-1 in
farmed Atlantic and Chinook (Oncorhynchus tshawytscha) salmon in BC,
respectively. In situ hybridization localized PRV-1 within developing lesions
in both diseases. The two diseases showed dissimilar pathological pathways,
with inflammatory lesions in heart and skeletal muscle in Atlantic salmon, and
degenerative-necrotic lesions in kidney and liver in Chinook salmon, plausibly
explained by differences in PRV load tolerance in red blood cells. Viral genome
sequencing revealed no consistent differences in PRV-1 variants intimately
involved in the development of both diseases, suggesting that migratory Chinook
salmon may be at more than a minimal risk of disease from exposure to the high
levels of PRV occurring on salmon farms.
",0,0,0,0,1,0
6613,Calibration for Weak Variance-Alpha-Gamma Processes,"  The weak variance-alpha-gamma process is a multivariate Lévy process
constructed by weakly subordinating Brownian motion, possibly with correlated
components with an alpha-gamma subordinator. It generalises the
variance-alpha-gamma process of Semeraro constructed by traditional
subordination. We compare three calibration methods for the weak
variance-alpha-gamma process, method of moments, maximum likelihood estimation
(MLE) and digital moment estimation (DME). We derive a condition for Fourier
invertibility needed to apply MLE and show in our simulations that MLE produces
a better fit when this condition holds, while DME produces a better fit when it
is violated. We also find that the weak variance-alpha-gamma process exhibits a
wider range of dependence and produces a significantly better fit than the
variance-alpha-gamma process on an S&P500-FTSE100 data set, and that DME
produces the best fit in this situation.
",0,0,0,0,0,1
364,OpenML Benchmarking Suites and the OpenML100,"  We advocate the use of curated, comprehensive benchmark suites of machine
learning datasets, backed by standardized OpenML-based interfaces and
complementary software toolkits written in Python, Java and R. Major
distinguishing features of OpenML benchmark suites are (a) ease of use through
standardized data formats, APIs, and existing client libraries; (b)
machine-readable meta-information regarding the contents of the suite; and (c)
online sharing of results, enabling large scale comparisons. As a first such
suite, we propose the OpenML100, a machine learning benchmark suite of
100~classification datasets carefully curated from the thousands of datasets
available on OpenML.org.
",1,0,0,1,0,0
2003,"The role of industry, occupation, and location specific knowledge in the survival of new firms","  How do regions acquire the knowledge they need to diversify their economic
activities? How does the migration of workers among firms and industries
contribute to the diffusion of that knowledge? Here we measure the industry,
occupation, and location-specific knowledge carried by workers from one
establishment to the next using a dataset summarizing the individual work
history for an entire country. We study pioneer firms--firms operating in an
industry that was not present in a region--because the success of pioneers is
the basic unit of regional economic diversification. We find that the growth
and survival of pioneers increase significantly when their first hires are
workers with experience in a related industry, and with work experience in the
same location, but not with past experience in a related occupation. We compare
these results with new firms that are not pioneers and find that
industry-specific knowledge is significantly more important for pioneer than
non-pioneer firms. To address endogeneity we use Bartik instruments, which
leverage national fluctuations in the demand for an activity as shocks for
local labor supply. The instrumental variable estimates support the finding
that industry-related knowledge is a predictor of the survival and growth of
pioneer firms. These findings expand our understanding of the micro-mechanisms
underlying regional economic diversification events.
",0,0,0,0,0,1
559,Trusted Multi-Party Computation and Verifiable Simulations: A Scalable Blockchain Approach,"  Large-scale computational experiments, often running over weeks and over
large datasets, are used extensively in fields such as epidemiology,
meteorology, computational biology, and healthcare to understand phenomena, and
design high-stakes policies affecting everyday health and economy. For
instance, the OpenMalaria framework is a computationally-intensive simulation
used by various non-governmental and governmental agencies to understand
malarial disease spread and effectiveness of intervention strategies, and
subsequently design healthcare policies. Given that such shared results form
the basis of inferences drawn, technological solutions designed, and day-to-day
policies drafted, it is essential that the computations are validated and
trusted. In particular, in a multi-agent environment involving several
independent computing agents, a notion of trust in results generated by peers
is critical in facilitating transparency, accountability, and collaboration.
Using a novel combination of distributed validation of atomic computation
blocks and a blockchain-based immutable audits mechanism, this work proposes a
universal framework for distributed trust in computations. In particular we
address the scalaibility problem by reducing the storage and communication
costs using a lossy compression scheme. This framework guarantees not only
verifiability of final results, but also the validity of local computations,
and its cost-benefit tradeoffs are studied using a synthetic example of
training a neural network.
",1,0,0,0,0,0
15786,The risk of contagion spreading and its optimal control in the economy,"  The global crisis of 2008 provoked a heightened interest among scientists to
study the phenomenon, its propagation and negative consequences. The process of
modelling the spread of a virus is commonly used in epidemiology. Conceptually,
the spread of a disease among a population is similar to the contagion process
in economy. This similarity allows considering the contagion in the world
financial system using the same mathematical model of infection spread that is
often used in epidemiology. Our research focuses on the dynamic behaviour of
contagion spreading in the global financial network. The effect of infection by
a systemic spread of risks in the network of national banking systems of
countries is tested. An optimal control problem is then formulated to simulate
a control that may avoid significant financial losses. The results show that
the proposed approach describes well the reality of the world economy, and
emphasizes the importance of international relations between countries on the
financial stability.
",0,0,0,0,0,1
254,A Categorical Approach for Recognizing Emotional Effects of Music,"  Recently, digital music libraries have been developed and can be plainly
accessed. Latest research showed that current organization and retrieval of
music tracks based on album information are inefficient. Moreover, they
demonstrated that people use emotion tags for music tracks in order to search
and retrieve them. In this paper, we discuss separability of a set of emotional
labels, proposed in the categorical emotion expression, using Fisher's
separation theorem. We determine a set of adjectives to tag music parts: happy,
sad, relaxing, exciting, epic and thriller. Temporal, frequency and energy
features have been extracted from the music parts. It could be seen that the
maximum separability within the extracted features occurs between relaxing and
epic music parts. Finally, we have trained a classifier using Support Vector
Machines to automatically recognize and generate emotional labels for a music
part. Accuracy for recognizing each label has been calculated; where the
results show that epic music can be recognized more accurately (77.4%),
comparing to the other types of music.
",1,0,0,1,0,0
231,Photonic topological pumping through the edges of a dynamical four-dimensional quantum Hall system,"  When a two-dimensional electron gas is exposed to a perpendicular magnetic
field and an in-plane electric field, its conductance becomes quantized in the
transverse in-plane direction: this is known as the quantum Hall (QH) effect.
This effect is a result of the nontrivial topology of the system's electronic
band structure, where an integer topological invariant known as the first Chern
number leads to the quantization of the Hall conductance. Interestingly, it was
shown that the QH effect can be generalized mathematically to four spatial
dimensions (4D), but this effect has never been realized for the obvious reason
that experimental systems are bound to three spatial dimensions. In this work,
we harness the high tunability and control offered by photonic waveguide arrays
to experimentally realize a dynamically-generated 4D QH system using a 2D array
of coupled optical waveguides. The inter-waveguide separation is constructed
such that the propagation of light along the device samples over
higher-dimensional momenta in the directions orthogonal to the two physical
dimensions, thus realizing a 2D topological pump. As a result, the device's
band structure is associated with 4D topological invariants known as second
Chern numbers which support a quantized bulk Hall response with a 4D symmetry.
In a finite-sized system, the 4D topological bulk response is carried by
localized edges modes that cross the sample as a function of of the modulated
auxiliary momenta. We directly observe this crossing through photon pumping
from edge-to-edge and corner-to-corner of our system. These are equivalent to
the pumping of charge across a 4D system from one 3D hypersurface to the
opposite one and from one 2D hyperedge to another, and serve as first
experimental realization of higher-dimensional topological physics.
",0,1,0,0,0,0
599,Banach synaptic algebras,"  Using a representation theorem of Erik Alfsen, Frederic Schultz, and Erling
Stormer for special JB-algebras, we prove that a synaptic algebra is norm
complete (i.e., Banach) if and only if it is isomorphic to the self-adjoint
part of a Rickart C*-algebra. Also, we give conditions on a Banach synaptic
algebra that are equivalent to the condition that it is isomorphic to the
self-adjoint part of an AW*-algebra. Moreover, we study some relationships
between synaptic algebras and so-called generalized Hermitian algebras.
",0,0,1,0,0,0
6625,A framework for cost-constrained genome rearrangement under Double Cut and Join,"  The study of genome rearrangement has many flavours, but they all are somehow
tied to edit distances on variations of a multi-graph called the breakpoint
graph. We study a weighted 2-break distance on Eulerian 2-edge-colored
multi-graphs, which generalizes weighted versions of several Double Cut and
Join problems, including those on genomes with unequal gene content. We affirm
the connection between cycle decompositions and edit scenarios first discovered
with the Sorting By Reversals problem. Using this we show that the problem of
finding a parsimonious scenario of minimum cost on an Eulerian 2-edge-colored
multi-graph - with a general cost function for 2-breaks - can be solved by
decomposing the problem into independent instances on simple alternating
cycles. For breakpoint graphs, and a more constrained cost function, based on
coloring the vertices, we give a polynomial-time algorithm for finding a
parsimonious 2-break scenario of minimum cost, while showing that finding a
non-parsimonious 2-break scenario of minimum cost is NP-Hard.
",0,0,0,0,1,0
860,Monitoring Telluric Absorption with CAMAL,"  Ground-based astronomical observations may be limited by telluric water vapor
absorption, which is highly variable in time and significantly complicates both
spectroscopy and photometry in the near-infrared (NIR). To achieve the
sensitivity required to detect Earth-sized exoplanets in the NIR, simultaneous
monitoring of precipitable water vapor (PWV) becomes necessary to mitigate the
impact of variable telluric lines on radial velocity measurements and transit
light curves. To address this issue, we present the Camera for the Automatic
Monitoring of Atmospheric Lines (CAMAL), a stand-alone, inexpensive six-inch
aperture telescope dedicated to measuring PWV at the Fred Lawrence Whipple
Observatory on Mount Hopkins. CAMAL utilizes three narrowband NIR filters to
trace the amount of atmospheric water vapor affecting simultaneous observations
with the MINiature Exoplanet Radial Velocity Array (MINERVA) and MINERVA-Red
telescopes. Here we present the current design of CAMAL, discuss our data
analysis methods, and show results from 11 nights of PWV measurements taken
with CAMAL. For seven nights of data, we have independent PWV measurements
extracted from high-resolution stellar spectra taken with the Tillinghast
Reflector Echelle Spectrometer (TRES) also located on Mount Hopkins. We use the
TRES spectra to calibrate the CAMAL absolute PWV scale. Comparisons between
CAMAL and TRES PWV estimates show excellent agreement, matching to within 1 mm
over a 10 mm range in PWV. Analysis of CAMAL's photometric precision propagates
to PWV measurements precise to better than 0.5 mm in dry (PWV < 4 mm)
conditions. We also find that CAMAL-derived PWVs are highly correlated with
those from a GPS-based water vapor monitor located approximately 90 km away at
Kitt Peak National Observatory, with a root mean square PWV difference of 0.8
mm.
",0,1,0,0,0,0
609,High SNR Consistent Compressive Sensing,"  High signal to noise ratio (SNR) consistency of model selection criteria in
linear regression models has attracted a lot of attention recently. However,
most of the existing literature on high SNR consistency deals with model order
selection. Further, the limited literature available on the high SNR
consistency of subset selection procedures (SSPs) is applicable to linear
regression with full rank measurement matrices only. Hence, the performance of
SSPs used in underdetermined linear models (a.k.a compressive sensing (CS)
algorithms) at high SNR is largely unknown. This paper fills this gap by
deriving necessary and sufficient conditions for the high SNR consistency of
popular CS algorithms like $l_0$-minimization, basis pursuit de-noising or
LASSO, orthogonal matching pursuit and Dantzig selector. Necessary conditions
analytically establish the high SNR inconsistency of CS algorithms when used
with the tuning parameters discussed in literature. Novel tuning parameters
with SNR adaptations are developed using the sufficient conditions and the
choice of SNR adaptations are discussed analytically using convergence rate
analysis. CS algorithms with the proposed tuning parameters are numerically
shown to be high SNR consistent and outperform existing tuning parameters in
the moderate to high SNR regime.
",1,0,0,1,0,0
252,Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework,"  Linear time-periodic (LTP) dynamical systems frequently appear in the
modeling of phenomena related to fluid dynamics, electronic circuits, and
structural mechanics via linearization centered around known periodic orbits of
nonlinear models. Such LTP systems can reach orders that make repeated
simulation or other necessary analysis prohibitive, motivating the need for
model reduction.
We develop here an algorithmic framework for constructing reduced models that
retains the linear time-periodic structure of the original LTP system. Our
approach generalizes optimal approaches that have been established previously
for linear time-invariant (LTI) model reduction problems. We employ an
extension of the usual H2 Hardy space defined for the LTI setting to
time-periodic systems and within this broader framework develop an a posteriori
error bound expressible in terms of related LTI systems. Optimization of this
bound motivates our algorithm. We illustrate the success of our method on two
numerical examples.
",1,0,0,0,0,0
5580,Scalar Reduction of a Neural Field Model with Spike Frequency Adaptation,"  We study a deterministic version of a one- and two-dimensional attractor
neural network model of hippocampal activity first studied by Itskov et al
2011. We analyze the dynamics of the system on the ring and torus domain with
an even periodized weight matrix, assum- ing weak and slow spike frequency
adaptation and a weak stationary input current. On these domains, we find
transitions from spatially localized stationary solutions (""bumps"") to
(periodically modulated) solutions (""sloshers""), as well as constant and
non-constant velocity traveling bumps depending on the relative strength of
external input current and adaptation. The weak and slow adaptation allows for
a reduction of the system from a distributed partial integro-differential
equation to a system of scalar Volterra integro-differential equations
describing the movement of the centroid of the bump solution. Using this
reduction, we show that on both domains, sloshing solutions arise through an
Andronov-Hopf bifurcation and derive a normal form for the Hopf bifurcation on
the ring. We also show existence and stability of constant velocity solutions
on both domains using Evans functions. In contrast to existing studies, we
assume a general weight matrix of Mexican-hat type in addition to a smooth
firing rate function.
",0,0,0,0,1,0
201,Jastrow form of the Ground State Wave Functions for Fractional Quantum Hall States,"  The topological morphology--order of zeros at the positions of electrons with
respect to a specific electron--of Laughlin state at filling fractions $1/m$
($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the
positions of other electrons. Although fairly accurate ground state wave
functions for most of the other quantum Hall states in the lowest Landau level
are quite well-known, it had been an open problem in expressing the ground
state wave functions in terms of flux-attachment to particles, {\em a la}, this
morphology of Laughlin state. With a very general consideration of
flux-particle relations only, in spherical geometry, we here report a novel
method for determining morphologies of these states. Based on these, we
construct almost exact ground state wave-functions for the Coulomb interaction.
Although the form of interaction may change the ground state wave-function, the
same morphology constructs the latter irrespective of the nature of the
interaction between electrons.
",0,1,0,0,0,0
1009,The Generalized Cross Validation Filter,"  Generalized cross validation (GCV) is one of the most important approaches
used to estimate parameters in the context of inverse problems and
regularization techniques. A notable example is the determination of the
smoothness parameter in splines. When the data are generated by a state space
model, like in the spline case, efficient algorithms are available to evaluate
the GCV score with complexity that scales linearly in the data set size.
However, these methods are not amenable to on-line applications since they rely
on forward and backward recursions. Hence, if the objective has been evaluated
at time $t-1$ and new data arrive at time t, then O(t) operations are needed to
update the GCV score. In this paper we instead show that the update cost is
$O(1)$, thus paving the way to the on-line use of GCV. This result is obtained
by deriving the novel GCV filter which extends the classical Kalman filter
equations to efficiently propagate the GCV score over time. We also illustrate
applications of the new filter in the context of state estimation and on-line
regularized linear system identification.
",1,0,0,1,0,0
620,An enthalpy-based multiple-relaxation-time lattice Boltzmann method for solid-liquid phase change heat transfer in metal foams,"  In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice
Boltzmann (LB) method is developed for solid-liquid phase change heat transfer
in metal foams under local thermal non-equilibrium (LTNE) condition. The
enthalpy-based MRT-LB method consists of three different MRT-LB models: one for
flow field based on the generalized non-Darcy model, and the other two for
phase change material (PCM) and metal foam temperature fields described by the
LTNE model. The moving solid-liquid phase interface is implicitly tracked
through the liquid fraction, which is simultaneously obtained when the energy
equations of PCM and metal foam are solved. The present method has several
distinctive features. First, as compared with previous studies, the present
method avoids the iteration procedure, thus it retains the inherent merits of
the standard LB method and is superior over the iteration method in terms of
accuracy and computational efficiency. Second, a volumetric LB scheme instead
of the bounce-back scheme is employed to realize the no-slip velocity condition
in the interface and solid phase regions, which is consistent with the actual
situation. Last but not least, the MRT collision model is employed, and with
additional degrees of freedom, it has the ability to reduce the numerical
diffusion across phase interface induced by solid-liquid phase change.
Numerical tests demonstrate that the present method can be served as an
accurate and efficient numerical tool for studying metal foam enhanced
solid-liquid phase change heat transfer in latent heat storage. Finally,
comparisons and discussions are made to offer useful information for practical
applications of the present method.
",0,1,0,0,0,0
3442,The Stretch to Stray on Time: Resonant Length of Random Walks in a Transient,"  First-passage times in random walks have a vast number of diverse
applications in physics, chemistry, biology, and finance. In general,
environmental conditions for a stochastic process are not constant on the time
scale of the average first-passage time, or control might be applied to reduce
noise. We investigate moments of the first-passage time distribution under a
transient describing relaxation of environmental conditions. We solve the
Laplace-transformed (generalized) master equation analytically using a novel
method that is applicable to general state schemes. The first-passage time from
one end to the other of a linear chain of states is our application for the
solutions. The dependence of its average on the relaxation rate obeys a power
law for slow transients. The exponent $\nu$ depends on the chain length $N$
like $\nu=-N/(N+1)$ to leading order. Slow transients substantially reduce the
noise of first-passage times expressed as the coefficient of variation (CV),
even if the average first-passage time is much longer than the transient. The
CV has a pronounced minimum for some lengths, which we call resonant lengths.
These results also suggest a simple and efficient noise control strategy, and
are closely related to the timing of repetitive excitations, coherence
resonance and information transmission by noisy excitable systems. A resonant
number of steps from the inhibited state to the excitation threshold and slow
recovery from negative feedback provide optimal timing noise reduction and
information transmission.
",0,0,0,0,1,1
613,Distance Measure Machines,"  This paper presents a distance-based discriminative framework for learning
with probability distributions. Instead of using kernel mean embeddings or
generalized radial basis kernels, we introduce embeddings based on
dissimilarity of distributions to some reference distributions denoted as
templates. Our framework extends the theory of similarity of Balcan et al.
(2008) to the population distribution case and we show that, for some learning
problems, some dissimilarity on distribution achieves low-error linear decision
functions with high probability. Our key result is to prove that the theory
also holds for empirical distributions. Algorithmically, the proposed approach
consists in computing a mapping based on pairwise dissimilarity where learning
a linear decision function is amenable. Our experimental results show that the
Wasserstein distance embedding performs better than kernel mean embeddings and
computing Wasserstein distance is far more tractable than estimating pairwise
Kullback-Leibler divergence of empirical distributions.
",0,0,0,1,0,0
546,Spherical Functions on Riemannian Symmetric Spaces,"  This paper deals with some simple results about spherical functions of type
$\delta$, namely new integral formulas, new results about behavior at infinity
and some facts about the related $C_\sigma$ functions.
",0,0,1,0,0,0
1566,Why Abeta42 Is Much More Toxic Than Abeta40,"  Amyloid precursor with 770 amino acids dimerizes and aggregates, as do its c
terminal 99 amino acids and amyloid 40,42 amino acids fragments. The titled
question has been discussed extensively, and here it is addressed further using
thermodynamic scaling theory to analyze mutational trends in structural factors
and kinetics. Special attention is given to Family Alzheimer's Disease
mutations outside amyloid 42. The scaling analysis is connected to extensive
docking simulations which included membranes, thereby confirming their results
and extending them to Amyloid precursor.
",0,0,0,0,1,0
274,Calibration-Free Relaxation-Based Multi-Color Magnetic Particle Imaging,"  Magnetic Particle Imaging (MPI) is a novel imaging modality with important
applications such as angiography, stem cell tracking, and cancer imaging.
Recently, there have been efforts to increase the functionality of MPI via
multi-color imaging methods that can distinguish the responses of different
nanoparticles, or nanoparticles in different environmental conditions. The
proposed techniques typically rely on extensive calibrations that capture the
differences in the harmonic responses of the nanoparticles. In this work, we
propose a method to directly estimate the relaxation time constant of the
nanoparticles from the MPI signal, which is then used to generate a multi-color
relaxation map. The technique is based on the underlying mirror symmetry of the
adiabatic MPI signal when the same region is scanned back and forth. We
validate the proposed method via extensive simulations, and via experiments on
our in-house Magnetic Particle Spectrometer (MPS) setup at 550 Hz and our
in-house MPI scanner at 9.7 kHz. Our results show that nanoparticles can be
successfully distinguished with the proposed technique, without any calibration
or prior knowledge about the nanoparticles.
",0,1,0,0,0,0
820,The Tutte embedding of the mated-CRT map converges to Liouville quantum gravity,"  We prove that the Tutte embeddings (a.k.a. harmonic/embeddings) of certain
random planar maps converge to $\gamma$-Liouville quantum gravity
($\gamma$-LQG). Specifically, we treat mated-CRT maps, which are discretized
matings of correlated continuum random trees, and $\gamma$ ranges from $0$ to
$2$ as one varies the correlation parameter. We also show that the associated
space-filling path on the embedded map converges to space-filling
SLE$_{\kappa}$ for $\kappa =16/\gamma^2$ (in the annealed sense) and that
simple random walk on the embedded map converges to Brownian motion (in the
quenched sense). Our arguments also yield analogous statements for the Smith
(square tiling) embedding of the mated-CRT map.
This work constitutes the first proof that a discrete conformal embedding of
a random planar map converges to LQG. Many more such statements have been
conjectured. Since the mated-CRT map can be viewed as a coarse-grained
approximation to other random planar maps (the UIPT, tree-weighted maps,
bipolar-oriented maps, etc.), our results indicate a potential approach for
proving that embeddings of these maps converge to LQG as well.
To prove the main result, we establish several (independently interesting)
theorems about LQG surfaces decorated by space-filling SLE. There is a natural
way to use the SLE curve to divide the plane into `cells' corresponding to
vertices of the mated-CRT map. We study the law of the shape of the
origin-containing cell, in particular proving moments for the ratio of its
squared diameter to its area. We also give bounds on the degree of the
origin-containing cell and establish a form of ergodicity for the entire
configuration. Ultimately, we use these properties to show (using a general
theorem proved in a separate paper) that random walk on these cells converges
to a time change of Brownian motion, which in turn leads to the Tutte embedding
result.
",0,0,1,0,0,0
5243,Introduction to the Special Issue on Approaches to Control Biological and Biologically Inspired Networks,"  The emerging field at the intersection of quantitative biology, network
modeling, and control theory has enjoyed significant progress in recent years.
This Special Issue brings together a selection of papers on complementary
approaches to observe, identify, and control biological and biologically
inspired networks. These approaches advance the state of the art in the field
by addressing challenges common to many such networks, including high
dimensionality, strong nonlinearity, uncertainty, and limited opportunities for
observation and intervention. Because these challenges are not unique to
biological systems, it is expected that many of the results presented in these
contributions will also find applications in other domains, including physical,
social, and technological networks.
",1,0,0,0,1,0
668,The Diverse Club: The Integrative Core of Complex Networks,"  A complex system can be represented and analyzed as a network, where nodes
represent the units of the network and edges represent connections between
those units. For example, a brain network represents neurons as nodes and axons
between neurons as edges. In many networks, some nodes have a
disproportionately high number of edges. These nodes also have many edges
between each other, and are referred to as the rich club. In many different
networks, the nodes of this club are assumed to support global network
integration. However, another set of nodes potentially exhibits a connectivity
structure that is more advantageous to global network integration. Here, in a
myriad of different biological and man-made networks, we discover the diverse
club--a set of nodes that have edges diversely distributed across the network.
The diverse club exhibits, to a greater extent than the rich club, properties
consistent with an integrative network function--these nodes are more highly
interconnected and their edges are more critical for efficient global
integration. Moreover, we present a generative evolutionary network model that
produces networks with a diverse club but not a rich club, thus demonstrating
that these two clubs potentially evolved via distinct selection pressures.
Given the variety of different networks that we analyzed--the c. elegans, the
macaque brain, the human brain, the United States power grid, and global air
traffic--the diverse club appears to be ubiquitous in complex networks. These
results warrant the distinction and analysis of two critical clubs of nodes in
all complex systems.
",0,1,0,0,0,0
446,Fan-type spin structure in uni-axial chiral magnets,"  We investigate the spin structure of a uni-axial chiral magnet near the
transition temperatures in low fields perpendicular to the helical axis. We
find a fan-type modulation structure where the clockwise and counterclockwise
windings appear alternatively along the propagation direction of the modulation
structure. This structure is often realized in a Yoshimori-type (non-chiral)
helimagnet but it is rarely realized in a chiral helimagnet. To discuss
underlying physics of this structure, we reconsider the phase diagram (phase
boundary and crossover lines) through the free energy and asymptotic behaviors
of isolated solitons. The fan structure appears slightly below the phase
boundary of the continuous transition of instability-type. In this region,
there are no solutions containing any types of isolated solitons to the mean
field equations.
",0,1,0,0,0,0
311,A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs,"  We report the discovery of three small transiting planets orbiting GJ 9827, a
bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$
$R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$
$R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm
\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting
GJ 9827 span the transition between predominantly rocky and gaseous planets,
and GJ 9827 b and c fall in or close to the known gap in the radius
distribution of small planets between these populations. At a distance of 30
parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making
these planets well-suited for atmospheric studies with the upcoming James Webb
Space Telescope. The GJ 9827 system provides a valuable opportunity to
characterize interior structure and atmospheric properties of coeval planets
spanning the rocky to gaseous transition.
",0,1,0,0,0,0
236,An automata group of intermediate growth and exponential activity,"  We give a new example of an automata group of intermediate growth. It is
generated by an automaton with 4 states on an alphabet with 8 letters. This
automata group has exponential activity and its limit space is not simply
connected.
",0,0,1,0,0,0
6175,Fooling the classifier: Ligand antagonism and adversarial examples,"  Machine learning algorithms are sensitive to so-called adversarial
perturbations. This is reminiscent of cellular decision-making where antagonist
ligands may prevent correct signaling, like during the early immune response.
We draw a formal analogy between neural networks used in machine learning and
the general class of adaptive proofreading networks. We then apply simple
adversarial strategies from machine learning to models of ligand
discrimination. We show how kinetic proofreading leads to ""boundary tilting""
and identify three types of perturbation (adversarial, non adversarial and
ambiguous). We then use a gradient-descent approach to compare different
adaptive proofreading models, and we reveal the existence of two qualitatively
different regimes characterized by the presence or absence of a critical point.
These regimes are reminiscent of the ""feature-to-prototype"" transition
identified in machine learning, corresponding to two strategies in ligand
antagonism (broad vs. specialized). Overall, our work connects evolved cellular
decision-making to classification in machine learning, showing that behaviours
close to the decision boundary can be understood through the same mechanisms.
",0,0,0,1,1,0
15842,Advertising and Brand Attitudes: Evidence from 575 Brands over Five Years,"  Little is known about how different types of advertising affect brand
attitudes. We investigate the relationships between three brand attitude
variables (perceived quality, perceived value and recent satisfaction) and
three types of advertising (national traditional, local traditional and
digital). The data represent ten million brand attitude surveys and $264
billion spent on ads by 575 regular advertisers over a five-year period,
approximately 37% of all ad spend measured between 2008 and 2012. Inclusion of
brand/quarter fixed effects and industry/week fixed effects brings parameter
estimates closer to expectations without major reductions in estimation
precision. The findings indicate that (i) national traditional ads increase
perceived quality, perceived value, and recent satisfaction; (ii) local
traditional ads increase perceived quality and perceived value; (iii) digital
ads increase perceived value; and (iv) competitor ad effects are generally
negative.
",0,0,0,0,0,1
284,Infinitary first-order categorical logic,"  We present a unified categorical treatment of completeness theorems for
several classical and intuitionistic infinitary logics with a proposed
axiomatization. This provides new completeness theorems and subsumes previous
ones by Gödel, Kripke, Beth, Karp, Joyal, Makkai and Fourman/Grayson. As an
application we prove, using large cardinals assumptions, the disjunction and
existence properties for infinitary intuitionistic first-order logics.
",0,0,1,0,0,0
5501,Compact Convolutional Neural Networks for Classification of Asynchronous Steady-state Visual Evoked Potentials,"  Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from
the parietal and occipital regions of the brain that are evoked from flickering
visual stimuli. SSVEPs are robust signals measurable in the
electroencephalogram (EEG) and are commonly used in brain-computer interfaces
(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require
hand-crafted approaches that leverage domain-specific knowledge of the stimulus
signals, such as specific temporal frequencies in the visual stimuli and their
relative spatial arrangement. When this knowledge is unavailable, such as when
SSVEP signals are acquired asynchronously, such approaches tend to fail. In
this paper, we show how a compact convolutional neural network (Compact-CNN),
which only requires raw EEG signals for automatic feature extraction, can be
used to decode signals from a 12-class SSVEP dataset without the need for any
domain-specific knowledge or calibration data. We report across subject mean
accuracy of approximately 80% (chance being 8.3%) and show this is
substantially better than current state-of-the-art hand-crafted approaches
using canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we
analyze our Compact-CNN to examine the underlying feature representation,
discovering that the deep learner extracts additional phase and amplitude
related features associated with the structure of the dataset. We discuss how
our Compact-CNN shows promise for BCI applications that allow users to freely
gaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as
provides a method for analyzing SSVEP signals in a way that might augment our
understanding about the basic processing in the visual cortex.
",0,0,0,1,1,0
865,Symmetries of handlebodies and their fixed points: Dihedral extended Schottky groups,"  A Schottky structure on a handlebody $M$ of genus $g$ is provided by a
Schottky group of rank $g$. A symmetry (an orientation-reversing involution) of
$M$ is known to have at most $(g+1)$ connected components of fixed points. Each
of these components is either a point or a compact bordered surface (either
orientable or not) whose boundary is contained in the border of $M$. In this
paper, we derive sharp upper bounds for the total number of connected
components of the sets of fixed points of given two or three symmetries of $M$.
In order to obtain such an upper bound, we obtain a geometrical structure
description of those extended Kleinian groups $K$ containing a Schottky group
$\Gamma$ as finite index normal subgroup so that $K/\Gamma$ is a dihedral group
(called dihedral Schottky groups). Our upper bounds turn out to be different to
the corresponding ones at the level of closed Riemann surfaces. In contrast to
the case of Riemann surfaces, we observe that $M$ cannot have two different
maximal symmetries.
",0,0,1,0,0,0
6466,"A mechanistic model of connector hubs, modularity, and cognition","  The human brain network is modular--comprised of communities of tightly
interconnected nodes. This network contains local hubs, which have many
connections within their own communities, and connector hubs, which have
connections diversely distributed across communities. A mechanistic
understanding of these hubs and how they support cognition has not been
demonstrated. Here, we leveraged individual differences in hub connectivity and
cognition. We show that a model of hub connectivity accurately predicts the
cognitive performance of 476 individuals in four distinct tasks. Moreover,
there is a general optimal network structure for cognitive
performance--individuals with diversely connected hubs and consequent modular
brain networks exhibit increased cognitive performance, regardless of the task.
Critically, we find evidence consistent with a mechanistic model in which
connector hubs tune the connectivity of their neighbors to be more modular
while allowing for task appropriate information integration across communities,
which increases global modularity and cognitive performance.
",0,0,0,0,1,0
648,Control Strategies for the Fokker-Planck Equation,"  Using a projection-based decoupling of the Fokker-Planck equation, control
strategies that allow to speed up the convergence to the stationary
distribution are investigated. By means of an operator theoretic framework for
a bilinear control system, two different feedback control laws are proposed.
Projected Riccati and Lyapunov equations are derived and properties of the
associated solutions are given. The well-posedness of the closed loop systems
is shown and local and global stabilization results, respectively, are
obtained. An essential tool in the construction of the controls is the choice
of appropriate control shape functions. Results for a two dimensional double
well potential illustrate the theoretical findings in a numerical setup.
",0,0,1,0,0,0
3271,Modularity Matters: Learning Invariant Relational Reasoning Tasks,"  We focus on two supervised visual reasoning tasks whose labels encode a
semantic relational rule between two or more objects in an image: the MNIST
Parity task and the colorized Pentomino task. The objects in the images undergo
random translation, scaling, rotation and coloring transformations. Thus these
tasks involve invariant relational reasoning. We report uneven performance of
various deep CNN models on these two tasks. For the MNIST Parity task, we
report that the VGG19 model soundly outperforms a family of ResNet models.
Moreover, the family of ResNet models exhibits a general sensitivity to random
initialization for the MNIST Parity task. For the colorized Pentomino task, now
both the VGG19 and ResNet models exhibit sluggish optimization and very poor
test generalization, hovering around 30% test error. The CNN we tested all
learn hierarchies of fully distributed features and thus encode the distributed
representation prior. We are motivated by a hypothesis from cognitive
neuroscience which posits that the human visual cortex is modularized, and this
allows the visual cortex to learn higher order invariances. To this end, we
consider a modularized variant of the ResNet model, referred to as a Residual
Mixture Network (ResMixNet) which employs a mixture-of-experts architecture to
interleave distributed representations with more specialized, modular
representations. We show that very shallow ResMixNets are capable of learning
each of the two tasks well, attaining less than 2% and 1% test error on the
MNIST Parity and the colorized Pentomino tasks respectively. Most importantly,
the ResMixNet models are extremely parameter efficient: generalizing better
than various non-modular CNNs that have over 10x the number of parameters.
These experimental results support the hypothesis that modularity is a robust
prior for learning invariant relational reasoning.
",0,0,0,1,1,0
12688,Thought Viruses and Asset Prices,"  We use insights from epidemiology, namely the SIR model, to study how agents
infect each other with ""investment ideas."" Once an investment idea ""goes
viral,"" equilibrium prices exhibit the typical ""fever peak,"" which is
characteristic for speculative excesses. Using our model, we identify a time
line of symptoms that indicate whether a boom is in its early or later stages.
Regarding the market's top, we find that prices start to decline while the
number of infected agents, who buy the asset, is still rising. Moreover, the
presence of fully rational agents (i) accelerates booms (ii) lowers peak prices
and (iii) produces broad, drawn-out, market tops.
",0,0,0,0,0,1
474,Comparative Investigation of the High Pressure Autoignition of the Butanol Isomers,"  Investigation of the autoignition delay of the butanol isomers has been
performed at elevated pressures of 15 bar and 30 bar and low to intermediate
temperatures of 680-860 K. The reactivity of the stoichiometric isomers of
butanol, in terms of inverse ignition delay, was ranked as n-butanol >
sec-butanol ~ iso-butanol > tert-butanol at a compressed pressure of 15 bar but
changed to n-butanol > tert-butanol > sec-butanol > iso-butanol at 30 bar. For
the temperature and pressure conditions in this study, no NTC or two-stage
ignition behavior were observed. However, for both of the compressed pressures
studied in this work, tert-butanol exhibited unique pre-ignition heat release
characteristics. As such, tert-butanol was further studied at two additional
equivalence ratios ($\phi$ = 0.5 and 2.0) to help determine the cause of the
heat release.
",0,1,0,0,0,0
456,Deadly dark matter cusps vs faint and extended star clusters: Eridanus II and Andromeda XXV,"  The recent detection of two faint and extended star clusters in the central
regions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,
raises the question of whether clusters with such low densities can survive the
tidal field of cold dark matter haloes with central density cusps. Using both
analytic arguments and a suite of collisionless N-body simulations, I show that
these clusters are extremely fragile and quickly disrupted in the presence of
central cusps $\rho\sim r^{-\alpha}$ with $\alpha\gtrsim 0.2$. Furthermore, the
scenario in which the clusters where originally more massive and sank to the
center of the halo requires extreme fine tuning and does not naturally
reproduce the observed systems. In turn, these clusters are long lived in cored
haloes, whose central regions are safe shelters for $\alpha\lesssim 0.2$. The
only viable scenario for hosts that have preserved their primoridal cusp to the
present time is that the clusters formed at rest at the bottom of the
potential, which is easily tested by measurement of the clusters proper
velocity within the host. This offers means to readily probe the central
density profile of two dwarf galaxies as faint as $L_V\sim5\times 10^5 L_\odot$
and $L_V\sim6\times10^4 L_\odot$, in which stellar feedback is unlikely to be
effective.
",0,1,0,0,0,0
393,Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence,"  In this paper, we study stochastic non-convex optimization with non-convex
random functions. Recent studies on non-convex optimization revolve around
establishing second-order convergence, i.e., converging to a nearly
second-order optimal stationary points. However, existing results on stochastic
non-convex optimization are limited, especially with a high probability
second-order convergence. We propose a novel updating step (named NCG-S) by
leveraging a stochastic gradient and a noisy negative curvature of a stochastic
Hessian, where the stochastic gradient and Hessian are based on a proper
mini-batch of random functions. Building on this step, we develop two
algorithms and establish their high probability second-order convergence. To
the best of our knowledge, the proposed stochastic algorithms are the first
with a second-order convergence in {\it high probability} and a time complexity
that is {\it almost linear} in the problem's dimensionality.
",1,0,0,1,0,0
581,A Globally Linearly Convergent Method for Pointwise Quadratically Supportable Convex-Concave Saddle Point Problems,"  We study the \emph{Proximal Alternating Predictor-Corrector} (PAPC) algorithm
introduced recently by Drori, Sabach and Teboulle to solve nonsmooth structured
convex-concave saddle point problems consisting of the sum of a smooth convex
function, a finite collection of nonsmooth convex functions and bilinear terms.
We introduce the notion of pointwise quadratic supportability, which is a
relaxation of a standard strong convexity assumption and allows us to show that
the primal sequence is R-linearly convergent to an optimal solution and the
primal-dual sequence is globally Q-linearly convergent. We illustrate the
proposed method on total variation denoising problems and on locally adaptive
estimation in signal/image deconvolution and denoising with multiresolution
statistical constraints.
",0,0,1,0,0,0
624,Perils of Zero-Interaction Security in the Internet of Things,"  The Internet of Things (IoT) demands authentication systems which can provide
both security and usability. Recent research utilizes the rich sensing
capabilities of smart devices to build security schemes operating without human
interaction, such as zero-interaction pairing (ZIP) and zero-interaction
authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and
reported promising results. However, those schemes were often evaluated under
conditions which do not reflect realistic IoT scenarios. In addition, drawing
any comparison among the existing schemes is impossible due to the lack of a
common public dataset and unavailability of scheme implementations.
In this paper, we address these challenges by conducting the first
large-scale comparative study of ZIP and ZIA schemes, carried out under
realistic conditions. We collect and release the most comprehensive dataset in
the domain to date, containing over 4250 hours of audio recordings and 1
billion sensor readings from three different scenarios, and evaluate five
state-of-the-art schemes based on these data. Our study reveals that the
effectiveness of the existing proposals is highly dependent on the scenario
they are used in. In particular, we show that these schemes are subject to
error rates between 0.6% and 52.8%.
",1,0,0,0,0,0
6947,Corruption-free scheme of entering into contract: mathematical model,"  The main purpose of this paper is to formalize the modelling process,
analysis and mathematical definition of corruption when entering into a
contract between principal agent and producers. The formulation of the problem
and the definition of concepts for the general case are considered. For
definiteness, all calculations and formulas are given for the case of three
producers, one principal agent and one intermediary. Economic analysis of
corruption allowed building a mathematical model of interaction between agents.
Financial resources distribution problem in a contract with a corrupted
intermediary is considered.Then proposed conditions for corruption emergence
and its possible consequences. Optimal non-corruption schemes of financial
resources distribution in a contract are formed, when principal agent's choice
is limited first only by asymmetrical information and then also by external
influences.Numerical examples suggesting optimal corruption-free agents'
behaviour are presented.
",0,0,0,0,0,1
16978,Explicit solutions to utility maximization problems in a regime-switching market model via Laplace transforms,"  We study the problem of utility maximization from terminal wealth in which an
agent optimally builds her portfolio by investing in a bond and a risky asset.
The asset price dynamics follow a diffusion process with regime-switching
coefficients modeled by a continuous-time finite-state Markov chain. We
consider an investor with a Constant Relative Risk Aversion (CRRA) utility
function. We deduce the associated Hamilton-Jacobi-Bellman equation to
construct the solution and the optimal trading strategy and verify optimality
by showing that the value function is the unique constrained viscosity solution
of the HJB equation. By means of a Laplace transform method, we show how to
explicitly compute the value function and illustrate the method with the two-
and three-states cases. This method is interesting in its own right and can be
adapted in other applications involving hybrid systems and using other types of
transforms with basic properties similar to the Laplace transform.
",0,0,0,0,0,1
480,Vortex Nucleation Limited Mobility of Free Electron Bubbles in the Gross-Pitaevskii Model of a Superfluid,"  We study the motion of an electron bubble in the zero temperature limit where
neither phonons nor rotons provide a significant contribution to the drag
exerted on an ion moving within the superfluid. By using the Gross-Clark model,
in which a Gross-Pitaevskii equation for the superfluid wavefunction is coupled
to a Schrödinger equation for the electron wavefunction, we study how
vortex nucleation affects the measured drift velocity of the ion. We use
parameters that give realistic values of the ratio of the radius of the bubble
with respect to the healing length in superfluid $^4$He at a pressure of one
bar. By performing fully 3D spatio-temporal simulations of the superfluid
coupled to an electron, that is modelled within an adiabatic approximation and
moving under the influence of an applied electric field, we are able to recover
the key dynamics of the ion-vortex interactions that arise and the subsequent
ion-vortex complexes that can form. Using the numerically computed drift
velocity of the ion as a function of the applied electric field, we determine
the vortex-nucleation limited mobility of the ion to recover values in
reasonable agreement with measured data.
",0,1,0,0,0,0
1015,Hidden Community Detection in Social Networks,"  We introduce a new paradigm that is important for community detection in the
realm of network analysis. Networks contain a set of strong, dominant
communities, which interfere with the detection of weak, natural community
structure. When most of the members of the weak communities also belong to
stronger communities, they are extremely hard to be uncovered. We call the weak
communities the hidden community structure.
We present a novel approach called HICODE (HIdden COmmunity DEtection) that
identifies the hidden community structure as well as the dominant community
structure. By weakening the strength of the dominant structure, one can uncover
the hidden structure beneath. Likewise, by reducing the strength of the hidden
structure, one can more accurately identify the dominant structure. In this
way, HICODE tackles both tasks simultaneously.
Extensive experiments on real-world networks demonstrate that HICODE
outperforms several state-of-the-art community detection methods in uncovering
both the dominant and the hidden structure. In the Facebook university social
networks, we find multiple non-redundant sets of communities that are strongly
associated with residential hall, year of registration or career position of
the faculties or students, while the state-of-the-art algorithms mainly locate
the dominant ground truth category. In the Due to the difficulty of labeling
all ground truth communities in real-world datasets, HICODE provides a
promising approach to pinpoint the existing latent communities and uncover
communities for which there is no ground truth. Finding this unknown structure
is an extremely important community detection problem.
",1,1,0,1,0,0
270,An attentive neural architecture for joint segmentation and parsing and its application to real estate ads,"  In processing human produced text using natural language processing (NLP)
techniques, two fundamental subtasks that arise are (i) segmentation of the
plain text into meaningful subunits (e.g., entities), and (ii) dependency
parsing, to establish relations between subunits. In this paper, we develop a
relatively simple and effective neural joint model that performs both
segmentation and dependency parsing together, instead of one after the other as
in most state-of-the-art works. We will focus in particular on the real estate
ad setting, aiming to convert an ad to a structured description, which we name
property tree, comprising the tasks of (1) identifying important entities of a
property (e.g., rooms) from classifieds and (2) structuring them into a tree
format. In this work, we propose a new joint model that is able to tackle the
two tasks simultaneously and construct the property tree by (i) avoiding the
error propagation that would arise from the subtasks one after the other in a
pipelined fashion, and (ii) exploiting the interactions between the subtasks.
For this purpose, we perform an extensive comparative study of the pipeline
methods and the new proposed joint model, reporting an improvement of over
three percentage points in the overall edge F1 score of the property tree.
Also, we propose attention methods, to encourage our model to focus on salient
tokens during the construction of the property tree. Thus we experimentally
demonstrate the usefulness of attentive neural architectures for the proposed
joint model, showcasing a further improvement of two percentage points in edge
F1 score for our application.
",1,0,0,0,0,0
6025,Mining within-trial oscillatory brain dynamics to address the variability of optimized spatial filters,"  Data-driven spatial filtering algorithms optimize scores such as the contrast
between two conditions to extract oscillatory brain signal components. Most
machine learning approaches for filter estimation, however, disregard
within-trial temporal dynamics and are extremely sensitive to changes in
training data and involved hyperparameters. This leads to highly variable
solutions and impedes the selection of a suitable candidate for,
e.g.,~neurotechnological applications. Fostering component introspection, we
propose to embrace this variability by condensing the functional signatures of
a large set of oscillatory components into homogeneous clusters, each
representing specific within-trial envelope dynamics.
The proposed method is exemplified by and evaluated on a complex hand force
task with a rich within-trial structure. Based on electroencephalography data
of 18 healthy subjects, we found that the components' distinct temporal
envelope dynamics are highly subject-specific. On average, we obtained seven
clusters per subject, which were strictly confined regarding their underlying
frequency bands. As the analysis method is not limited to a specific spatial
filtering algorithm, it could be utilized for a wide range of
neurotechnological applications, e.g., to select and monitor functionally
relevant features for brain-computer interface protocols in stroke
rehabilitation.
",0,0,0,1,1,0
691,Analysis and mitigation of interface losses in trenched superconducting coplanar waveguide resonators,"  Improving the performance of superconducting qubits and resonators generally
results from a combination of materials and fabrication process improvements
and design modifications that reduce device sensitivity to residual losses. One
instance of this approach is to use trenching into the device substrate in
combination with superconductors and dielectrics with low intrinsic losses to
improve quality factors and coherence times. Here we demonstrate titanium
nitride coplanar waveguide resonators with mean quality factors exceeding two
million and controlled trenching reaching 2.2 $\mu$m into the silicon
substrate. Additionally, we measure sets of resonators with a range of sizes
and trench depths and compare these results with finite-element simulations to
demonstrate quantitative agreement with a model of interface dielectric loss.
We then apply this analysis to determine the extent to which trenching can
improve resonator performance.
",0,1,0,0,0,0
1459,Kinetics of Protein-DNA Interactions: First-Passage Analysis,"  All living systems can function only far away from equilibrium, and for this
reason chemical kinetic methods are critically important for uncovering the
mechanisms of biological processes. Here we present a new theoretical method of
investigating dynamics of protein-DNA interactions, which govern all major
biological processes. It is based on a first-passage analysis of biochemical
and biophysical transitions, and it provides a fully analytic description of
the processes. Our approach is explained for the case of a single protein
searching for a specific binding site on DNA. In addition, the application of
the method to investigations of the effect of DNA sequence heterogeneity, and
the role multiple targets and traps in the protein search dynamics are
discussed.
",0,0,0,0,1,0
255,Utilizing artificial neural networks to predict demand for weather-sensitive products at retail stores,"  One key requirement for effective supply chain management is the quality of
its inventory management. Various inventory management methods are typically
employed for different types of products based on their demand patterns,
product attributes, and supply network. In this paper, our goal is to develop
robust demand prediction methods for weather sensitive products at retail
stores. We employ historical datasets from Walmart, whose customers and markets
are often exposed to extreme weather events which can have a huge impact on
sales regarding the affected stores and products. We want to accurately predict
the sales of 111 potentially weather-sensitive products around the time of
major weather events at 45 of Walmart retails locations in the U.S.
Intuitively, we may expect an uptick in the sales of umbrellas before a big
thunderstorm, but it is difficult for replenishment managers to predict the
level of inventory needed to avoid being out-of-stock or overstock during and
after that storm. While they rely on a variety of vendor tools to predict sales
around extreme weather events, they mostly employ a time-consuming process that
lacks a systematic measure of effectiveness. We employ all the methods critical
to any analytics project and start with data exploration. Critical features are
extracted from the raw historical dataset for demand forecasting accuracy and
robustness. In particular, we employ Artificial Neural Network for forecasting
demand for each product sold around the time of major weather events. Finally,
we evaluate our model to evaluate their accuracy and robustness.
",1,0,0,1,0,0
212,Experimental Design of a Prescribed Burn Instrumentation,"  Observational data collected during experiments, such as the planned Fire and
Smoke Model Evaluation Experiment (FASMEE), are critical for progressing and
transitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM
into operational use. Historical meteorological data, representing typical
weather conditions for the anticipated burn locations and times, have been
processed to initialize and run a set of simulations representing the planned
experimental burns. Based on an analysis of these numerical simulations, this
paper provides recommendations on the experimental setup that include the
ignition procedures, size and duration of the burns, and optimal sensor
placement. New techniques are developed to initialize coupled fire-atmosphere
simulations with weather conditions typical of the planned burn locations and
time of the year. Analysis of variation and sensitivity analysis of simulation
design to model parameters by repeated Latin Hypercube Sampling are used to
assess the locations of the sensors. The simulations provide the locations of
the measurements that maximize the expected variation of the sensor outputs
with the model parameters.
",0,0,0,1,0,0
487,Stigmergy-based modeling to discover urban activity patterns from positioning data,"  Positioning data offer a remarkable source of information to analyze crowds
urban dynamics. However, discovering urban activity patterns from the emergent
behavior of crowds involves complex system modeling. An alternative approach is
to adopt computational techniques belonging to the emergent paradigm, which
enables self-organization of data and allows adaptive analysis. Specifically,
our approach is based on stigmergy. By using stigmergy each sample position is
associated with a digital pheromone deposit, which progressively evaporates and
aggregates with other deposits according to their spatiotemporal proximity.
Based on this principle, we exploit positioning data to identify high density
areas (hotspots) and characterize their activity over time. This
characterization allows the comparison of dynamics occurring in different days,
providing a similarity measure exploitable by clustering techniques. Thus, we
cluster days according to their activity behavior, discovering unexpected urban
activity patterns. As a case study, we analyze taxi traces in New York City
during 2015.
",1,1,0,0,0,0
227,Joint Atlas-Mapping of Multiple Histological Series combined with Multimodal MRI of Whole Marmoset Brains,"  Development of a mesoscale neural circuitry map of the common marmoset is an
essential task due to the ideal characteristics of the marmoset as a model
organism for neuroscience research. To facilitate this development there is a
need for new computational tools to cross-register multi-modal data sets
containing MRI volumes as well as multiple histological series, and to register
the combined data set to a common reference atlas. We present a fully automatic
pipeline for same-subject-MRI guided reconstruction of image volumes from a
series of histological sections of different modalities, followed by
diffeomorphic mapping to a reference atlas. We show registration results for
Nissl, myelin, CTB, and fluorescent tracer images using a same-subject ex-vivo
MRI as our reference and show that our method achieves accurate registration
and eliminates artifactual warping that may be result from the absence of a
reference MRI data set. Examination of the determinant of the local metric
tensor of the diffeomorphic mapping between each subject's ex-vivo MRI and
resultant Nissl reconstruction allows an unprecedented local quantification of
geometrical distortions resulting from the histological processing, showing a
slight shrinkage, a median linear scale change of ~-1% in going from the
ex-vivo MRI to the tape-transfer generated histological image data.
",0,0,0,0,1,0
825,Remarks on Inner Functions and Optimal Approximants,"  We discuss the concept of inner function in reproducing kernel Hilbert spaces
with an orthogonal basis of monomials and examine connections between inner
functions and optimal polynomial approximants to $1/f$, where $f$ is a function
in the space. We revisit some classical examples from this perspective, and
show how a construction of Shapiro and Shields can be modified to produce inner
functions.
",0,0,1,0,0,0
552,Learning Neural Models for End-to-End Clustering,"  We propose a novel end-to-end neural network architecture that, once trained,
directly outputs a probabilistic clustering of a batch of input examples in one
pass. It estimates a distribution over the number of clusters $k$, and for each
$1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster
assignment for each data point. The network is trained in advance in a
supervised fashion on separate data to learn grouping by any perceptual
similarity criterion based on pairwise labels (same/different group). It can
then be applied to different data containing different groups. We demonstrate
promising performance on high-dimensional data like images (COIL-100) and
speech (TIMIT). We call this ``learning to cluster'' and show its conceptual
difference to deep metric learning, semi-supervise clustering and other related
approaches while having the advantage of performing learnable clustering fully
end-to-end.
",0,0,0,1,0,0
512,Optimal Identity Testing with High Probability,"  We study the problem of testing identity against a given distribution with a
focus on the high confidence regime. More precisely, given samples from an
unknown distribution $p$ over $n$ elements, an explicitly given distribution
$q$, and parameters $0< \epsilon, \delta < 1$, we wish to distinguish, {\em
with probability at least $1-\delta$}, whether the distributions are identical
versus $\varepsilon$-far in total variation distance. Most prior work focused
on the case that $\delta = \Omega(1)$, for which the sample complexity of
identity testing is known to be $\Theta(\sqrt{n}/\epsilon^2)$. Given such an
algorithm, one can achieve arbitrarily small values of $\delta$ via black-box
amplification, which multiplies the required number of samples by
$\Theta(\log(1/\delta))$.
We show that black-box amplification is suboptimal for any $\delta = o(1)$,
and give a new identity tester that achieves the optimal sample complexity. Our
new upper and lower bounds show that the optimal sample complexity of identity
testing is \[
\Theta\left( \frac{1}{\epsilon^2}\left(\sqrt{n \log(1/\delta)} +
\log(1/\delta) \right)\right) \] for any $n, \varepsilon$, and $\delta$. For
the special case of uniformity testing, where the given distribution is the
uniform distribution $U_n$ over the domain, our new tester is surprisingly
simple: to test whether $p = U_n$ versus $d_{\mathrm TV}(p, U_n) \geq
\varepsilon$, we simply threshold $d_{\mathrm TV}(\widehat{p}, U_n)$, where
$\widehat{p}$ is the empirical probability distribution. The fact that this
simple ""plug-in"" estimator is sample-optimal is surprising, even in the
constant $\delta$ case. Indeed, it was believed that such a tester would not
attain sublinear sample complexity even for constant values of $\varepsilon$
and $\delta$.
",1,0,1,1,0,0
957,Adversarial Phenomenon in the Eyes of Bayesian Deep Learning,"  Deep Learning models are vulnerable to adversarial examples, i.e.\ images
obtained via deliberate imperceptible perturbations, such that the model
misclassifies them with high confidence. However, class confidence by itself is
an incomplete picture of uncertainty. We therefore use principled Bayesian
methods to capture model uncertainty in prediction for observing adversarial
misclassification. We provide an extensive study with different Bayesian neural
networks attacked in both white-box and black-box setups. The behaviour of the
networks for noise, attacks and clean test data is compared. We observe that
Bayesian neural networks are uncertain in their predictions for adversarial
perturbations, a behaviour similar to the one observed for random Gaussian
perturbations. Thus, we conclude that Bayesian neural networks can be
considered for detecting adversarial examples.
",1,0,0,1,0,0
481,Radio variability and non-thermal components in stars evolving toward planetary nebulae,"  We present new JVLA multi-frequency measurements of a set of stars in
transition from the post-AGB to the Planetary Nebula phase monitored in the
radio range over several years. Clear variability is found for five sources.
Their light curves show increasing and decreasing patterns. New radio
observations at high angular resolution are also presented for two sources.
Among these is IRAS 18062+2410, whose radio structure is compared to
near-infrared images available in the literature. With these new maps, we can
estimate inner and outer radii of 0.03$""$ and 0.08$""$ for the ionised shell, an
ionised mass of $3.2\times10^{-4}$ M$_\odot$, and a density at the inner radius
of $7.7\times 10^{-5}$ cm$^{-3}$, obtained by modelling the radio shell with
the new morphological constraints. The combination of multi-frequency data and,
where available, spectral-index maps leads to the detection of spectral indices
not due to thermal emission, contrary to what one would expect in planetary
nebulae. Our results allow us to hypothesise the existence of a link between
radio variability and non-thermal emission mechanisms in the nebulae. This link
seems to hold for IRAS 22568+6141 and may generally hold for those nebulae
where the radio flux decreases over time.
",0,1,0,0,0,0
779,Gaussian-Dirichlet Posterior Dominance in Sequential Learning,"  We consider the problem of sequential learning from categorical observations
bounded in [0,1]. We establish an ordering between the Dirichlet posterior over
categorical outcomes and a Gaussian posterior under observations with N(0,1)
noise. We establish that, conditioned upon identical data with at least two
observations, the posterior mean of the categorical distribution will always
second-order stochastically dominate the posterior mean of the Gaussian
distribution. These results provide a useful tool for the analysis of
sequential learning under categorical outcomes.
",0,0,1,1,0,0
294,On Deep Neural Networks for Detecting Heart Disease,"  Heart disease is the leading cause of death, and experts estimate that
approximately half of all heart attacks and strokes occur in people who have
not been flagged as ""at risk."" Thus, there is an urgent need to improve the
accuracy of heart disease diagnosis. To this end, we investigate the potential
of using data analysis, and in particular the design and use of deep neural
networks (DNNs) for detecting heart disease based on routine clinical data. Our
main contribution is the design, evaluation, and optimization of DNN
architectures of increasing depth for heart disease diagnosis. This work led to
the discovery of a novel five layer DNN architecture - named Heart Evaluation
for Algorithmic Risk-reduction and Optimization Five (HEARO-5) -- that yields
best prediction accuracy. HEARO-5's design employs regularization optimization
and automatically deals with missing data and/or data outliers. To evaluate and
tune the architectures we use k-way cross-validation as well as Matthews
correlation coefficient (MCC) to measure the quality of our classifications.
The study is performed on the publicly available Cleveland dataset of medical
information, and we are making our developments open source, to further
facilitate openness and research on the use of DNNs in medicine. The HEARO-5
architecture, yielding 99% accuracy and 0.98 MCC, significantly outperforms
currently published research in the area.
",1,0,0,1,0,0
397,Temporal correlation detection using computational phase-change memory,"  For decades, conventional computers based on the von Neumann architecture
have performed computation by repeatedly transferring data between their
processing and their memory units, which are physically separated. As
computation becomes increasingly data-centric and as the scalability limits in
terms of performance and power are being reached, alternative computing
paradigms are searched for in which computation and storage are collocated. A
fascinating new approach is that of computational memory where the physics of
nanoscale memory devices are used to perform certain computational tasks within
the memory unit in a non-von Neumann manner. Here we present a large-scale
experimental demonstration using one million phase-change memory devices
organized to perform a high-level computational primitive by exploiting the
crystallization dynamics. Also presented is an application of such a
computational memory to process real-world data-sets. The results show that
this co-existence of computation and storage at the nanometer scale could be
the enabler for new, ultra-dense, low power, and massively parallel computing
systems.
",1,0,0,0,0,0
600,"Pressure tuning of structure, superconductivity and novel magnetic order in the Ce-underdoped electron-doped cuprate T'-Pr_1.3-xLa_0.7Ce_xCuO_4 (x = 0.1)","  High-pressure neutron powder diffraction, muon-spin rotation and
magnetization studies of the structural, magnetic and the superconducting
properties of the Ce-underdoped superconducting (SC) electron-doped cuprate
system T'-Pr_1.3-xLa_0.7Ce_xCuO_4 with x = 0.1 are reported. A strong reduction
of the lattice constants a and c is observed under pressure. However, no
indication of any pressure induced phase transition from T' to T structure is
observed up to the maximum applied pressure of p = 11 GPa. Large and non-linear
increase of the short-range magnetic order temperature T_so in
T'-Pr_1.3-xLa_0.7Ce_xCuO_4 (x = 0.1) was observed under pressure.
Simultaneously pressure causes a non-linear decrease of the SC transition
temperature T_c. All these experiments establish the short-range magnetic order
as an intrinsic and a new competing phase in SC T'-Pr_1.2La_0.7Ce_0.1CuO_4. The
observed pressure effects may be interpreted in terms of the improved nesting
conditions through the reduction of the in-plane and out-of-plane lattice
constants upon hydrostatic pressure.
",0,1,0,0,0,0
8885,Portfolio Optimization in Fractional and Rough Heston Models,"  We consider a fractional version of the Heston volatility model which is
inspired by [16]. Within this model we treat portfolio optimization problems
for power utility functions. Using a suitable representation of the fractional
part, followed by a reasonable approximation we show that it is possible to
cast the problem into the classical stochastic control framework. This approach
is generic for fractional processes. We derive explicit solutions and obtain as
a by-product the Laplace transform of the integrated volatility. In order to
get rid of some undesirable features we introduce a new model for the rough
path scenario which is based on the Marchaud fractional derivative. We provide
a numerical study to underline our results.
",0,0,0,0,0,1
562,Design of the Artificial: lessons from the biological roots of general intelligence,"  Our desire and fascination with intelligent machines dates back to the
antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought
(syllogism) and Heron of Alexandria's mechanical machines and automata.
However, the quest for Artificial General Intelligence (AGI) is troubled with
repeated failures of strategies and approaches throughout the history. This
decade has seen a shift in interest towards bio-inspired software and hardware,
with the assumption that such mimicry entails intelligence. Though these steps
are fruitful in certain directions and have advanced automation, their singular
design focus renders them highly inefficient in achieving AGI. Which set of
requirements have to be met in the design of AGI? What are the limits in the
design of the artificial? Here, a careful examination of computation in
biological systems hints that evolutionary tinkering of contextual processing
of information enabled by a hierarchical architecture is the key to build AGI.
",1,1,0,0,0,0
463,Cost-Effective Seed Selection in Online Social Networks,"  We study the min-cost seed selection problem in online social networks, where
the goal is to select a set of seed nodes with the minimum total cost such that
the expected number of influenced nodes in the network exceeds a predefined
threshold. We propose several algorithms that outperform the previous studies
both on the theoretical approximation ratios and on the experimental
performance. Under the case where the nodes have heterogeneous costs, our
algorithms are the first bi- criteria approximation algorithms with polynomial
running time and provable logarithmic performance bounds using a general
contagion model. Under the case where the users have uniform costs, our
algorithms achieve logarithmic approximation ratio and provable time complexity
which is smaller than that of existing algorithms in orders of magnitude. We
conduct extensive experiments using real social networks. The experimental
results show that, our algorithms significantly outperform the existing
algorithms both on the total cost and on the running time, and also scale well
to billion-scale networks.
",1,0,0,0,0,0
232,On Scalable Inference with Stochastic Gradient Descent,"  In many applications involving large dataset or online updating, stochastic
gradient descent (SGD) provides a scalable way to compute parameter estimates
and has gained increasing popularity due to its numerical convenience and
memory efficiency. While the asymptotic properties of SGD-based estimators have
been established decades ago, statistical inference such as interval estimation
remains much unexplored. The traditional resampling method such as the
bootstrap is not computationally feasible since it requires to repeatedly draw
independent samples from the entire dataset. The plug-in method is not
applicable when there are no explicit formulas for the covariance matrix of the
estimator. In this paper, we propose a scalable inferential procedure for
stochastic gradient descent, which, upon the arrival of each observation,
updates the SGD estimate as well as a large number of randomly perturbed SGD
estimates. The proposed method is easy to implement in practice. We establish
its theoretical properties for a general class of models that includes
generalized linear models and quantile regression models as special cases. The
finite-sample performance and numerical utility is evaluated by simulation
studies and two real data applications.
",1,0,0,1,0,0
603,A Hybrid Approach to Video Source Identification,"  Multimedia Forensics allows to determine whether videos or images have been
captured with the same device, and thus, eventually, by the same person.
Currently, the most promising technology to achieve this task, exploits the
unique traces left by the camera sensor into the visual content. Anyway, image
and video source identification are still treated separately from one another.
This approach is limited and anachronistic if we consider that most of the
visual media are today acquired using smartphones, that capture both images and
videos. In this paper we overcome this limitation by exploring a new approach
that allows to synergistically exploit images and videos to study the device
from which they both come. Indeed, we prove it is possible to identify the
source of a digital video by exploiting a reference sensor pattern noise
generated from still images taken by the same device of the query video. The
proposed method provides comparable or even better performance, when compared
to the current video identification strategies, where a reference pattern is
estimated from video frames. We also show how this strategy can be effective
even in case of in-camera digitally stabilized videos, where a non-stabilized
reference is not available, by solving some state-of-the-art limitations. We
explore a possible direct application of this result, that is social media
profile linking, i.e. discovering relationships between two or more social
media profiles by comparing the visual contents - images or videos - shared
therein.
",1,0,0,0,0,0
827,Asymptotic Confidence Regions for High-dimensional Structured Sparsity,"  In the setting of high-dimensional linear regression models, we propose two
frameworks for constructing pointwise and group confidence sets for penalized
estimators which incorporate prior knowledge about the organization of the
non-zero coefficients. This is done by desparsifying the estimator as in van de
Geer et al. [18] and van de Geer and Stucky [17], then using an appropriate
estimator for the precision matrix $\Theta$. In order to estimate the precision
matrix a corresponding structured matrix norm penalty has to be introduced.
After normalization the result is an asymptotic pivot.
The asymptotic behavior is studied and simulations are added to study the
differences between the two schemes.
",0,0,1,1,0,0
910,A Domain Specific Language for Performance Portable Molecular Dynamics Algorithms,"  Developers of Molecular Dynamics (MD) codes face significant challenges when
adapting existing simulation packages to new hardware. In a continuously
diversifying hardware landscape it becomes increasingly difficult for
scientists to be experts both in their own domain (physics/chemistry/biology)
and specialists in the low level parallelisation and optimisation of their
codes. To address this challenge, we describe a ""Separation of Concerns""
approach for the development of parallel and optimised MD codes: the science
specialist writes code at a high abstraction level in a domain specific
language (DSL), which is then translated into efficient computer code by a
scientific programmer. In a related context, an abstraction for the solution of
partial differential equations with grid based methods has recently been
implemented in the (Py)OP2 library. Inspired by this approach, we develop a
Python code generation system for molecular dynamics simulations on different
parallel architectures, including massively parallel distributed memory systems
and GPUs. We demonstrate the efficiency of the auto-generated code by studying
its performance and scalability on different hardware and compare it to other
state-of-the-art simulation packages. With growing data volumes the extraction
of physically meaningful information from the simulation becomes increasingly
challenging and requires equally efficient implementations. A particular
advantage of our approach is the easy expression of such analysis algorithms.
We consider two popular methods for deducing the crystalline structure of a
material from the local environment of each atom, show how they can be
expressed in our abstraction and implement them in the code generation
framework.
",1,1,0,0,0,0
209,Clamped seismic metamaterials: Ultra-low broad frequency stop-bands,"  The regularity of earthquakes, their destructive power, and the nuisance of
ground vibration in urban environments, all motivate designs of defence
structures to lessen the impact of seismic and ground vibration waves on
buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
up to a few tens of Hz for vibrations generated by human activities, cause a
large amount of damage, or inconvenience, depending on the geological
conditions they can travel considerable distances and may match the resonant
fundamental frequency of buildings. The ultimate aim of any seismic
metamaterial, or any other seismic shield, is to protect over this entire range
of frequencies, the long wavelengths involved, and low frequency, have meant
this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin
elastic plates, can be designed to have a broad zero-frequency stop-band using
a periodic array of very small clamped circles. Inspired by this experimental
and theoretical observation, all be it in a situation far removed from seismic
waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
and body (pressure P and shear S) wave reflectors at very large wavelengths in
structured soils modelled as a fully elastic layer periodically clamped to
bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns
of concrete clamped at their base to the bedrock. In a realistic configuration
of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
covering a broad frequency range of $0$ to $30$ Hz.
",0,1,0,0,0,0
540,Vanishing theorems for the negative K-theory of stacks,"  We prove that the homotopy algebraic K-theory of tame quasi-DM stacks
satisfies cdh-descent. We apply this descent result to prove that if X is a
Noetherian tame quasi-DM stack and i < -dim(X), then K_i(X)[1/n] = 0 (resp.
K_i(X, Z/n) = 0) provided that n is nilpotent on X (resp. is invertible on X).
Our descent and vanishing results apply more generally to certain Artin stacks
whose stabilizers are extensions of finite group schemes by group schemes of
multiplicative type.
",0,0,1,0,0,0
13860,Stock Market Visualization,"  We provide complete source code for a front-end GUI and its back-end
counterpart for a stock market visualization tool. It is built based on the
""functional visualization"" concept we discuss, whereby functionality is not
sacrificed for fancy graphics. The GUI, among other things, displays a
color-coded signal (computed by the back-end code) based on how ""out-of-whack""
each stock is trading compared with its peers (""mean-reversion""), and the most
sizable changes in the signal (""momentum""). The GUI also allows to efficiently
filter/tier stocks by various parameters (e.g., sector, exchange, signal,
liquidity, market cap) and functionally display them. The tool can be run as a
web-based or local application.
",0,0,0,0,0,1
679,"An efficient data structure for counting all linear extensions of a poset, calculating its jump number, and the likes","  Achieving the goals in the title (and others) relies on a cardinality-wise
scanning of the ideals of the poset. Specifically, the relevant numbers
attached to the k+1 element ideals are inferred from the corresponding numbers
of the k-element (order) ideals. Crucial in all of this is a compressed
representation (using wildcards) of the ideal lattice. The whole scheme invites
distributed computation.
",1,0,0,0,0,0
302,La notion d'involution dans le Brouillon Project de Girard Desargues,"  Nous tentons dans cet article de proposer une thèse cohérente concernant
la formation de la notion d'involution dans le Brouillon Project de Desargues.
Pour cela, nous donnons une analyse détaillée des dix premières pages
dudit Brouillon, comprenant les développements de cas particuliers qui aident
à comprendre l'intention de Desargues. Nous mettons cette analyse en regard
de la lecture qu'en fait Jean de Beaugrand et que l'on trouve dans les Advis
Charitables.
The purpose of this article is to propose a coherent thesis on how Girard
Desargues arrived at the notion of involution in his Brouillon Project of 1639.
To this purpose we give a detailed analysis of the ten first pages of the
Brouillon, including developments of particular cases which help to understand
the goal of Desargues, as well as to clarify the links between the notion of
involution and that of harmonic division. We compare the conclusions of this
analysis with the very critical reading Jean de Beaugrand made of the Brouillon
Project in the Advis Charitables of 1640.
",0,0,1,0,0,0
2441,Classification of grasping tasks based on EEG-EMG coherence,"  This work presents an innovative application of the well-known concept of
cortico-muscular coherence for the classification of various motor tasks, i.e.,
grasps of different kinds of objects. Our approach can classify objects with
different weights (motor-related features) and different surface frictions
(haptics-related features) with high accuracy (over 0:8). The outcomes
presented here provide information about the synchronization existing between
the brain and the muscles during specific activities; thus, this may represent
a new effective way to perform activity recognition.
",0,0,0,0,1,0
219,"Intersections of $ω$ classes in $\overline{\mathcal{M}}_{g,n}$","  We provide a graph formula which describes an arbitrary monomial in {\omega}
classes (also referred to as stable {\psi} classes) in terms of a simple family
of dual graphs (pinwheel graphs) with edges decorated by rational functions in
{\psi} classes. We deduce some numerical consequences and in particular a
combinatorial formula expressing top intersections of \k{appa} classes on Mg in
terms of top intersections of {\psi} classes.
",0,0,1,0,0,0
14054,On the degree of incompleteness of an incomplete financial market,"  In order to find a way of measuring the degree of incompleteness of an
incomplete financial market, the rank of the vector price process of the traded
assets and the dimension of the associated acceptance set are introduced. We
show that they are equal and state a variety of consequences.
",0,0,0,0,0,1
660,Asymmetric Mach-Zehnder atom interferometers,"  It is shown that using beam splitters with non-equal wave vectors results in
a new recoil diagram which is qualitatively different from the well-known
diagram associated with the Mach-Zehnder atom interferometer. We predict a new
asymmetric Mach-Zehnder atom interferometer (AMZAI) and study it when one uses
a Raman beam splitter. The main feature is that the phase of AMZAI contains a
quantum part proportional to the recoil frequency. A response sensitive only to
the quantum phase was found. A new technique to measure the recoil frequency
and fine structure constant is proposed and studied outside of the Raman-Nath
approximation.
",0,1,0,0,0,0
561,A Survey of Deep Learning Techniques for Mobile Robot Applications,"  Advancements in deep learning over the years have attracted research into how
deep artificial neural networks can be used in robotic systems. This research
survey will present a summarization of the current research with a specific
focus on the gains and obstacles for deep learning to be applied to mobile
robotics.
",1,0,0,0,0,0
1340,Protein Folding and Machine Learning: Fundamentals,"  In spite of decades of research, much remains to be discovered about folding:
the detailed structure of the initial (unfolded) state, vestigial folding
instructions remaining only in the unfolded state, the interaction of the
molecule with the solvent, instantaneous power at each point within the
molecule during folding, the fact that the process is stable in spite of myriad
possible disturbances, potential stabilization of trajectory by chaos, and, of
course, the exact physical mechanism (code or instructions) by which the
folding process is specified in the amino acid sequence. Simulations based upon
microscopic physics have had some spectacular successes and continue to
improve, particularly as super-computer capabilities increase. The simulations,
exciting as they are, are still too slow and expensive to deal with the
enormous number of molecules of interest. In this paper, we introduce an
approximate model based upon physics, empirics, and information science which
is proposed for use in machine learning applications in which very large
numbers of sub-simulations must be made. In particular, we focus upon machine
learning applications in the learning phase and argue that our model is
sufficiently close to the physics that, in spite of its approximate nature, can
facilitate stepping through machine learning solutions to explore the mechanics
of folding mentioned above. We particularly emphasize the exploration of energy
flow (power) within the molecule during folding, the possibility of energy
scale invariance (above a threshold), vestigial information in the unfolded
state as attractive targets for such machine language analysis, and statistical
analysis of an ensemble of folding micro-steps.
",0,0,0,0,1,0
422,Rethinking Information Sharing for Actionable Threat Intelligence,"  In the past decade, the information security and threat landscape has grown
significantly making it difficult for a single defender to defend against all
attacks at the same time. This called for introduc- ing information sharing, a
paradigm in which threat indicators are shared in a community of trust to
facilitate defenses. Standards for representation, exchange, and consumption of
indicators are pro- posed in the literature, although various issues are
undermined. In this paper, we rethink information sharing for actionable
intelli- gence, by highlighting various issues that deserve further explo-
ration. We argue that information sharing can benefit from well- defined use
models, threat models, well-understood risk by mea- surement and robust
scoring, well-understood and preserved pri- vacy and quality of indicators and
robust mechanism to avoid free riding behavior of selfish agent. We call for
using the differential nature of data and community structures for optimizing
sharing.
",1,0,0,0,0,0
6457,Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors in Deep Learning,"  We introduce Error Forward-Propagation, a biologically plausible mechanism to
propagate error feedback forward through the network. Architectural constraints
on connectivity are virtually eliminated for error feedback in the brain;
systematic backward connectivity is not used or needed to deliver error
feedback. Feedback as a means of assigning credit to neurons earlier in the
forward pathway for their contribution to the final output is thought to be
used in learning in the brain. How the brain solves the credit assignment
problem is unclear. In machine learning, error backpropagation is a highly
successful mechanism for credit assignment in deep multilayered networks.
Backpropagation requires symmetric reciprocal connectivity for every neuron.
From a biological perspective, there is no evidence of such an architectural
constraint, which makes backpropagation implausible for learning in the brain.
This architectural constraint is reduced with the use of random feedback
weights. Models using random feedback weights require backward connectivity
patterns for every neuron, but avoid symmetric weights and reciprocal
connections. In this paper, we practically remove this architectural
constraint, requiring only a backward loop connection for effective error
feedback. We propose reusing the forward connections to deliver the error
feedback by feeding the outputs into the input receiving layer. This mechanism,
Error Forward-Propagation, is a plausible basis for how error feedback occurs
deep in the brain independent of and yet in support of the functionality
underlying intricate network architectures. We show experimentally that
recurrent neural networks with two and three hidden layers can be trained using
Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving
$1.90\%$ and $11\%$ generalization errors respectively.
",0,0,0,0,1,0
5894,Technological Parasitism,"  Technological parasitism is a new theory to explain the evolution of
technology in society. In this context, this study proposes a model to analyze
the interaction between a host technology (system) and a parasitic technology
(subsystem) to explain evolutionary pathways of technologies as complex
systems. The coefficient of evolutionary growth of the model here indicates the
typology of evolution of parasitic technology in relation to host technology:
i.e., underdevelopment, growth and development. This approach is illustrated
with realistic examples using empirical data of product and process
technologies. Overall, then, the theory of technological parasitism can be
useful for bringing a new perspective to explain and generalize the evolution
of technology and predict which innovations are likely to evolve rapidly in
society.
",0,0,0,0,0,1
580,Slow and Long-ranged Dynamical Heterogeneities in Dissipative Fluids,"  A two-dimensional bidisperse granular fluid is shown to exhibit pronounced
long-ranged dynamical heterogeneities as dynamical arrest is approached. Here
we focus on the most direct approach to study these heterogeneities: we
identify clusters of slow particles and determine their size, $N_c$, and their
radius of gyration, $R_G$. We show that $N_c\propto R_G^{d_f}$, providing
direct evidence that the most immobile particles arrange in fractal objects
with a fractal dimension, $d_f$, that is observed to increase with packing
fraction $\phi$. The cluster size distribution obeys scaling, approaching an
algebraic decay in the limit of structural arrest, i.e., $\phi\to\phi_c$.
Alternatively, dynamical heterogeneities are analyzed via the four-point
structure factor $S_4(q,t)$ and the dynamical susceptibility $\chi_4(t)$.
$S_4(q,t)$ is shown to obey scaling in the full range of packing fractions,
$0.6\leq\phi\leq 0.805$, and to become increasingly long-ranged as
$\phi\to\phi_c$. Finite size scaling of $\chi_4(t)$ provides a consistency
check for the previously analyzed divergences of $\chi_4(t)\propto
(\phi-\phi_c)^{-\gamma_{\chi}}$ and the correlation length $\xi\propto
(\phi-\phi_c)^{-\gamma_{\xi}}$. We check the robustness of our results with
respect to our definition of mobility. The divergences and the scaling for
$\phi\to\phi_c$ suggest a non-equilibrium glass transition which seems
qualitatively independent of the coefficient of restitution.
",0,1,0,0,0,0
2922,Dynamic Clearing and Contagion in Financial Networks,"  In this paper we will consider a generalized extension of the Eisenberg-Noe
model of financial contagion to allow for time dynamics in both discrete and
continuous time. Derivation and interpretation of the financial implications
will be provided. Emphasis will be placed on the continuous-time framework and
its formulation as a differential equation driven by the operating cash flows.
Mathematical results on existence and uniqueness of firm wealths under the
discrete and continuous-time models will be provided. Finally, the financial
implications of time dynamics will be considered. The focus will be on how the
dynamic clearing solutions differ from those of the static Eisenberg-Noe model.
",0,0,0,0,0,1
3285,Jensen's force and the statistical mechanics of cortical asynchronous states,"  The cortex exhibits self-sustained highly-irregular activity even under
resting conditions, whose origin and function need to be fully understood. It
is believed that this can be described as an ""asynchronous state"" stemming from
the balance between excitation and inhibition, with important consequences for
information-processing, though a competing hypothesis claims it stems from
critical dynamics. By analyzing a parsimonious neural-network model with
excitatory and inhibitory interactions, we elucidate a noise-induced mechanism
called ""Jensen's force"" responsible for the emergence of a novel phase of
arbitrarily-low but self-sustained activity, which reproduces all the
experimental features of asynchronous states. The simplicity of our framework
allows for a deep understanding of asynchronous states from a broad
statistical-mechanics perspective and of the phase transitions to other
standard phases it exhibits, opening the door to reconcile, asynchronous-state
and critical-state hypotheses. We argue that Jensen's forces are measurable
experimentally and might be relevant in contexts beyond neuroscience.
",0,0,0,0,1,0
291,A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization,"  This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling.
",1,0,0,1,0,0
490,Bounded gaps between primes in short intervals,"  Baker, Harman, and Pintz showed that a weak form of the Prime Number Theorem
holds in intervals of the form $[x-x^{0.525},x]$ for large $x$. In this paper,
we extend a result of Maynard and Tao concerning small gaps between primes to
intervals of this length. More precisely, we prove that for any $\delta\in
[0.525,1]$ there exist positive integers $k,d$ such that for sufficiently large
$x$, the interval $[x-x^\delta,x]$ contains $\gg_{k} \frac{x^\delta}{(\log
x)^k}$ pairs of consecutive primes differing by at most $d$. This confirms a
speculation of Maynard that results on small gaps between primes can be refined
to the setting of short intervals of this length.
",0,0,1,0,0,0
817,"Physical problem solving: Joint planning with symbolic, geometric, and dynamic constraints","  In this paper, we present a new task that investigates how people interact
with and make judgments about towers of blocks. In Experiment~1, participants
in the lab solved a series of problems in which they had to re-configure three
blocks from an initial to a final configuration. We recorded whether they used
one hand or two hands to do so. In Experiment~2, we asked participants online
to judge whether they think the person in the lab used one or two hands. The
results revealed a close correspondence between participants' actions in the
lab, and the mental simulations of participants online. To explain
participants' actions and mental simulations, we develop a model that plans
over a symbolic representation of the situation, executes the plan using a
geometric solver, and checks the plan's feasibility by taking into account the
physical constraints of the scene. Our model explains participants' actions and
judgments to a high degree of quantitative accuracy.
",1,0,0,1,0,0
378,On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising,"  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for
estimating Gaussian location mixture densities in $d$-dimensions from
independent observations. Unlike usual likelihood-based methods for fitting
mixtures, NPMLEs are based on convex optimization. We prove finite sample
results on the Hellinger accuracy of every NPMLE. Our results imply, in
particular, that every NPMLE achieves near parametric risk (up to logarithmic
multiplicative factors) when the true density is a discrete Gaussian mixture
without any prior information on the number of mixture components. NPMLEs can
naturally be used to yield empirical Bayes estimates of the Oracle Bayes
estimator in the Gaussian denoising problem. We prove bounds for the accuracy
of the empirical Bayes estimate as an approximation to the Oracle Bayes
estimator. Here our results imply that the empirical Bayes estimator performs
at nearly the optimal level (up to logarithmic multiplicative factors) for
denoising in clustering situations without any prior knowledge of the number of
clusters.
",0,0,1,1,0,0
13879,An Efficient Approach for Removing Look-ahead Bias in the Least Square Monte Carlo Algorithm: Leave-One-Out,"  The least square Monte Carlo (LSM) algorithm proposed by Longstaff and
Schwartz [2001] is the most widely used method for pricing options with early
exercise features. The LSM estimator contains look-ahead bias, and the
conventional technique of removing it necessitates an independent set of
simulations. This study proposes a new approach for efficiently eliminating
look-ahead bias by using the leave-one-out method, a well-known
cross-validation technique for machine learning applications. The leave-one-out
LSM (LOOLSM) method is illustrated with examples, including multi-asset options
whose LSM price is biased high. The asymptotic behavior of look-ahead bias is
also discussed with the LOOLSM approach.
",0,0,0,0,0,1
244,Robust Estimation of Change-Point Location,"  We introduce a robust estimator of the location parameter for the
change-point in the mean based on the Wilcoxon statistic and establish its
consistency for $L_1$ near epoch dependent processes. It is shown that the
consistency rate depends on the magnitude of change. A simulation study is
performed to evaluate finite sample properties of the Wilcoxon-type estimator
in standard cases, as well as under heavy-tailed distributions and disturbances
by outliers, and to compare it with a CUSUM-type estimator. It shows that the
Wilcoxon-type estimator is equivalent to the CUSUM-type estimator in standard
cases, but outperforms the CUSUM-type estimator in presence of heavy tails or
outliers in the data.
",0,0,1,1,0,0
416,A homotopy decomposition of the fibre of the squaring map on $Ω^3S^{17}$,"  We use Richter's $2$-primary proof of Gray's conjecture to give a homotopy
decomposition of the fibre $\Omega^3S^{17}\{2\}$ of the $H$-space squaring map
on the triple loop space of the $17$-sphere. This induces a splitting of the
mod-$2$ homotopy groups $\pi_\ast(S^{17}; \mathbb{Z}/2\mathbb{Z})$ in terms of
the integral homotopy groups of the fibre of the double suspension
$E^2:S^{2n-1} \to \Omega^2S^{2n+1}$ and refines a result of Cohen and Selick,
who gave similar decompositions for $S^5$ and $S^9$. We relate these
decompositions to various Whitehead products in the homotopy groups of mod-$2$
Moore spaces and Stiefel manifolds to show that the Whitehead square $[i_{2n},
i_{2n}]$ of the inclusion of the bottom cell of the Moore space $P^{2n+1}(2)$
is divisible by $2$ if and only if $2n=2, 4, 8$ or $16$.
",0,0,1,0,0,0
6162,New face of multifractality: Multi-branched left-sidedness and phase transitions in multifractality of interevent times,"  We develop an extended multifractal analysis based on the Legendre-Fenchel
transform rather than the routinely used Legendre transform. We apply this
analysis to studying time series consisting of inter-event times. As a result,
we discern the non-monotonic behavior of the generalized Hurst exponent - the
fundamental exponent studied by us - and hence a multi-branched left-sided
spectrum of dimensions. This kind of multifractality is a direct result of the
non-monotonic behavior of the generalized Hurst exponent and is not caused by
non-analytic behavior as has been previously suggested. We examine the main
thermodynamic consequences of the existence of this type of multifractality
related to the thermal stable, metastable, and unstable phases within a
hierarchy of fluctuations, and also to the first and second order phase
transitions between them.
",0,0,0,0,0,1
455,Multiphase Flows of N Immiscible Incompressible Fluids: An Outflow/Open Boundary Condition and Algorithm,"  We present a set of effective outflow/open boundary conditions and an
associated algorithm for simulating the dynamics of multiphase flows consisting
of $N$ ($N\geqslant 2$) immiscible incompressible fluids in domains involving
outflows or open boundaries. These boundary conditions are devised based on the
properties of energy stability and reduction consistency. The energy stability
property ensures that the contributions of these boundary conditions to the
energy balance will not cause the total energy of the N-phase system to
increase over time. Therefore, these open/outflow boundary conditions are very
effective in overcoming the backflow instability in multiphase systems. The
reduction consistency property ensures that if some fluid components are absent
from the N-phase system then these N-phase boundary conditions will reduce to
those corresponding boundary conditions for the equivalent smaller system. Our
numerical algorithm for the proposed boundary conditions together with the
N-phase governing equations involves only the solution of a set of de-coupled
individual Helmholtz-type equations within each time step, and the resultant
linear algebraic systems after discretization involve only constant and
time-independent coefficient matrices which can be pre-computed. Therefore, the
algorithm is computationally very efficient and attractive. We present
extensive numerical experiments for flow problems involving multiple fluid
components and inflow/outflow boundaries to test the proposed method. In
particular, we compare in detail the simulation results of a three-phase
capillary wave problem with Prosperetti's exact physical solution and
demonstrate that the method developed herein produces physically accurate
results.
",0,1,0,0,0,0
1488,Exact Inference of Causal Relations in Dynamical Systems,"  From philosophers of ancient times to modern economists, biologists and other
researchers are engaged in revealing causal relations. The most challenging
problem is inferring the type of the causal relationship: whether it is uni- or
bi-directional or only apparent - implied by a hidden common cause only. Modern
technology provides us tools to record data from complex systems such as the
ecosystem of our planet or the human brain, but understanding their functioning
needs detection and distinction of causal relationships of the system
components without interventions. Here we present a new method, which
distinguishes and assigns probabilities to the presence of all the possible
causal relations between two or more time series from dynamical systems. The
new method is validated on synthetic datasets and applied to EEG
(electroencephalographic) data recorded in epileptic patients. Given the
universality of our method, it may find application in many fields of science.
",0,0,0,0,1,0
867,A Robust Multi-Batch L-BFGS Method for Machine Learning,"  This paper describes an implementation of the L-BFGS method designed to deal
with two adversarial situations. The first occurs in distributed computing
environments where some of the computational nodes devoted to the evaluation of
the function and gradient are unable to return results on time. A similar
challenge occurs in a multi-batch approach in which the data points used to
compute function and gradients are purposely changed at each iteration to
accelerate the learning process. Difficulties arise because L-BFGS employs
gradient differences to update the Hessian approximations, and when these
gradients are computed using different data points the updating process can be
unstable. This paper shows how to perform stable quasi-Newton updating in the
multi-batch setting, studies the convergence properties for both convex and
nonconvex functions, and illustrates the behavior of the algorithm in a
distributed computing platform on binary classification logistic regression and
neural network training problems that arise in machine learning.
",1,0,1,1,0,0
831,Optimal Timing of Decisions: A General Theory Based on Continuation Values,"  Building on insights of Jovanovic (1982) and subsequent authors, we develop a
comprehensive theory of optimal timing of decisions based around continuation
value functions and operators that act on them. Optimality results are provided
under general settings, with bounded or unbounded reward functions. This
approach has several intrinsic advantages that we exploit in developing the
theory. One is that continuation value functions are smoother than value
functions, allowing for sharper analysis of optimal policies and more efficient
computation. Another is that, for a range of problems, the continuation value
function exists in a lower dimensional space than the value function,
mitigating the curse of dimensionality. In one typical experiment, this reduces
the computation time from over a week to less than three minutes.
",0,0,1,0,0,0
904,Adaptive Quantization for Deep Neural Network,"  In recent years Deep Neural Networks (DNNs) have been rapidly developed in
various applications, together with increasingly complex architectures. The
performance gain of these DNNs generally comes with high computational costs
and large memory consumption, which may not be affordable for mobile platforms.
Deep model quantization can be used for reducing the computation and memory
costs of DNNs, and deploying complex DNNs on mobile equipment. In this work, we
propose an optimization framework for deep model quantization. First, we
propose a measurement to estimate the effect of parameter quantization errors
in individual layers on the overall model prediction accuracy. Then, we propose
an optimization process based on this measurement for finding optimal
quantization bit-width for each layer. This is the first work that
theoretically analyse the relationship between parameter quantization errors of
individual layers and model accuracy. Our new quantization algorithm
outperforms previous quantization optimization methods, and achieves 20-40%
higher compression rate compared to equal bit-width quantization at the same
model prediction accuracy.
",1,0,0,1,0,0
479,Dimension-free Wasserstein contraction of nonlinear filters,"  For a class of partially observed diffusions, sufficient conditions are given
for the map from initial condition of the signal to filtering distribution to
be contractive with respect to Wasserstein distances, with rate which has no
dependence on the dimension of the state-space and is stable under tensor
products of the model. The main assumptions are that the signal has affine
drift and constant diffusion coefficient, and that the likelihood functions are
log-concave. Contraction estimates are obtained from an $h$-process
representation of the transition probabilities of the signal reweighted so as
to condition on the observations.
",0,0,1,1,0,0
1668,Phylogenetic networks that are their own fold-ups,"  Phylogenetic networks are becoming of increasing interest to evolutionary
biologists due to their ability to capture complex non-treelike evolutionary
processes. From a combinatorial point of view, such networks are certain types
of rooted directed acyclic graphs whose leaves are labelled by, for example,
species. A number of mathematically interesting classes of phylogenetic
networks are known. These include the biologically relevant class of stable
phylogenetic networks whose members are defined via certain fold-up and un-fold
operations that link them with concepts arising within the theory of, for
example, graph fibrations. Despite this exciting link, the structural
complexity of stable phylogenetic networks is still relatively poorly
understood. Employing the popular tree-based, reticulation-visible, and
tree-child properties which allow one to gauge this complexity in one way or
another, we provide novel characterizations for when a stable phylogenetic
network satisfies either one of these three properties.
",0,0,0,0,1,0
970,Structural changes in the interbank market across the financial crisis from multiple core-periphery analysis,"  Interbank markets are often characterised in terms of a core-periphery
network structure, with a highly interconnected core of banks holding the
market together, and a periphery of banks connected mostly to the core but not
internally. This paradigm has recently been challenged for short time scales,
where interbank markets seem better characterised by a bipartite structure with
more core-periphery connections than inside the core. Using a novel
core-periphery detection method on the eMID interbank market, we enrich this
picture by showing that the network is actually characterised by multiple
core-periphery pairs. Moreover, a transition from core-periphery to bipartite
structures occurs by shortening the temporal scale of data aggregation. We
further show how the global financial crisis transformed the market, in terms
of composition, multiplicity and internal organisation of core-periphery pairs.
By unveiling such a fine-grained organisation and transformation of the
interbank market, our method can find important applications in the
understanding of how distress can propagate over financial networks.
",0,0,0,0,0,1
735,A Theory of Complex Stochastic Systems with Two Types of Counteracting Entities,"  Many complex systems share two characteristics: 1) they are stochastic in
nature, and 2) they are characterized by a large number of factors. At the same
time, various natural complex systems appear to have two types of intertwined
constituents that exhibit counteracting effects on their equilibrium. In this
study, we employ these few characteristics to lay the groundwork for analyzing
such complex systems. The equilibrium point of these systems is generally
studied either through the kinetic notion of equilibrium or its energetic
notion, but not both. We postulate that these systems attempt to regulate the
state vector of their constituents such that both the kinetic and the energetic
notions of equilibrium are met. Based on this postulate, we prove: 1) the
existence of a point such that the kinetic notion of equilibrium is met for the
less abundant constituents and, at the same time, the state vector of more
abundant entities is regulated to minimize the energetic notion of equilibrium;
2) the effect of unboundedly increasing less (more) abundant constituents
stabilizes (destabilizes) the system; and 3) the (unrestricted) equilibrium of
the system is the point at which the number of stabilizing and destabilizing
entities increase unboundedly with the same rate.
",0,1,0,0,0,0
11604,A Machine Learning Framework for Stock Selection,"  This paper demonstrates how to apply machine learning algorithms to
distinguish good stocks from the bad stocks. To this end, we construct 244
technical and fundamental features to characterize each stock, and label stocks
according to their ranking with respect to the return-to-volatility ratio.
Algorithms ranging from traditional statistical learning methods to recently
popular deep learning method, e.g. Logistic Regression (LR), Random Forest
(RF), Deep Neural Network (DNN), and the Stacking, are trained to solve the
classification task. Genetic Algorithm (GA) is also used to implement feature
selection. The effectiveness of the stock selection strategy is validated in
Chinese stock market in both statistical and practical aspects, showing that:
1) Stacking outperforms other models reaching an AUC score of 0.972; 2) Genetic
Algorithm picks a subset of 114 features and the prediction performances of all
models remain almost unchanged after the selection procedure, which suggests
some features are indeed redundant; 3) LR and DNN are radical models; RF is
risk-neutral model; Stacking is somewhere between DNN and RF. 4) The portfolios
constructed by our models outperform market average in back tests.
",0,0,0,1,0,1
352,Neeman's characterization of K(R-Proj) via Bousfield localization,"  Let $R$ be an associative ring with unit and denote by $K({\rm R
\mbox{-}Proj})$ the homotopy category of complexes of projective left
$R$-modules. Neeman proved the theorem that $K({\rm R \mbox{-}Proj})$ is
$\aleph_1$-compactly generated, with the category $K^+ ({\rm R \mbox{-}proj})$
of left bounded complexes of finitely generated projective $R$-modules
providing an essentially small class of such generators. Another proof of
Neeman's theorem is explained, using recent ideas of Christensen and Holm, and
Emmanouil. The strategy of the proof is to show that every complex in $K({\rm R
\mbox{-}Proj})$ vanishes in the Bousfield localization $K({\rm R
\mbox{-}Flat})/\langle K^+ ({\rm R \mbox{-}proj}) \rangle.$
",0,0,1,0,0,0
619,Representing Hybrid Automata by Action Language Modulo Theories,"  Both hybrid automata and action languages are formalisms for describing the
evolution of dynamic systems. This paper establishes a formal relationship
between them. We show how to succinctly represent hybrid automata in an action
language which in turn is defined as a high-level notation for answer set
programming modulo theories (ASPMT) --- an extension of answer set programs to
the first-order level similar to the way satisfiability modulo theories (SMT)
extends propositional satisfiability (SAT). We first show how to represent
linear hybrid automata with convex invariants by an action language modulo
theories. A further translation into SMT allows for computing them using SMT
solvers that support arithmetic over reals. Next, we extend the representation
to the general class of non-linear hybrid automata allowing even non-convex
invariants. We represent them by an action language modulo ODE (Ordinary
Differential Equations), which can be compiled into satisfiability modulo ODE.
We developed a prototype system cplus2aspmt based on these translations, which
allows for a succinct representation of hybrid transition systems that can be
computed effectively by the state-of-the-art SMT solver dReal.
",1,0,0,0,0,0
12049,"Private Information, Credit Risk and Graph Structure in P2P Lending Networks","  This research investigated the potential for improving Peer-to-Peer (P2P)
credit scoring by using ""private information"" about communications and travels
of borrowers. We found that P2P borrowers' ego networks exhibit scale-free
behavior driven by underlying preferential attachment mechanisms that connect
borrowers in a fashion that can be used to predict loan profitability. The
projection of these private networks onto networks of mobile phone
communication and geographical locations from mobile phone GPS potentially give
loan providers access to private information through graph and location metrics
which we used to predict loan profitability. Graph topology was found to be an
important predictor of loan profitability, explaining over 5.5% of variability.
Networks of borrower location information explain an additional 19% of the
profitability. Machine learning algorithms were applied to the data set
previously analyzed to develop the predictive model and resulted in a 4%
reduction in mean squared error.
",1,0,0,0,0,1
452,"From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter","  We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.
",0,1,0,0,0,0
218,Interpretations of family size distributions: The Datura example,"  Young asteroid families are unique sources of information about fragmentation
physics and the structure of their parent bodies, since their physical
properties have not changed much since their birth. Families have different
properties such as age, size, taxonomy, collision severity and others, and
understanding the effect of those properties on our observations of the
size-frequency distribution (SFD) of family fragments can give us important
insights into the hypervelocity collision processes at scales we cannot achieve
in our laboratories. Here we take as an example the very young Datura family,
with a small 8-km parent body, and compare its size distribution to other
families, with both large and small parent bodies, and created by both
catastrophic and cratering formation events. We conclude that most likely
explanation for the shallower size distribution compared to larger families is
a more pronounced observational bias because of its small size. Its size
distribution is perfectly normal when its parent body size is taken into
account. We also discuss some other possibilities. In addition, we study
another common feature: an offset or ""bump"" in the distribution occurring for a
few of the larger elements. We hypothesize that it can be explained by a newly
described regime of cratering, ""spall cratering"", which controls the majority
of impact craters on the surface of small asteroids like Datura.
",0,1,0,0,0,0
6425,Optimal VWAP execution under transient price impact,"  We solve the problem of optimal liquidation with volume weighted average
price (VWAP) benchmark when the market impact is linear and transient. Our
setting is indeed more general as it considers the case when the trading
interval is not necessarily coincident with the benchmark interval:
Implementation Shortfall and Target Close execution are shown to be particular
cases of our setting. We find explicit solutions in continuous and discrete
time considering risk averse investors having a CARA utility function. Finally,
we show that, contrary to what is observed for Implementation Shortfall, the
optimal VWAP solution contains both buy and sell trades also when the decay
kernel is convex.
",0,0,0,0,0,1
5285,The unreasonable effectiveness of small neural ensembles in high-dimensional brain,"  Despite the widely-spread consensus on the brain complexity, sprouts of the
single neuron revolution emerged in neuroscience in the 1970s. They brought
many unexpected discoveries, including grandmother or concept cells and sparse
coding of information in the brain.
In machine learning for a long time, the famous curse of dimensionality
seemed to be an unsolvable problem. Nevertheless, the idea of the blessing of
dimensionality becomes gradually more and more popular. Ensembles of
non-interacting or weakly interacting simple units prove to be an effective
tool for solving essentially multidimensional problems. This approach is
especially useful for one-shot (non-iterative) correction of errors in large
legacy artificial intelligence systems.
These simplicity revolutions in the era of complexity have deep fundamental
reasons grounded in geometry of multidimensional data spaces. To explore and
understand these reasons we revisit the background ideas of statistical
physics. In the course of the 20th century they were developed into the
concentration of measure theory. New stochastic separation theorems reveal the
fine structure of the data clouds.
We review and analyse biological, physical, and mathematical problems at the
core of the fundamental question: how can high-dimensional brain organise
reliable and fast learning in high-dimensional world of data by simple tools?
Two critical applications are reviewed to exemplify the approach: one-shot
correction of errors in intellectual systems and emergence of static and
associative memories in ensembles of single neurons.
",0,0,0,0,1,0
444,Position Aided Beam Alignment for Millimeter Wave Backhaul Systems with Large Phased Arrays,"  Wireless backhaul communication has been recently realized with large
antennas operating in the millimeter wave (mmWave) frequency band and
implementing highly directional beamforming. In this paper, we focus on the
alignment problem of narrow beams between fixed position network nodes in
mmWave backhaul systems that are subject to small displacements due to wind
flow or ground vibration. We consider nodes equipped with antenna arrays that
are capable of performing only analog processing and communicate through
wireless channels including a line-of-sight component. Aiming at minimizing the
time needed to achieve beam alignment, we present an efficient method that
capitalizes on the exchange of position information between the nodes to design
their beamforming and combining vectors. Some numerical results on the outage
probability with the proposed beam alignment method offer useful preliminary
insights on the impact of some system and operation parameters.
",1,0,1,0,0,0
631,Spin mediated enhanced negative magnetoresistance in Ni80Fe20 and p-silicon bilayer,"  In this work, we present an experimental study of spin mediated enhanced
negative magnetoresistance in Ni80Fe20 (50 nm)/p-Si (350 nm) bilayer. The
resistance measurement shows a reduction of ~2.5% for the bilayer specimen as
compared to 1.3% for Ni80Fe20 (50 nm) on oxide specimen for an out-of-plane
applied magnetic field of 3T. In the Ni80Fe20-only film, the negative
magnetoresistance behavior is attributed to anisotropic magnetoresistance. We
propose that spin polarization due to spin-Hall effect is the underlying cause
of the enhanced negative magnetoresistance observed in the bilayer. Silicon has
weak spin orbit coupling so spin Hall magnetoresistance measurement is not
feasible. We use V2{\omega} and V3{\omega} measurement as a function of
magnetic field and angular rotation of magnetic field in direction normal to
electric current to elucidate the spin-Hall effect. The angular rotation of
magnetic field shows a sinusoidal behavior for both V2{\omega} and V3{\omega},
which is attributed to the spin phonon interactions resulting from the
spin-Hall effect mediated spin polarization. We propose that the spin
polarization leads to a decrease in hole-phonon scattering resulting in
enhanced negative magnetoresistance.
",0,1,0,0,0,0
3214,Rapid micro fluorescence in situ hybridization in tissue sections,"  This paper describes a micro fluorescence in situ hybridization
({\mu}FISH)-based rapid detection of cytogenetic biomarkers on formalin-fixed
paraffin embedded (FFPE) tissue sections. We demonstrated this method in the
context of detecting human epidermal growth factor 2 (HER2) in breast tissue
sections. This method uses a non-contact microfluidic scanning probe (MFP),
which localizes FISH probes at the micrometer length-scale to selected cells of
the tissue section. The scanning ability of the MFP allows for a versatile
implementation of FISH on tissue sections. We demonstrated the use of
oligonucleotide FISH probes in ethylene carbonate-based buffer enabling rapid
hybridization within < 1 min for chromosome enumeration and 10-15 min for
assessment of the HER2 status in FFPE sections. We further demonstrated
recycling of FISH probes for multiple sequential tests using a defined volume
of probes by forming hierarchical hydrodynamic flow confinements. This
microscale method is compatible with the standard FISH protocols and with the
Instant Quality (IQ) FISH assay, reduces the FISH probe consumption ~100-fold
and the hybridization time 4-fold, resulting in an assay turnaround time of < 3
h. We believe rapid {\mu}FISH has the potential of being used in pathology
workflows as a standalone method or in combination with other molecular methods
for diagnostic and prognostic analysis of FFPE sections.
",0,0,0,0,1,0
3770,Particle-without-Particle: a practical pseudospectral collocation method for linear partial differential equations with distributional sources,"  Partial differential equations with distributional sources---in particular,
involving (derivatives of) delta distributions---have become increasingly
ubiquitous in numerous areas of physics and applied mathematics. It is often of
considerable interest to obtain numerical solutions for such equations, but any
singular (""particle""-like) source modeling invariably introduces nontrivial
computational obstacles. A common method to circumvent these is through some
form of delta function approximation procedure on the computational grid;
however, this often carries significant limitations on the efficiency of the
numerical convergence rates, or sometimes even the resolvability of the problem
at all.
In this paper, we present an alternative technique for tackling such
equations which avoids the singular behavior entirely: the
""Particle-without-Particle"" method. Previously introduced in the context of the
self-force problem in gravitational physics, the idea is to discretize the
computational domain into two (or more) disjoint pseudospectral
(Chebyshev-Lobatto) grids such that the ""particle"" is always at the interface
between them; thus, one only needs to solve homogeneous equations in each
domain, with the source effectively replaced by jump (boundary) conditions
thereon. We prove here that this method yields solutions to any linear PDE the
source of which is any linear combination of delta distributions and
derivatives thereof supported on a one-dimensional subspace of the problem
domain. We then implement it to numerically solve a variety of relevant PDEs:
hyperbolic (with applications to neuroscience and acoustics), parabolic (with
applications to finance), and elliptic. We generically obtain improved
convergence rates relative to typical past implementations relying on delta
function approximations.
",0,0,0,0,1,1
332,A Systematic Approach for Exploring Tradeoffs in Predictive HVAC Control Systems for Buildings,"  Heating, Ventilation, and Cooling (HVAC) systems are often the most
significant contributor to the energy usage, and the operational cost, of large
office buildings. Therefore, to understand the various factors affecting the
energy usage, and to optimize the operational efficiency of building HVAC
systems, energy analysts and architects often create simulations (e.g.,
EnergyPlus or DOE-2), of buildings prior to construction or renovation to
determine energy savings and quantify the Return-on-Investment (ROI). While
useful, these simulations usually use static HVAC control strategies such as
lowering room temperature at night, or reactive control based on simulated room
occupancy. Recently, advances have been made in HVAC control algorithms that
predict room occupancy. However, these algorithms depend on costly sensor
installations and the tradeoffs between predictive accuracy, energy savings,
comfort and expenses are not well understood. Current simulation frameworks do
not support easy analysis of these tradeoffs. Our contribution is a simulation
framework that can be used to explore this design space by generating objective
estimates of the energy savings and occupant comfort for different levels of
HVAC prediction and control performance. We validate our framework on a
real-world occupancy dataset spanning 6 months for 235 rooms in a large
university office building. Using the gold standard of energy use modeling and
simulation (Revit and Energy Plus), we compare the energy consumption and
occupant comfort in 29 independent simulations that explore our parameter
space. Our results highlight a number of potentially useful tradeoffs with
respect to energy savings, comfort, and algorithmic performance among
predictive, reactive, and static schedules, for a stakeholder of our building.
",1,0,0,0,0,0
295,Exponential Stability Analysis via Integral Quadratic Constraints,"  The theory of integral quadratic constraints (IQCs) allows verification of
stability and gain-bound properties of systems containing nonlinear or
uncertain elements. Gain bounds often imply exponential stability, but it can
be challenging to compute useful numerical bounds on the exponential decay
rate. This work presents a generalization of the classical IQC results of
Megretski and Rantzer that leads to a tractable computational procedure for
finding exponential rate certificates that are far less conservative than ones
computed from $L_2$ gain bounds alone. An expanded library of IQCs for
certifying exponential stability is also provided and the effectiveness of the
technique is demonstrated via numerical examples.
",1,0,1,0,0,0
347,Combining learned and analytical models for predicting action effects,"  One of the most basic skills a robot should possess is predicting the effect
of physical interactions with objects in the environment. This enables optimal
action selection to reach a certain goal state. Traditionally, dynamics are
approximated by physics-based analytical models. These models rely on specific
state representations that may be hard to obtain from raw sensory data,
especially if no knowledge of the object shape is assumed. More recently, we
have seen learning approaches that can predict the effect of complex physical
interactions directly from sensory input. It is however an open question how
far these models generalize beyond their training data. In this work, we
investigate the advantages and limitations of neural network based learning
approaches for predicting the effects of actions based on sensory input and
show how analytical and learned models can be combined to leverage the best of
both worlds. As physical interaction task, we use planar pushing, for which
there exists a well-known analytical model and a large real-world dataset. We
propose to use a convolutional neural network to convert raw depth images or
organized point clouds into a suitable representation for the analytical model
and compare this approach to using neural networks for both, perception and
prediction. A systematic evaluation of the proposed approach on a very large
real-world dataset shows two main advantages of the hybrid architecture.
Compared to a pure neural network, it significantly (i) reduces required
training data and (ii) improves generalization to novel physical interaction.
",1,0,0,0,0,0
839,"Single-trial P300 Classification using PCA with LDA, QDA and Neural Networks","  The P300 event-related potential (ERP), evoked in scalp-recorded
electroencephalography (EEG) by external stimuli, has proven to be a reliable
response for controlling a BCI. The P300 component of an event related
potential is thus widely used in brain-computer interfaces to translate the
subjects' intent by mere thoughts into commands to control artificial devices.
The main challenge in the classification of P300 trials in
electroencephalographic (EEG) data is the low signal-to-noise ratio (SNR) of
the P300 response. To overcome the low SNR of individual trials, it is common
practice to average together many consecutive trials, which effectively
diminishes the random noise. Unfortunately, when more repeated trials are
required for applications such as the P300 speller, the communication rate is
greatly reduced. This has resulted in a need for better methods to improve
single-trial classification accuracy of P300 response. In this work, we use
Principal Component Analysis (PCA) as a preprocessing method and use Linear
Discriminant Analysis (LDA)and neural networks for classification. The results
show that a combination of PCA with these methods provided as high as 13\%
accuracy gain for single-trial classification while using only 3 to 4 principal
components.
",1,0,0,1,0,0
4865,Simple property of heterogeneous aspiration dynamics: Beyond weak selection,"  How individuals adapt their behavior in cultural evolution remains elusive.
Theoretical studies have shown that the update rules chosen to model individual
decision making can dramatically modify the evolutionary outcome of the
population as a whole. This hints at the complexities of considering the
personality of individuals in a population, where each one uses its own rule.
Here, we investigate whether and how heterogeneity in the rules of behavior
update alters the evolutionary outcome. We assume that individuals update
behaviors by aspiration-based self-evaluation and they do so in their own ways.
Under weak selection, we analytically reveal a simple property that holds for
any two-strategy multi-player games in well-mixed populations and on regular
graphs: the evolutionary outcome in a population with heterogeneous update
rules is the weighted average of the outcomes in the corresponding homogeneous
populations, and the associated weights are the frequencies of each update rule
in the heterogeneous population. Beyond weak selection, we show that this
property holds for public goods games. Our finding implies that heterogeneous
aspiration dynamics is additive. This additivity greatly reduces the complexity
induced by the underlying individual heterogeneity. Our work thus provides an
efficient method to calculate evolutionary outcomes under heterogeneous update
rules.
",0,0,0,0,1,0
383,Eigenvalues of symmetric tridiagonal interval matrices revisited,"  In this short note, we present a novel method for computing exact lower and
upper bounds of eigenvalues of a symmetric tridiagonal interval matrix.
Compared to the known methods, our approach is fast, simple to present and to
implement, and avoids any assumptions. Our construction explicitly yields those
matrices for which particular lower and upper bounds are attained.
",1,0,0,0,0,0
661,Partial Information Stochastic Differential Games for Backward Stochastic Systems Driven By Lévy Processes,"  In this paper, we consider a partial information two-person zero-sum
stochastic differential game problem where the system is governed by a backward
stochastic differential equation driven by Teugels martingales associated with
a Lévy process and an independent Brownian motion. One sufficient (a
verification theorem) and one necessary conditions for the existence of optimal
controls are proved. To illustrate the general results, a linear quadratic
stochastic differential game problem is discussed.
",0,0,1,0,0,0
9327,Asset Price Bubbles: An Option-based Indicator,"  We construct a statistical indicator for the detection of short-term asset
price bubbles based on the information content of bid and ask market quotes for
plain vanilla put and call options. Our construction makes use of the
martingale theory of asset price bubbles and the fact that such scenarios where
the price for an asset exceeds its fundamental value can in principle be
detected by analysis of the asymptotic behavior of the implied volatility
surface. For extrapolating this implied volatility, we choose the SABR model,
mainly because of its decent fit to real option market quotes for a broad range
of maturities and its ease of calibration. As main theoretical result, we show
that under lognormal SABR dynamics, we can compute a simple yet powerful
closed-form martingale defect indicator by solving an ill-posed inverse
calibration problem. In order to cope with the ill-posedness and to quantify
the uncertainty which is inherent to such an indicator, we adopt a Bayesian
statistical parameter estimation perspective. We probe the resulting posterior
densities with a combination of optimization and adaptive Markov chain Monte
Carlo methods, thus providing a full-blown uncertainty estimation of all the
underlying parameters and the martingale defect indicator. Finally, we provide
real-market tests of the proposed option-based indicator with focus on tech
stocks due to increasing concerns about a tech bubble 2.0.
",0,0,0,0,0,1
360,Deep Learning Scooping Motion using Bilateral Teleoperations,"  We present bilateral teleoperation system for task learning and robot motion
generation. Our system includes a bilateral teleoperation platform and a deep
learning software. The deep learning software refers to human demonstration
using the bilateral teleoperation platform to collect visual images and robotic
encoder values. It leverages the datasets of images and robotic encoder
information to learn about the inter-modal correspondence between visual images
and robot motion. In detail, the deep learning software uses a combination of
Deep Convolutional Auto-Encoders (DCAE) over image regions, and Recurrent
Neural Network with Long Short-Term Memory units (LSTM-RNN) over robot motor
angles, to learn motion taught be human teleoperation. The learnt models are
used to predict new motion trajectories for similar tasks. Experimental results
show that our system has the adaptivity to generate motion for similar scooping
tasks. Detailed analysis is performed based on failure cases of the
experimental results. Some insights about the cans and cannots of the system
are summarized.
",1,0,0,0,0,0
8183,"Growth, Industrial Externality, Prospect Dynamics and Well-being on Markets","  Functions or 'functionnings' enable to give a structure to any economic
activity whether they are used to describe a good or a service that is
exchanged on a market or they constitute the capability of an agent to provide
the labor market with specific work and skills. That structure encompasses the
basic law of supply and demand and the conditions of growth within a
transaction and of the inflation control. Functional requirements can be
followed from the design of a product to the delivery of a solution to a
customer needs with different levels of externalities while value is created
integrating organizational and technical constraints whereas a budget is
allocated to the various entities of the firm involved in the production.
Entering the market through that structure leads to designing basic equations
of its dynamics and to finding canonical solutions out of particular
equilibria. This approach enables to tackle behavioral foundations of Prospect
Theory within a generalization of its probability weighting function turned
into an operator which applies to Western, Educated, Industrialized, Rich, and
Democratic societies as well as to the poorest ones. The nature of reality and
well-being appears then as closely related to the relative satisfaction reached
on the market, as it can be conceived by an agent, according to business
cycles. This reality being the result of the complementary systems that govern
human mind as structured by rational psychologists.
",0,0,0,0,0,1
767,Deep Convolutional Networks as shallow Gaussian Processes,"  We show that the output of a (residual) convolutional neural network (CNN)
with an appropriate prior over the weights and biases is a Gaussian process
(GP) in the limit of infinitely many convolutional filters, extending similar
results for dense networks. For a CNN, the equivalent kernel can be computed
exactly and, unlike ""deep kernels"", has very few parameters: only the
hyperparameters of the original CNN. Further, we show that this kernel has two
properties that allow it to be computed efficiently; the cost of evaluating the
kernel for a pair of images is similar to a single forward pass through the
original CNN with only one filter per layer. The kernel equivalent to a
32-layer ResNet obtains 0.84% classification error on MNIST, a new record for
GPs with a comparable number of parameters.
",0,0,0,1,0,0
328,Graph Clustering using Effective Resistance,"  $ \def\vecc#1{\boldsymbol{#1}} $We design a polynomial time algorithm that
for any weighted undirected graph $G = (V, E,\vecc w)$ and sufficiently large
$\delta > 1$, partitions $V$ into subsets $V_1, \ldots, V_h$ for some $h\geq
1$, such that
$\bullet$ at most $\delta^{-1}$ fraction of the weights are between clusters,
i.e. \[ w(E - \cup_{i = 1}^h E(V_i)) \lesssim \frac{w(E)}{\delta};\]
$\bullet$ the effective resistance diameter of each of the induced subgraphs
$G[V_i]$ is at most $\delta^3$ times the average weighted degree, i.e. \[
\max_{u, v \in V_i} \mathsf{Reff}_{G[V_i]}(u, v) \lesssim \delta^3 \cdot
\frac{|V|}{w(E)} \quad \text{ for all } i=1, \ldots, h.\]
In particular, it is possible to remove one percent of weight of edges of any
given graph such that each of the resulting connected components has effective
resistance diameter at most the inverse of the average weighted degree.
Our proof is based on a new connection between effective resistance and low
conductance sets. We show that if the effective resistance between two vertices
$u$ and $v$ is large, then there must be a low conductance cut separating $u$
from $v$. This implies that very mildly expanding graphs have constant
effective resistance diameter. We believe that this connection could be of
independent interest in algorithm design.
",1,0,0,0,0,0
5473,Complexity of human response delay in intermittent control: The case of virtual stick balancing,"  Response delay is an inherent and essential part of human actions. In the
context of human balance control, the response delay is traditionally modeled
using the formalism of delay-differential equations, which adopts the
approximation of fixed delay. However, experimental studies revealing
substantial variability, adaptive anticipation, and non-stationary dynamics of
response delay provide evidence against this approximation. In this paper, we
call for development of principally new mathematical formalism describing human
response delay. To support this, we present the experimental data from a simple
virtual stick balancing task. Our results demonstrate that human response delay
is a widely distributed random variable with complex properties, which can
exhibit oscillatory and adaptive dynamics characterized by long-range
correlations. Given this, we argue that the fixed-delay approximation ignores
essential properties of human response, and conclude with possible directions
for future developments of new mathematical notions describing human control.
",0,0,0,0,1,0
1240,Generalizing Geometric Brownian Motion,"  To convert standard Brownian motion $Z$ into a positive process, Geometric
Brownian motion (GBM) $e^{\beta Z_t}, \beta >0$ is widely used. We generalize
this positive process by introducing an asymmetry parameter $ \alpha \geq 0$
which describes the instantaneous volatility whenever the process reaches a new
low. For our new process, $\beta$ is the instantaneous volatility as prices
become arbitrarily high. Our generalization preserves the positivity, constant
proportional drift, and tractability of GBM, while expressing the instantaneous
volatility as a randomly weighted $L^2$ mean of $\alpha$ and $\beta$. The
running minimum and relative drawup of this process are also analytically
tractable. Letting $\alpha = \beta$, our positive process reduces to Geometric
Brownian motion. By adding a jump to default to the new process, we introduce a
non-negative martingale with the same tractabilities. Assuming a security's
dynamics are driven by these processes in risk neutral measure, we price
several derivatives including vanilla, barrier and lookback options.
",0,0,0,0,0,1
1872,Transition probability of Brownian motion in the octant and its application to default modeling,"  We derive a semi-analytic formula for the transition probability of
three-dimensional Brownian motion in the positive octant with absorption at the
boundaries. Separation of variables in spherical coordinates leads to an
eigenvalue problem for the resulting boundary value problem in the two angular
components. The main theoretical result is a solution to the original problem
expressed as an expansion into special functions and an eigenvalue which has to
be chosen to allow a matching of the boundary condition. We discuss and test
several computational methods to solve a finite-dimensional approximation to
this nonlinear eigenvalue problem. Finally, we apply our results to the
computation of default probabilities and credit valuation adjustments in a
structural credit model with mutual liabilities.
",0,0,0,0,0,1
906,High-temperature terahertz optical diode effect without magnetic order in polar FeZnMo$_3$O$_8$,"  We present a terahertz spectroscopic study of polar ferrimagnet
FeZnMo$_3$O$_8$. Our main finding is a giant high-temperature optical diode
effect, or nonreciprocal directional dichroism, where the transmitted light
intensity in one direction is over 100 times lower than intensity transmitted
in the opposite direction. The effect takes place in the paramagnetic phase
with no long-range magnetic order in the crystal, which contrasts sharply with
all existing reports of the terahertz optical diode effect in other
magnetoelectric materials, where the long-range magnetic ordering is a
necessary prerequisite. In \fzmo, the effect occurs resonantly with a strong
magnetic dipole active transition centered at 1.27 THz and assigned as electron
spin resonance between the eigenstates of the single-ion anisotropy
Hamiltonian. We propose that the optical diode effect in paramagnetic
FeZnMo$_3$O$_8$ is driven by signle-ion terms in magnetoelectric free energy.
",0,1,0,0,0,0
3019,Susceptibility of Methicillin Resistant Staphylococcus aureus to Vancomycin using Liposomal Drug Delivery System,"  Staphylococcus aureus responsible for nosocomial infections is a significant
threat to the public health. The increasing resistance of S.aureus to various
antibiotics has drawn it to a prime focus for research on designing an
appropriate drug delivery system. Emergence of Methicillin Resistant
Staphylococcus aureus (MRSA) in 1961, necessitated the use of vancomycin ""the
drug of last resort"" to treat these infections. Unfortunately, S.aureus has
already started gaining resistances to vancomycin. Liposome encapsulation of
drugs have been earlier shown to provide an efficient method of microbial
inhibition in many cases. We have studied the effect of liposome encapsulated
vancomycin on MRSA and evaluated the antibacterial activity of the
liposome-entrapped drug in comparison to that of the free drug based on the
minimum inhibitory concentration (MIC) of the drug. The MIC for liposomal
vancomycin was found to be about half of that of free vancomycin. The growth
response of MRSA showed that the liposomal vancomycin induced the culture to go
into bacteriostatic state and phagocytic killing was enhanced. Administration
of the antibiotic encapsulated in liposome thus was shown to greatly improve
the drug delivery as well as the drug resistance caused by MRSA.
",0,0,0,0,1,0
509,A Multiple Source Framework for the Identification of Activities of Daily Living Based on Mobile Device Data,"  The monitoring of the lifestyles may be performed based on a system for the
recognition of Activities of Daily Living (ADL) and their environments,
combining the results obtained with the user agenda. The system may be
developed with the use of the off-the-shelf mobile devices commonly used,
because they have several types of sensors available, including motion,
magnetic, acoustic, and location sensors. Data acquisition, data processing,
data fusion, and artificial intelligence methods are applied in different
stages of the system developed, which recognizes the ADL with pattern
recognition methods. The motion and magnetic sensors allow the recognition of
activities with movement, but the acoustic sensors allow the recognition of the
environments. The fusion of the motion, magnetic and acoustic sensors allows
the differentiation of other ADL. On the other hand, the location sensors
allows the recognition of ADL with large movement, and the combination of these
sensors with the other sensors increases the number of ADL recognized by the
system. This study consists on the comparison of different types of ANN for
choosing the best methods for the recognition of several ADL, which they are
implemented in a system for the recognition of ADL that combines the sensors
data with the users agenda for the monitoring of the lifestyles. Conclusions
point to the use of Deep Neural Networks (DNN) with normalized data for the
identification of ADL with 85.89% of accuracy, the use of Feedforward neural
networks with non-normalized data for the identification of the environments
with 86.50% of accuracy, and the use of DNN with normalized data for the
identification of standing activities with 100% of accuracy, proving the
reliability of the framework presented in this study.
",1,0,0,0,0,0
235,Streaming Algorithm for Euler Characteristic Curves of Multidimensional Images,"  We present an efficient algorithm to compute Euler characteristic curves of
gray scale images of arbitrary dimension. In various applications the Euler
characteristic curve is used as a descriptor of an image.
Our algorithm is the first streaming algorithm for Euler characteristic
curves. The usage of streaming removes the necessity to store the entire image
in RAM. Experiments show that our implementation handles terabyte scale images
on commodity hardware. Due to lock-free parallelism, it scales well with the
number of processor cores. Our software---CHUNKYEuler---is available as open
source on Bitbucket.
Additionally, we put the concept of the Euler characteristic curve in the
wider context of computational topology. In particular, we explain the
connection with persistence diagrams.
",1,0,1,0,0,0
567,Improving Foot-Mounted Inertial Navigation Through Real-Time Motion Classification,"  We present a method to improve the accuracy of a foot-mounted,
zero-velocity-aided inertial navigation system (INS) by varying estimator
parameters based on a real-time classification of motion type. We train a
support vector machine (SVM) classifier using inertial data recorded by a
single foot-mounted sensor to differentiate between six motion types (walking,
jogging, running, sprinting, crouch-walking, and ladder-climbing) and report
mean test classification accuracy of over 90% on a dataset with five different
subjects. From these motion types, we select two of the most common (walking
and running), and describe a method to compute optimal zero-velocity detection
parameters tailored to both a specific user and motion type by maximizing the
detector F-score. By combining the motion classifier with a set of optimal
detection parameters, we show how we can reduce INS position error during mixed
walking and running motion. We evaluate our adaptive system on a total of 5.9
km of indoor pedestrian navigation performed by five different subjects moving
along a 130 m path with surveyed ground truth markers.
",1,0,0,0,0,0
306,Assessing inter-modal and inter-regional dependencies in prodromal Alzheimer's disease using multimodal MRI/PET and Gaussian graphical models,"  A sequence of pathological changes takes place in Alzheimer's disease, which
can be assessed in vivo using various brain imaging methods. Currently, there
is no appropriate statistical model available that can easily integrate
multiple imaging modalities, being able to utilize the additional information
provided from the combined data. We applied Gaussian graphical models (GGMs)
for analyzing the conditional dependency networks of multimodal neuroimaging
data and assessed alterations of the network structure in mild cognitive
impairment (MCI) and Alzheimer's dementia (AD) compared to cognitively healthy
controls.
Data from N=667 subjects were obtained from the Alzheimer's Disease
Neuroimaging Initiative. Mean amyloid load (AV45-PET), glucose metabolism
(FDG-PET), and gray matter volume (MRI) was calculated for each brain region.
Separate GGMs were estimated using a Bayesian framework for the combined
multimodal data for each diagnostic category. Graph-theoretical statistics were
calculated to determine network alterations associated with disease severity.
Network measures clustering coefficient, path length and small-world
coefficient were significantly altered across diagnostic groups, with a
biphasic u-shape trajectory, i.e. increased small-world coefficient in early
MCI, intermediate values in late MCI, and decreased values in AD patients
compared to controls. In contrast, no group differences were found for
clustering coefficient and small-world coefficient when estimating conditional
dependency networks on single imaging modalities.
GGMs provide a useful methodology to analyze the conditional dependency
networks of multimodal neuroimaging data.
",0,0,0,1,1,0
1627,Dynamical regularities of US equities opening and closing auctions,"  We first investigate the evolution of opening and closing auctions volumes of
US equities along the years. We then report dynamical properties of pre-auction
periods: the indicative match price is strongly mean-reverting because the
imbalance is; the final auction price reacts to a single auction order
placement or cancellation in markedly different ways in the opening and closing
auctions when computed conditionally on imbalance improving or worsening
events; the indicative price reverts towards the mid price of the regular limit
order book but is not especially bound to the spread.
",0,0,0,0,0,1
2378,Global stability of a network-based SIRS epidemic model with nonmonotone incidence rate,"  This paper studies the dynamics of a network-based SIRS epidemic model with
vaccination and a nonmonotone incidence rate. This type of nonlinear incidence
can be used to describe the psychological or inhibitory effect from the
behavioral change of the susceptible individuals when the number of infective
individuals on heterogeneous networks is getting larger. Using the analytical
method, epidemic threshold $R_0$ is obtained. When $R_0$ is less than one, we
prove the disease-free equilibrium is globally asymptotically stable and the
disease dies out, while $R_0$ is greater than one, there exists a unique
endemic equilibrium. By constructing a suitable Lyapunov function, we also
prove the endemic equilibrium is globally asymptotically stable if the
inhibitory factor $\alpha$ is sufficiently large. Numerical experiments are
also given to support the theoretical results. It is shown both theoretically
and numerically a larger $\alpha$ can accelerate the extinction of the disease
and reduce the level of disease.
",0,0,0,0,1,0
256,Deformable Generator Network: Unsupervised Disentanglement of Appearance and Geometry,"  We propose a deformable generator model to disentangle the appearance and
geometric information from images into two independent latent vectors. The
appearance generator produces the appearance information, including color,
illumination, identity or category, of an image. The geometric generator
produces displacement of the coordinates of each pixel and performs geometric
warping, such as stretching and rotation, on the appearance generator to obtain
the final synthesized image. The proposed model can learn both representations
from image data in an unsupervised manner. The learned geometric generator can
be conveniently transferred to the other image datasets to facilitate
downstream AI tasks.
",0,0,0,1,0,0
563,Continuum of quantum fluctuations in a three-dimensional $S\!=\!1$ Heisenberg magnet,"  Conventional crystalline magnets are characterized by symmetry breaking and
normal modes of excitation called magnons with quantized angular momentum
$\hbar$. Neutron scattering correspondingly features extra magnetic Bragg
diffraction at low temperatures and dispersive inelastic scattering associated
with single magnon creation and annihilation. Exceptions are anticipated in
so-called quantum spin liquids as exemplified by the one-dimensional spin-1/2
chain which has no magnetic order and where magnons accordingly fractionalize
into spinons with angular momentum $\hbar/2$. This is spectacularly revealed by
a continuum of inelastic neutron scattering associated with two-spinon
processes and the absence of magnetic Bragg diffraction. Here, we report
evidence for these same key features of a quantum spin liquid in the
three-dimensional Heisenberg antiferromagnet NaCaNi$_2$F$_7$. Through specific
heat and neutron scattering measurements, Monte Carlo simulations, and analytic
approximations to the equal time correlations, we show that NaCaNi$_2$F$_7$ is
an almost ideal realization of the spin-1 antiferromagnetic Heisenberg model on
a pyrochlore lattice with weak connectivity and frustrated interactions.
Magnetic Bragg diffraction is absent and 90\% of the spectral weight forms a
continuum of magnetic scattering not dissimilar to that of the spin-1/2 chain
but with low energy pinch points indicating NaCaNi$_2$F$_7$ is in a Coulomb
phase. The residual entropy and diffuse elastic scattering points to an exotic
state of matter driven by frustration, quantum fluctuations and weak exchange
disorder.
",0,1,0,0,0,0
442,Forecasting in the light of Big Data,"  Predicting the future state of a system has always been a natural motivation
for science and practical applications. Such a topic, beyond its obvious
technical and societal relevance, is also interesting from a conceptual point
of view. This owes to the fact that forecasting lends itself to two equally
radical, yet opposite methodologies. A reductionist one, based on the first
principles, and the naive inductivist one, based only on data. This latter view
has recently gained some attention in response to the availability of
unprecedented amounts of data and increasingly sophisticated algorithmic
analytic techniques. The purpose of this note is to assess critically the role
of big data in reshaping the key aspects of forecasting and in particular the
claim that bigger data leads to better predictions. Drawing on the
representative example of weather forecasts we argue that this is not generally
the case. We conclude by suggesting that a clever and context-dependent
compromise between modelling and quantitative analysis stands out as the best
forecasting strategy, as anticipated nearly a century ago by Richardson and von
Neumann.
",0,1,0,0,0,0
680,Perception-in-the-Loop Adversarial Examples,"  We present a scalable, black box, perception-in-the-loop technique to find
adversarial examples for deep neural network classifiers. Black box means that
our procedure only has input-output access to the classifier, and not to the
internal structure, parameters, or intermediate confidence values.
Perception-in-the-loop means that the notion of proximity between inputs can be
directly queried from human participants rather than an arbitrarily chosen
metric. Our technique is based on covariance matrix adaptation evolution
strategy (CMA-ES), a black box optimization approach. CMA-ES explores the
search space iteratively in a black box manner, by generating populations of
candidates according to a distribution, choosing the best candidates according
to a cost function, and updating the posterior distribution to favor the best
candidates. We run CMA-ES using human participants to provide the fitness
function, using the insight that the choice of best candidates in CMA-ES can be
naturally modeled as a perception task: pick the top $k$ inputs perceptually
closest to a fixed input. We empirically demonstrate that finding adversarial
examples is feasible using small populations and few iterations. We compare the
performance of CMA-ES on the MNIST benchmark with other black-box approaches
using $L_p$ norms as a cost function, and show that it performs favorably both
in terms of success in finding adversarial examples and in minimizing the
distance between the original and the adversarial input. In experiments on the
MNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find
perceptually similar adversarial inputs with a small number of iterations and
small population sizes when using perception-in-the-loop. Finally, we show that
networks trained specifically to be robust against $L_\infty$ norm can still be
susceptible to perceptually similar adversarial examples.
",1,0,0,1,0,0
439,It Takes Two to Tango: Towards Theory of AI's Mind,"  Theory of Mind is the ability to attribute mental states (beliefs, intents,
knowledge, perspectives, etc.) to others and recognize that these mental states
may differ from one's own. Theory of Mind is critical to effective
communication and to teams demonstrating higher collective performance. To
effectively leverage the progress in Artificial Intelligence (AI) to make our
lives more productive, it is important for humans and AI to work well together
in a team. Traditionally, there has been much emphasis on research to make AI
more accurate, and (to a lesser extent) on having it better understand human
intentions, tendencies, beliefs, and contexts. The latter involves making AI
more human-like and having it develop a theory of our minds. In this work, we
argue that for human-AI teams to be effective, humans must also develop a
theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,
and quirks. We instantiate these ideas within the domain of Visual Question
Answering (VQA). We find that using just a few examples (50), lay people can be
trained to better predict responses and oncoming failures of a complex VQA
model. We further evaluate the role existing explanation (or interpretability)
modalities play in helping humans build ToAIM. Explainable AI has received
considerable scientific and popular attention in recent times. Surprisingly, we
find that having access to the model's internal states - its confidence in its
top-k predictions, explicit or implicit attention maps which highlight regions
in the image (and words in the question) the model is looking at (and listening
to) while answering a question about an image - do not help people better
predict its behavior.
",1,0,0,0,0,0
511,Hipsters on Networks: How a Small Group of Individuals Can Lead to an Anti-Establishment Majority,"  The spread of opinions, memes, diseases, and ""alternative facts"" in a
population depends both on the details of the spreading process and on the
structure of the social and communication networks on which they spread. In
this paper, we explore how \textit{anti-establishment} nodes (e.g.,
\textit{hipsters}) influence the spreading dynamics of two competing products.
We consider a model in which spreading follows a deterministic rule for
updating node states (which describe which product has been adopted) in which
an adjustable fraction $p_{\rm Hip}$ of the nodes in a network are hipsters,
who choose to adopt the product that they believe is the less popular of the
two. The remaining nodes are conformists, who choose which product to adopt by
considering which products their immediate neighbors have adopted. We simulate
our model on both synthetic and real networks, and we show that the hipsters
have a major effect on the final fraction of people who adopt each product:
even when only one of the two products exists at the beginning of the
simulations, a very small fraction of hipsters in a network can still cause the
other product to eventually become the more popular one. To account for this
behavior, we construct an approximation for the steady-state adoption fraction
on $k$-regular trees in the limit of few hipsters. Additionally, our
simulations demonstrate that a time delay $\tau$ in the knowledge of the
product distribution in a population, as compared to immediate knowledge of
product adoption among nearest neighbors, can have a large effect on the final
distribution of product adoptions. Our simple model and analysis may help shed
light on the road to success for anti-establishment choices in elections, as
such success can arise rather generically in our model from a small number of
anti-establishment individuals and ordinary processes of social influence on
normal individuals.
",1,1,0,0,0,0
325,Long-Term Load Forecasting Considering Volatility Using Multiplicative Error Model,"  Long-term load forecasting plays a vital role for utilities and planners in
terms of grid development and expansion planning. An overestimate of long-term
electricity load will result in substantial wasted investment in the
construction of excess power facilities, while an underestimate of future load
will result in insufficient generation and unmet demand. This paper presents
first-of-its-kind approach to use multiplicative error model (MEM) in
forecasting load for long-term horizon. MEM originates from the structure of
autoregressive conditional heteroscedasticity (ARCH) model where conditional
variance is dynamically parameterized and it multiplicatively interacts with an
innovation term of time-series. Historical load data, accessed from a U.S.
regional transmission operator, and recession data for years 1993-2016 is used
in this study. The superiority of considering volatility is proven by
out-of-sample forecast results as well as directional accuracy during the great
economic recession of 2008. To incorporate future volatility, backtesting of
MEM model is performed. Two performance indicators used to assess the proposed
model are mean absolute percentage error (for both in-sample model fit and
out-of-sample forecasts) and directional accuracy.
",0,0,0,1,0,0
289,Trajectories and orbital angular momentum of necklace beams in nonlinear colloidal suspensions,"  Recently, we have predicted that the modulation instability of optical vortex
solitons propagating in nonlinear colloidal suspensions with exponential
saturable nonlinearity leads to formation of necklace beams (NBs)
[S.~Z.~Silahli, W.~Walasik and N.~M.~Litchinitser, Opt.~Lett., \textbf{40},
5714 (2015)]. Here, we investigate the dynamics of NB formation and
propagation, and show that the distance at which the NB is formed depends on
the input power of the vortex beam. Moreover, we show that the NB trajectories
are not necessarily tangent to the initial vortex ring, and that their
velocities have components stemming both from the beam diffraction and from the
beam orbital angular momentum. We also demonstrate the generation of twisted
solitons and analyze the influence of losses on their propagation. Finally, we
investigate the conservation of the orbital angular momentum in necklace and
twisted beams. Our studies, performed in ideal lossless media and in realistic
colloidal suspensions with losses, provide a detailed description of NB
dynamics and may be useful in studies of light propagation in highly scattering
colloids and biological samples.
",0,1,0,0,0,0
3395,Characterization of catastrophic instabilities: Market crashes as paradigm,"  Catastrophic events, though rare, do occur and when they occur, they have
devastating effects. It is, therefore, of utmost importance to understand the
complexity of the underlying dynamics and signatures of catastrophic events,
such as market crashes. For deeper understanding, we choose the US and Japanese
markets from 1985 onward, and study the evolution of the cross-correlation
structures of stock return matrices and their eigenspectra over different short
time-intervals or ""epochs"". A slight non-linear distortion is applied to the
correlation matrix computed for any epoch, leading to the emerging spectrum of
eigenvalues. The statistical properties of the emerging spectrum display: (i)
the shape of the emerging spectrum reflects the market instability, (ii) the
smallest eigenvalue may be able to statistically distinguish the nature of a
market turbulence or crisis -- internal instability or external shock, and
(iii) the time-lagged smallest eigenvalue has a statistically significant
correlation with the mean market cross-correlation. The smallest eigenvalue
seems to indicate that the financial market has become more turbulent in a
similar way as the mean does. Yet we show features of the smallest eigenvalue
of the emerging spectrum that distinguish different types of market
instabilities related to internal or external causes. Based on the paradigmatic
character of financial time series for other complex systems, the capacity of
the emerging spectrum to understand the nature of instability may be a new
feature, which can be broadly applied.
",0,0,0,0,0,1
436,Real-time brain machine interaction via social robot gesture control,"  Brain-Machine Interaction (BMI) system motivates interesting and promising
results in forward/feedback control consistent with human intention. It holds
great promise for advancements in patient care and applications to
neurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic
platform using a personalized social robot in order to assist patients having
cognitive deficits through bilateral rehabilitation and mental training. For
initial testing of the platform, electroencephalography (EEG) brainwaves of a
human user were collected in real time during tasks of imaginary movements.
First, the brainwaves associated with imagined body kinematics parameters were
decoded to control a cursor on a computer screen in training protocol. Then,
the experienced subject was able to interact with a social robot via our
real-time BMI robotic platform. Corresponding to subject's imagery performance,
he/she received specific gesture movements and eye color changes as
neural-based feedback from the robot. This hands-free neurofeedback interaction
not only can be used for mind control of a social robot's movements, but also
sets the stage for application to enhancing and recovering mental abilities
such as attention via training in humans by providing real-time neurofeedback
from a social robot.
",1,0,0,0,0,0
516,Optimal Envelope Approximation in Fourier Basis with Applications in TV White Space,"  Lowpass envelope approximation of smooth continuous-variable signals are
introduced in this work. Envelope approximations are necessary when a given
signal has to be approximated always to a larger value (such as in TV white
space protection regions). In this work, a near-optimal approximate algorithm
for finding a signal's envelope, while minimizing a mean-squared cost function,
is detailed. The sparse (lowpass) signal approximation is obtained in the
linear Fourier series basis. This approximate algorithm works by discretizing
the envelope property from an infinite number of points to a large (but finite)
number of points. It is shown that this approximate algorithm is near-optimal
and can be solved by using efficient convex optimization programs available in
the literature. Simulation results are provided towards the end to gain more
insights into the analytical results presented.
",1,0,0,0,0,0
8206,Incremental Sharpe and other performance ratios,"  We present a new methodology of computing incremental contribution for
performance ratios for portfolio like Sharpe, Treynor, Calmar or Sterling
ratios. Using Euler's homogeneous function theorem, we are able to decompose
these performance ratios as a linear combination of individual modified
performance ratios. This allows understanding the drivers of these performance
ratios as well as deriving a condition for a new asset to provide incremental
performance for the portfolio. We provide various numerical examples of this
performance ratio decomposition.
",0,0,0,0,0,1
7375,Investigating the configurations in cross-shareholding: a joint copula-entropy approach,"  --- the companies populating a Stock market, along with their connections,
can be effectively modeled through a directed network, where the nodes
represent the companies, and the links indicate the ownership. This paper deals
with this theme and discusses the concentration of a market. A
cross-shareholding matrix is considered, along with two key factors: the node
out-degree distribution which represents the diversification of investments in
terms of the number of involved companies, and the node in-degree distribution
which reports the integration of a company due to the sales of its own shares
to other companies. While diversification is widely explored in the literature,
integration is most present in literature on contagions. This paper captures
such quantities of interest in the two frameworks and studies the stochastic
dependence of diversification and integration through a copula approach. We
adopt entropies as measures for assessing the concentration in the market. The
main question is to assess the dependence structure leading to a better
description of the data or to market polarization (minimal entropy) or market
fairness (maximal entropy). In so doing, we derive information on the way in
which the in- and out-degrees should be connected in order to shape the market.
The question is of interest to regulators bodies, as witnessed by specific
alert threshold published on the US mergers guidelines for limiting the
possibility of acquisitions and the prevalence of a single company on the
market. Indeed, all countries and the EU have also rules or guidelines in order
to limit concentrations, in a country or across borders, respectively. The
calibration of copulas and model parameters on the basis of real data serves as
an illustrative application of the theoretical proposal.
",0,0,0,0,0,1
834,Curse of Heterogeneity: Computational Barriers in Sparse Mixture Models and Phase Retrieval,"  We study the fundamental tradeoffs between statistical accuracy and
computational tractability in the analysis of high dimensional heterogeneous
data. As examples, we study sparse Gaussian mixture model, mixture of sparse
linear regressions, and sparse phase retrieval model. For these models, we
exploit an oracle-based computational model to establish conjecture-free
computationally feasible minimax lower bounds, which quantify the minimum
signal strength required for the existence of any algorithm that is both
computationally tractable and statistically accurate. Our analysis shows that
there exist significant gaps between computationally feasible minimax risks and
classical ones. These gaps quantify the statistical price we must pay to
achieve computational tractability in the presence of data heterogeneity. Our
results cover the problems of detection, estimation, support recovery, and
clustering, and moreover, resolve several conjectures of Azizyan et al. (2013,
2015); Verzelen and Arias-Castro (2017); Cai et al. (2016). Interestingly, our
results reveal a new but counter-intuitive phenomenon in heterogeneous data
analysis that more data might lead to less computation complexity.
",0,0,0,1,0,0
365,On orbifold constructions associated with the Leech lattice vertex operator algebra,"  In this article, we study orbifold constructions associated with the Leech
lattice vertex operator algebra. As an application, we prove that the structure
of a strongly regular holomorphic vertex operator algebra of central charge
$24$ is uniquely determined by its weight one Lie algebra if the Lie algebra
has the type $A_{3,4}^3A_{1,2}$, $A_{4,5}^2$, $D_{4,12}A_{2,6}$, $A_{6,7}$,
$A_{7,4}A_{1,1}^3$, $D_{5,8}A_{1,2}$ or $D_{6,5}A_{1,1}^2$ by using the reverse
orbifold construction. Our result also provides alternative constructions of
these vertex operator algebras (except for the case $A_{6,7}$) from the Leech
lattice vertex operator algebra.
",0,0,1,0,0,0
326,Geometrically stopped Markovian random growth processes and Pareto tails,"  Many empirical studies document power law behavior in size distributions of
economic interest such as cities, firms, income, and wealth. One mechanism for
generating such behavior combines independent and identically distributed
Gaussian additive shocks to log-size with a geometric age distribution. We
generalize this mechanism by allowing the shocks to be non-Gaussian (but
light-tailed) and dependent upon a Markov state variable. Our main results
provide sharp bounds on tail probabilities and simple formulas for Pareto
exponents. We present two applications: (i) we show that the tails of the
wealth distribution in a heterogeneous-agent dynamic general equilibrium model
with idiosyncratic endowment risk decay exponentially, unlike models with
investment risk where the tails may be Paretian, and (ii) we show that a random
growth model for the population dynamics of Japanese prefectures is consistent
with the observed Pareto exponent but only after allowing for Markovian
dynamics.
",0,0,1,0,0,0
296,Fermi acceleration of electrons inside foreshock transient cores,"  Foreshock transients upstream of Earth's bow shock have been recently
observed to accelerate electrons to many times their thermal energy. How such
acceleration occurs is unknown, however. Using THEMIS case studies, we examine
a subset of acceleration events (31 of 247 events) in foreshock transients with
cores that exhibit gradual electron energy increases accompanied by low
background magnetic field strength and large-amplitude magnetic fluctuations.
Using the evolution of electron distributions and the energy increase rates at
multiple spacecraft, we suggest that Fermi acceleration between a converging
foreshock transient's compressional boundary and the bow shock is responsible
for the observed electron acceleration. We then show that a one-dimensional
test particle simulation of an ideal Fermi acceleration model in fluctuating
fields prescribed by the observations can reproduce the observed evolution of
electron distributions, energy increase rate, and pitch-angle isotropy,
providing further support for our hypothesis. Thus, Fermi acceleration is
likely the principal electron acceleration mechanism in at least this subset of
foreshock transient cores.
",0,1,0,0,0,0
630,Channel masking for multivariate time series shapelets,"  Time series shapelets are discriminative sub-sequences and their similarity
to time series can be used for time series classification. Initial shapelet
extraction algorithms searched shapelets by complete enumeration of all
possible data sub-sequences. Research on shapelets for univariate time series
proposed a mechanism called shapelet learning which parameterizes the shapelets
and learns them jointly with a prediction model in an optimization procedure.
Trivial extension of this method to multivariate time series does not yield
very good results due to the presence of noisy channels which lead to
overfitting. In this paper we propose a shapelet learning scheme for
multivariate time series in which we introduce channel masks to discount noisy
channels and serve as an implicit regularization.
",1,0,0,0,0,0
530,Searching for the Transit of the Earth--mass exoplanet Proxima~Centauri~b in Antarctica: Preliminary Result,"  Proxima Centauri is known as the closest star from the Sun. Recently, radial
velocity observations revealed the existence of an Earth-mass planet around it.
With an orbital period of ~11 days, the surface of Proxima Centauri b is
temperate and might be habitable. We took a photometric monitoring campaign to
search for its transit, using the Bright Star Survey Telescope at the Zhongshan
Station in Antarctica. A transit-like signal appearing on 2016 September 8th,
is identified tentatively. Its midtime, $T_{C}=2,457,640.1990\pm0.0017$ HJD, is
consistent with the predicted ephemeris based on RV orbit in a 1$\sigma$
confidence interval. Time-correlated noise is pronounced in the light curve of
Proxima Centauri, affecting detection of transits. We develop a technique, in a
Gaussian process framework, to gauge the statistical significance of potential
transit detection. The tentative transit signal reported here, has a confidence
level of $2.5\sigma$. Further detection of its periodic signals is necessary to
confirm the planetary transit of Proxima Centauri b. We plan to monitor Proxima
Centauri in next Polar night at Dome A in Antarctica, taking the advantage of
continuous darkness. \citet{Kipping17} reported two tentative transit-like
signals of Proxima Centauri b, observed by the Microvariability and Oscillation
of Stars space Telescope in 2014 and 2015, respectively. The midtransit time of
our detection is 138 minutes later than that predicted by their transit
ephemeris. If all the signals are real transits, the misalignment of the epochs
plausibly suggests transit timing variations of Proxima Centauri b induced by
an outer planet in this system.
",0,1,0,0,0,0
14251,A novel improved fuzzy support vector machine based stock price trend forecast model,"  Application of fuzzy support vector machine in stock price forecast. Support
vector machine is a new type of machine learning method proposed in 1990s. It
can deal with classification and regression problems very successfully. Due to
the excellent learning performance of support vector machine, the technology
has become a hot research topic in the field of machine learning, and it has
been successfully applied in many fields. However, as a new technology, there
are many limitations to support vector machines. There is a large amount of
fuzzy information in the objective world. If the training of support vector
machine contains noise and fuzzy information, the performance of the support
vector machine will become very weak and powerless. As the complexity of many
factors influence the stock price prediction, the prediction results of
traditional support vector machine cannot meet people with precision, this
study improved the traditional support vector machine fuzzy prediction
algorithm is proposed to improve the new model precision. NASDAQ Stock Market,
Standard & Poor's (S&P) Stock market are considered. Novel advanced- fuzzy
support vector machine (NA-FSVM) is the proposed methodology.
",0,0,0,1,0,1
549, Robust and Imperceptible Adversarial Attacks on Capsule Networks,"  Capsule Networks envision an innovative point of view about the
representation of the objects in the brain and preserve the hierarchical
spatial relationships between them. This type of networks exhibits a huge
potential for several Machine Learning tasks like image classification, while
outperforming Convolutional Neural Networks (CNNs). A large body of work has
explored adversarial examples for CNNs, but their efficacy to Capsule Networks
is not well explored. In our work, we study the vulnerabilities in Capsule
Networks to adversarial attacks. These perturbations, added to the test inputs,
are small and imperceptible to humans, but fool the network to mis-predict. We
propose a greedy algorithm to automatically generate targeted imperceptible
adversarial examples in a black-box attack scenario. We show that this kind of
attacks, when applied to the German Traffic Sign Recognition Benchmark (GTSRB),
mislead Capsule Networks. Moreover, we apply the same kind of adversarial
attacks to a 9-layer CNN and analyze the outcome, compared to the Capsule
Networks to study their differences / commonalities.
",1,0,0,1,0,0
363,Preventing Hospital Acquired Infections Through a Workflow-Based Cyber-Physical System,"  Hospital acquired infections (HAI) are infections acquired within the
hospital from healthcare workers, patients or from the environment, but which
have no connection to the initial reason for the patient's hospital admission.
HAI are a serious world-wide problem, leading to an increase in mortality
rates, duration of hospitalisation as well as significant economic burden on
hospitals. Although clear preventive guidelines exist, studies show that
compliance to them is frequently poor. This paper details the software
perspective for an innovative, business process software based cyber-physical
system that will be implemented as part of a European Union-funded research
project. The system is composed of a network of sensors mounted in different
sites around the hospital, a series of wearables used by the healthcare workers
and a server side workflow engine. For better understanding, we describe the
system through the lens of a single, simple clinical workflow that is
responsible for a significant portion of all hospital infections. The goal is
that when completed, the system will be configurable in the sense of
facilitating the creation and automated monitoring of those clinical workflows
that when combined, account for over 90\% of hospital infections.
",1,0,0,0,0,0
4820,Eco-evolutionary feedbacks - theoretical models and perspectives,"  1. Theoretical models pertaining to feedbacks between ecological and
evolutionary processes are prevalent in multiple biological fields. An
integrative overview is currently lacking, due to little crosstalk between the
fields and the use of different methodological approaches.
2. Here we review a wide range of models of eco-evolutionary feedbacks and
highlight their underlying assumptions. We discuss models where feedbacks occur
both within and between hierarchical levels of ecosystems, including
populations, communities, and abiotic environments, and consider feedbacks
across spatial scales.
3. Identifying the commonalities among feedback models, and the underlying
assumptions, helps us better understand the mechanistic basis of
eco-evolutionary feedbacks. Eco-evolutionary feedbacks can be readily modelled
by coupling demographic and evolutionary formalisms. We provide an overview of
these approaches and suggest future integrative modelling avenues.
4. Our overview highlights that eco-evolutionary feedbacks have been
incorporated in theoretical work for nearly a century. Yet, this work does not
always include the notion of rapid evolution or concurrent ecological and
evolutionary time scales. We discuss the importance of density- and
frequency-dependent selection for feedbacks, as well as the importance of
dispersal as a central linking trait between ecology and evolution in a spatial
context.
",0,0,0,0,1,0
10772,Zero-Inflated Autoregressive Conditional Duration Model for Discrete Trade Durations with Excessive Zeros,"  In finance, durations between successive transactions are usually modeled by
the autoregressive conditional duration model based on a continuous
distribution omitting frequent zero values. Zero durations can be caused by
either split transactions or independent transactions. We propose a discrete
model allowing for excessive zero values based on the zero-inflated negative
binomial distribution with score dynamics. We establish the invertibility of
the score filter. Additionally, we derive sufficient conditions for the
consistency and asymptotic normality of the maximum likelihood of the model
parameters. In an empirical study of DJIA stocks, we find that split
transactions cause on average 63% of zero values. Furthermore, the loss of
decimal places in the proposed model is less severe than incorrect treatment of
zero values in continuous models.
",0,0,0,0,0,1
598,Unveiling ADP-binding sites and channels in respiratory complexes: Validation of Murburn concept as a holistic explanation for oxidative phosphorylation,"  Mitochondrial oxidative phosphorylation (mOxPhos) makes ATP, the energy
currency of life. Chemiosmosis, a proton centric mechanism, advocates that
Complex V harnesses a transmembrane potential (TMP) for ATP synthesis. This
perception of cellular respiration requires oxygen to stay tethered at Complex
IV (an association inhibited by cyanide) and diffusible reactive oxygen species
(DROS) are considered wasteful and toxic products. With new mechanistic
insights on heme and flavin enzymes, an oxygen or DROS centric explanation
(called murburn concept) was recently proposed for mOxPhos. In the new
mechanism, TMP is not directly harnessed, protons are a rate limiting reactant
and DROS within matrix serve as the chemical coupling agents that directly link
NADH oxidation with ATP synthesis. Herein, we report multiple ADP binding sites
and solvent accessible DROS channels in respiratory proteins, which validate
the oxygen or DROS centric power generation (ATP synthesis) system in mOxPhos.
Since cyanide's heme binding Kd is high (mM), low doses (uM) of cyanide is
lethal because cyanide disrupts DROS dynamics in mOxPhos. The critical study
also provides comprehensive arguments against Mitchell's and Boyer's
explanations and extensive support for murburn concept based holistic
perspectives for mOxPhos.
",0,0,0,0,1,0
4245,A multiple timescales approach to bridging spiking- and population-level dynamics,"  A rigorous bridge between spiking-level and macroscopic quantities is an
on-going and well-developed story for asynchronously firing neurons, but focus
has shifted to include neural populations exhibiting varying synchronous
dynamics. Recent literature has used the Ott--Antonsen ansatz (2008) to great
effect, allowing a rigorous derivation of an order parameter for large
oscillator populations. The ansatz has been successfully applied using several
models including networks of Kuramoto oscillators, theta models, and
integrate-and-fire neurons, along with many types of network topologies. In the
present study, we take a converse approach: given the mean field dynamics of
slow synapses, predict the synchronization properties of finite neural
populations. The slow synapse assumption is amenable to averaging theory and
the method of multiple timescales. Our proposed theory applies to two
heterogeneous populations of N excitatory n-dimensional and N inhibitory
m-dimensional oscillators with homogeneous synaptic weights. We then
demonstrate our theory using two examples. In the first example we take a
network of excitatory and inhibitory theta neurons and consider the case with
and without heterogeneous inputs. In the second example we use Traub models
with calcium for the excitatory neurons and Wang-Buzs{á}ki models for the
inhibitory neurons. We accurately predict phase drift and phase locking in each
example even when the slow synapses exhibit non-trivial mean-field dynamics.
",0,0,0,0,1,0
4085,Interpretable LSTMs For Whole-Brain Neuroimaging Analyses,"  The analysis of neuroimaging data poses several strong challenges, in
particular, due to its high dimensionality, its strong spatio-temporal
correlation and the comparably small sample sizes of the respective datasets.
To address these challenges, conventional decoding approaches such as the
searchlight reduce the complexity of the decoding problem by considering local
clusters of voxels only. Thereby, neglecting the distributed spatial patterns
of brain activity underlying many cognitive states. In this work, we introduce
the DLight framework, which overcomes these challenges by utilizing a long
short-term memory unit (LSTM) based deep neural network architecture to analyze
the spatial dependency structure of whole-brain fMRI data. In order to maintain
interpretability of the neuroimaging data, we adapt the layer-wise relevance
propagation (LRP) method. Thereby, we enable the neuroscientist user to study
the learned association of the LSTM between the data and the cognitive state of
the individual. We demonstrate the versatility of DLight by applying it to a
large fMRI dataset of the Human Connectome Project. We show that the decoding
performance of our method scales better with large datasets, and moreover
outperforms conventional decoding approaches, while still detecting
physiologically appropriate brain areas for the cognitive states classified. We
also demonstrate that DLight is able to detect these areas on several levels of
data granularity (i.e., group, subject, trial, time point).
",0,0,0,0,1,0
11960,Multi-agent Economics and the Emergence of Critical Markets,"  The dual crises of the sub-prime mortgage crisis and the global financial
crisis has prompted a call for explanations of non-equilibrium market dynamics.
Recently a promising approach has been the use of agent based models (ABMs) to
simulate aggregate market dynamics. A key aspect of these models is the
endogenous emergence of critical transitions between equilibria, i.e. market
collapses, caused by multiple equilibria and changing market parameters.
Several research themes have developed microeconomic based models that include
multiple equilibria: social decision theory (Brock and Durlauf), quantal
response models (McKelvey and Palfrey), and strategic complementarities
(Goldstein). A gap that needs to be filled in the literature is a unified
analysis of the relationship between these models and how aggregate criticality
emerges from the individual agent level. This article reviews the agent-based
foundations of markets starting with the individual agent perspective of
McFadden and the aggregate perspective of catastrophe theory emphasising
connections between the different approaches. It is shown that changes in the
uncertainty agents have in the value of their interactions with one another,
even if these changes are one-sided, plays a central role in systemic market
risks such as market instability and the twin crises effect. These interactions
can endogenously cause crises that are an emergent phenomena of markets.
",0,0,0,0,0,1
499,"Change of grading, injective dimension and dualizing complexes","  Let $G,H$ be groups, $\phi: G \rightarrow H$ a group morphism, and $A$ a
$G$-graded algebra. The morphism $\phi$ induces an $H$-grading on $A$, and on
any $G$-graded $A$-module, which thus becomes an $H$-graded $A$-module. Given
an injective $G$-graded $A$-module, we give bounds for its injective dimension
when seen as $H$-graded $A$-module. Following ideas by Van den Bergh, we give
an application of our results to the stability of dualizing complexes through
change of grading.
",0,0,1,0,0,0
3845,Conformation Clustering of Long MD Protein Dynamics with an Adversarial Autoencoder,"  Recent developments in specialized computer hardware have greatly accelerated
atomic level Molecular Dynamics (MD) simulations. A single GPU-attached cluster
is capable of producing microsecond-length trajectories in reasonable amounts
of time. Multiple protein states and a large number of microstates associated
with folding and with the function of the protein can be observed as
conformations sampled in the trajectories. Clustering those conformations,
however, is needed for identifying protein states, evaluating transition rates
and understanding protein behavior. In this paper, we propose a novel
data-driven generative conformation clustering method based on the adversarial
autoencoder (AAE) and provide the associated software implementation Cong. The
method was tested using a 208 microseconds MD simulation of the fast-folding
peptide Trp-Cage (20 residues) obtained from the D.E. Shaw Research Group. The
proposed clustering algorithm identifies many of the salient features of the
folding process by grouping a large number of conformations that share common
features not easily identifiable in the trajectory.
",0,0,0,0,1,0
214,Sparse mean localization by information theory,"  Sparse feature selection is necessary when we fit statistical models, we have
access to a large group of features, don't know which are relevant, but assume
that most are not. Alternatively, when the number of features is larger than
the available data the model becomes over parametrized and the sparse feature
selection task involves selecting the most informative variables for the model.
When the model is a simple location model and the number of relevant features
does not grow with the total number of features, sparse feature selection
corresponds to sparse mean estimation. We deal with a simplified mean
estimation problem consisting of an additive model with gaussian noise and mean
that is in a restricted, finite hypothesis space. This restriction simplifies
the mean estimation problem into a selection problem of combinatorial nature.
Although the hypothesis space is finite, its size is exponential in the
dimension of the mean. In limited data settings and when the size of the
hypothesis space depends on the amount of data or on the dimension of the data,
choosing an approximation set of hypotheses is a desirable approach. Choosing a
set of hypotheses instead of a single one implies replacing the bias-variance
trade off with a resolution-stability trade off. Generalization capacity
provides a resolution selection criterion based on allowing the learning
algorithm to communicate the largest amount of information in the data to the
learner without error. In this work the theory of approximation set coding and
generalization capacity is explored in order to understand this approach. We
then apply the generalization capacity criterion to the simplified sparse mean
estimation problem and detail an importance sampling algorithm which at once
solves the difficulty posed by large hypothesis spaces and the slow convergence
of uniform sampling algorithms.
",0,0,0,1,0,0
428,Birefringence induced by pp-wave modes in an electromagnetically active dynamic aether,"  In the framework of the Einstein-Maxwell-aether theory we study the
birefringence effect, which can occur in the pp-wave symmetric dynamic aether.
The dynamic aether is considered to be latently birefringent quasi-medium,
which displays this hidden property if and only if the aether motion is
non-uniform, i.e., when the aether flow is characterized by the non-vanishing
expansion, shear, vorticity or acceleration. In accordance with the
dynamo-optical scheme of description of the interaction between electromagnetic
waves and the dynamic aether, we shall model the susceptibility tensors by the
terms linear in the covariant derivative of the aether velocity four-vector.
When the pp-wave modes appear in the dynamic aether, we deal with a
gravitationally induced degeneracy removal with respect to hidden
susceptibility parameters. As a consequence, the phase velocities of
electromagnetic waves possessing orthogonal polarizations do not coincide, thus
displaying the birefringence effect. Two electromagnetic field configurations
are studied in detail: longitudinal and transversal with respect to the aether
pp-wave front. For both cases the solutions are found, which reveal anomalies
in the electromagnetic response on the action of the pp-wave aether mode.
",0,1,0,0,0,0
532,Parity-Forbidden Transitions and Their Impacts on the Optical Absorption Properties of Lead-Free Metal Halide Perovskites and Double Perovskites,"  Using density-functional theory calculations, we analyze the optical
absorption properties of lead (Pb)-free metal halide perovskites
(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or
monovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent
metal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is
not Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions
because of their indirect bandgap nature. Among the nine possible types of
Pb-free metal halide double perovskites, six have direct bandgaps. Of these six
types, four show inversion symmetry-induced parity-forbidden or weak
transitions between band edges, making them not ideal for thin-film solar cell
application. Only one type of Pb-free double perovskite shows optical
absorption and electronic properties suitable for solar cell applications,
namely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide
important insights for designing new metal halide perovskites and double
perovskites for optoelectronic applications.
",0,1,0,0,0,0
923,Generative Models for Spear Phishing Posts on Social Media,"  Historically, machine learning in computer security has prioritized defense:
think intrusion detection systems, malware classification, and botnet traffic
identification. Offense can benefit from data just as well. Social networks,
with their access to extensive personal data, bot-friendly APIs, colloquial
syntax, and prevalence of shortened links, are the perfect venues for spreading
machine-generated malicious content. We aim to discover what capabilities an
adversary might utilize in such a domain. We present a long short-term memory
(LSTM) neural network that learns to socially engineer specific users into
clicking on deceptive URLs. The model is trained with word vector
representations of social media posts, and in order to make a click-through
more likely, it is dynamically seeded with topics extracted from the target's
timeline. We augment the model with clustering to triage high value targets
based on their level of social engagement, and measure success of the LSTM's
phishing expedition using click-rates of IP-tracked links. We achieve state of
the art success rates, tripling those of historic email attack campaigns, and
outperform humans manually performing the same task.
",0,0,0,1,0,0
4599,The sequential loss of allelic diversity,"  This paper gives a new flavor of what Peter Jagers and his co-authors call
`the path to extinction'. In a neutral population with constant size $N$, we
assume that each individual at time $0$ carries a distinct type, or allele. We
consider the joint dynamics of these $N$ alleles, for example the dynamics of
their respective frequencies and more plainly the nonincreasing process
counting the number of alleles remaining by time $t$. We call this process the
extinction process. We show that in the Moran model, the extinction process is
distributed as the process counting (in backward time) the number of common
ancestors to the whole population, also known as the block counting process of
the $N$-Kingman coalescent. Stimulated by this result, we investigate: (1)
whether it extends to an identity between the frequencies of blocks in the
Kingman coalescent and the frequencies of alleles in the extinction process,
both evaluated at jump times; (2) whether it extends to the general case of
$\Lambda$-Fleming-Viot processes.
",0,0,0,0,1,0
883,Finite-time scaling at the Anderson transition for vibrations in solids,"  A model in which a three-dimensional elastic medium is represented by a
network of identical masses connected by springs of random strengths and
allowed to vibrate only along a selected axis of the reference frame, exhibits
an Anderson localization transition. To study this transition, we assume that
the dynamical matrix of the network is given by a product of a sparse random
matrix with real, independent, Gaussian-distributed non-zero entries and its
transpose. A finite-time scaling analysis of system's response to an initial
excitation allows us to estimate the critical parameters of the localization
transition. The critical exponent is found to be $\nu = 1.57 \pm 0.02$ in
agreement with previous studies of Anderson transition belonging to the
three-dimensional orthogonal universality class.
",0,1,0,0,0,0
877,Refracting Metasurfaces without Spurious Diffraction,"  Refraction represents one of the most fundamental operations that may be
performed by a metasurface. However, simple phasegradient metasurface designs
suffer from restricted angular deflection due to spurious diffraction orders.
It has been recently shown, using a circuit-based approach, that refraction
without spurious diffraction, or diffraction-free, can fortunately be achieved
by a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,
we rederive these conditions using a medium-based - and hence more insightfull
- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface
susceptibility tensors, and experimentally demonstrate two diffraction-free
refractive metasurfaces that are essentially lossless, passive, bianisotropic
and reciprocal.
",0,1,0,0,0,0
5744,The Italian Pension Gap: a Stochastic Optimal Control Approach,"  We study the gap between the state pension provided by the Italian pension
system pre-Dini reform and post-Dini reform. The goal is to fill the gap
between the old and the new pension by joining a defined contribution pension
scheme and adopting an optimal investment strategy that is target-based. We
find that it is possible to cover, at least partially, this gap with the
additional income of the pension scheme, especially in the presence of late
retirement and in the presence of stagnant career. Workers with dynamic career
and workers who retire early are those who are most penalised by the reform.
Results are intuitive and in line with previous studies on the subject.
",0,0,0,0,0,1
519,Generalized Lambert series and arithmetic nature of odd zeta values,"  It is pointed out that the generalized Lambert series
$\displaystyle\sum_{n=1}^{\infty}\frac{n^{N-2h}}{e^{n^{N}x}-1}$ studied by
Kanemitsu, Tanigawa and Yoshimoto can be found on page $332$ of Ramanujan's
Lost Notebook in a slightly more general form. We extend an important
transformation of this series obtained by Kanemitsu, Tanigawa and Yoshimoto by
removing restrictions on the parameters $N$ and $h$ that they impose. From our
extension we deduce a beautiful new generalization of Ramanujan's famous
formula for odd zeta values which, for $N$ odd and $m>0$, gives a relation
between $\zeta(2m+1)$ and $\zeta(2Nm+1)$. A result complementary to the
aforementioned generalization is obtained for any even $N$ and
$m\in\mathbb{Z}$. It generalizes a transformation of Wigert and can be regarded
as a formula for $\zeta\left(2m+1-\frac{1}{N}\right)$. Applications of these
transformations include a generalization of the transformation for the
logarithm of Dedekind eta-function $\eta(z)$, Zudilin- and Rivoal-type results
on transcendence of certain values, and a transcendence criterion for Euler's
constant $\gamma$.
",0,0,1,0,0,0
507,Binaural Source Localization based on Modulation-Domain Features and Decision Pooling,"  In this work we apply Amplitude Modulation Spectrum (AMS) features to the
source localization problem. Our approach computes 36 bilateral features for 2s
long signal segments and estimates the azimuthal directions of a sound source
through a binaurally trained classifier. This directional information of a
sound source could be e.g. used to steer the beamformer in a hearing aid to the
source of interest in order to increase the SNR. We evaluated our approach on
the development set of the IEEE-AASP Challenge on sound source localization and
tracking (LOCATA) and achieved a 4.25° smaller MAE than the baseline
approach. Additionally, our approach is computationally less complex.
",1,0,0,0,0,0
1207,Multiplex core-periphery organization of the human connectome,"  The behavior of many complex systems is determined by a core of densely
interconnected units. While many methods are available to identify the core of
a network when connections between nodes are all of the same type, a principled
approach to define the core when multiple types of connectivity are allowed is
still lacking. Here we introduce a general framework to define and extract the
core-periphery structure of multi-layer networks by explicitly taking into
account the connectivity of the nodes at each layer. We show how our method
works on synthetic networks with different size, density, and overlap between
the cores at the different layers. We then apply the method to multiplex brain
networks whose layers encode information both on the anatomical and the
functional connectivity among regions of the human cortex. Results confirm the
presence of the main known hubs, but also suggest the existence of novel brain
core regions that have been discarded by previous analysis which focused
exclusively on the structural layer. Our work is a step forward in the
identification of the core of the human connectome, and contributes to shed
light to a fundamental question in modern neuroscience.
",1,0,0,0,1,0
387,Wiki-index of authors popularity,"  The new index of the author's popularity estimation is represented in the
paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
(Wikipedia Index - WI). Unlike the conventional existed citation indices, the
suggested mark allows to evaluate not only the popularity of the author, as it
can be done by means of calculating the general citation number or by the
Hirsch index, which is often used to measure the author's research rate. The
index gives an opportunity to estimate the author's popularity, his/her
influence within the sought-after area ""knowledge area"" in the Internet - in
the Wikipedia. The suggested index is supposed to be calculated in frames of
the subject domain, and it, on the one hand, avoids the mistaken computation of
the homonyms, and on the other hand - provides the entirety of the subject
area. There are proposed algorithms and the technique of the Wikipedia Index
calculation through the network encyclopedia sounding, the exemplified
calculations of the index for the prominent researchers, and also the methods
of the information networks formation - models of the subject domains by the
automatic monitoring and networks information reference resources analysis. The
considered in the paper notion network corresponds the terms-heads of the
Wikipedia articles.
",1,0,0,0,0,0
753,On well-posedness of a velocity-vorticity formulation of the Navier-Stokes equations with no-slip boundary conditions,"  We study well-posedness of a velocity-vorticity formulation of the
Navier--Stokes equations, supplemented with no-slip velocity boundary
conditions, a no-penetration vorticity boundary condition, along with a natural
vorticity boundary condition depending on a pressure functional. In the
stationary case we prove existence and uniqueness of a suitable weak solution
to the system under a small data condition. The topic of the paper is driven by
recent developments of vorticity based numerical methods for the Navier--Stokes
equations.
",0,0,1,0,0,0
354,Curvature in Hamiltonian Mechanics And The Einstein-Maxwell-Dilaton Action,"  Riemannian geometry is a particular case of Hamiltonian mechanics: the orbits
of the hamiltonian $H=\frac{1}{2}g^{ij}p_{i}p_{j}$ are the geodesics. Given a
symplectic manifold (\Gamma,\omega), a hamiltonian $H:\Gamma\to\mathbb{R}$ and
a Lagrangian sub-manifold $M\subset\Gamma$ we find a generalization of the
notion of curvature. The particular case
$H=\frac{1}{2}g^{ij}\left[p_{i}-A_{i}\right]\left[p_{j}-A_{j}\right]+\phi $ of
a particle moving in a gravitational, electromagnetic and scalar fields is
studied in more detail. The integral of the generalized Ricci tensor w.r.t. the
Boltzmann weight reduces to the action principle
$\int\left[R+\frac{1}{4}F_{ik}F_{jl}g^{kl}g^{ij}-g^{ij}\partial_{i}\phi\partial_{j}\phi\right]e^{-\phi}\sqrt{g}d^{n}q$
for the scalar, vector and tensor fields.
",0,0,1,0,0,0
3873,Implementing a Concept Network Model,"  The same concept can mean different things or be instantiated in different
forms depending on context, suggesting a degree of flexibility within the
conceptual system. We propose that a compositional network model can be used to
capture and predict this flexibility. We modeled individual concepts (e.g.,
BANANA, BOTTLE) as graph-theoretical networks, in which properties (e.g.,
YELLOW, SWEET) were represented as nodes and their associations as edges. In
this framework, networks capture the within-concept statistics that reflect how
properties correlate with each other across instances of a concept. We ran a
classification analysis using graph eigendecomposition to validate these
models, and find that these models can successfully discriminate between object
concepts. We then computed formal measures from these concept networks and
explored their relationship to conceptual structure. We find that diversity
coefficients and core-periphery structure can be interpreted as network-based
measures of conceptual flexibility and stability, respectively. These results
support the feasibility of a concept network framework and highlight its
ability to formally capture important characteristics of the conceptual system.
",0,0,0,0,1,0
14776,On the quasi-sure superhedging duality with frictions,"  We prove the superhedging duality for a discrete-time financial market with
proportional transaction costs under portfolio constraints and model
uncertainty. Frictions are modeled through solvency cones as in the original
model of [Kabanov, Y., Hedging and liquidation under transaction costs in
currency markets. Fin. Stoch., 3(2):237-248, 1999] adapted to the quasi-sure
setup of [Bouchard, B. and Nutz, M., Arbitrage and duality in nondominated
discrete-time models. Ann. Appl. Probab., 25(2):823-859, 2015]. Our results
hold under the condition of No Strict Arbitrage and under the efficient
friction hypothesis.
",0,0,0,0,0,1
5132,CTCF Degradation Causes Increased Usage of Upstream Exons in Mouse Embryonic Stem Cells,"  Transcriptional repressor CTCF is an important regulator of chromatin 3D
structure, facilitating the formation of topologically associating domains
(TADs). However, its direct effects on gene regulation is less well understood.
Here, we utilize previously published ChIP-seq and RNA-seq data to investigate
the effects of CTCF on alternative splicing of genes with CTCF sites. We
compared the amount of RNA-seq signals in exons upstream and downstream of
binding sites following auxin-induced degradation of CTCF in mouse embryonic
stem cells. We found that changes in gene expression following CTCF depletion
were significant, with a general increase in the presence of upstream exons. We
infer that a possible mechanism by which CTCF binding contributes to
alternative splicing is by causing pauses in the transcription mechanism during
which splicing elements are able to concurrently act on upstream exons already
transcribed into RNA.
",0,0,0,0,1,0
876,Latent Geometry and Memorization in Generative Models,"  It can be difficult to tell whether a trained generative model has learned to
generate novel examples or has simply memorized a specific set of outputs. In
published work, it is common to attempt to address this visually, for example
by displaying a generated example and its nearest neighbor(s) in the training
set (in, for example, the L2 metric). As any generative model induces a
probability density on its output domain, we propose studying this density
directly. We first study the geometry of the latent representation and
generator, relate this to the output density, and then develop techniques to
compute and inspect the output density. As an application, we demonstrate that
""memorization"" tends to a density made of delta functions concentrated on the
memorized examples. We note that without first understanding the geometry, the
measurement would be essentially impossible to make.
",1,0,0,1,0,0
3181,DeepTriangle: A Deep Learning Approach to Loss Reserving,"  We propose a novel approach for loss reserving based on deep neural networks.
The approach allows for jointly modeling of paid losses and claims outstanding,
and incorporation of heterogenous inputs. We validate the models on loss
reserving data across lines of business, and show that they attain or exceed
the predictive accuracy of existing stochastic methods. The models require
minimal feature engineering and expert input, and can be automated to produce
forecasts at a high frequency.
",0,0,0,1,0,1
253,Software metadata: How much is enough?,"  Broad efforts are underway to capture metadata about research software and
retain it across services; notable in this regard is the CodeMeta project. What
metadata are important to have about (research) software? What metadata are
useful for searching for codes? What would you like to learn about astronomy
software? This BoF sought to gather information on metadata most desired by
researchers and users of astro software and others interested in registering,
indexing, capturing, and doing research on this software. Information from this
BoF could conceivably result in changes to the Astrophysics Source Code Library
(ASCL) or other resources for the benefit of the community or provide input
into other projects concerned with software metadata.
",1,1,0,0,0,0
223,Objective Procedure for Reconstructing Couplings in Complex Systems,"  Inferring directional connectivity from point process data of multiple
elements is desired in various scientific fields such as neuroscience,
geography, economics, etc. Here, we propose an inference procedure for this
goal based on the kinetic Ising model. The procedure is composed of two steps:
(1) determination of the time-bin size for transforming the point-process data
to discrete time binary data and (2) screening of relevant couplings from the
estimated networks. For these, we develop simple methods based on information
theory and computational statistics. Applications to data from artificial and
\textit{in vitro} neuronal networks show that the proposed procedure performs
fairly well when identifying relevant couplings, including the discrimination
of their signs, with low computational cost. These results highlight the
potential utility of the kinetic Ising model to analyze real interacting
systems with event occurrences.
",0,0,0,0,1,0
435,Hausdorff dimensions in $p$-adic analytic groups,"  Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power
series. The associated metric and Hausdorff dimension function give rise to the
Hausdorff spectrum, which consists of the Hausdorff dimensions of closed
subgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff
dimension function is well understood; in particular, the Hausdorff spectrum
consists of finitely many rational numbers closely linked to the analytic
dimensions of subgroups of $G$.
Conversely, it is a long-standing open question whether the finiteness of the
Hausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the
answer is yes, in a strong sense, under the extra condition that $G$ is
soluble.
Furthermore, we explore the problem and related questions also for other
filtration series, such as the lower $p$-series, the Frattini series, the
modular dimension subgroup series and quite general filtration series. For
instance, we prove, for odd primes $p$, that every countably based pro-$p$
group $G$ with an open subgroup mapping onto 2 copies of the $p$-adic integers
admits a filtration series such that the corresponding Hausdorff spectrum
contains an infinite real interval.
",0,0,1,0,0,0
708,Computing representation matrices for the action of Frobenius to cohomology groups,"  This paper is concerned with the computation of representation matrices for
the action of Frobenius to the cohomology groups of algebraic varieties.
Specifically we shall give an algorithm to compute the matrices for arbitrary
algebraic varieties with defining equations over perfect fields of positive
characteristic, and estimate its complexity. Moreover, we propose a specific
efficient method, which works for complete intersections.
",1,0,1,0,0,0
343,Radially distributed values and normal families,"  Let $L_0$ and $L_1$ be two distinct rays emanating from the origin and let
${\mathcal F}$ be the family of all functions holomorphic in the unit disk
${\mathbb D}$ for which all zeros lie on $L_0$ while all $1$-points lie on
$L_1$. It is shown that ${\mathcal F}$ is normal in ${\mathbb
D}\backslash\{0\}$. The case where $L_0$ is the positive real axis and $L_1$ is
the negative real axis is studied in more detail.
",0,0,1,0,0,0
459,Criterion of positivity for semilinear problems with applications in biology,"  The goal of this article is to provide an useful criterion of positivity and
well-posedness for a wide range of infinite dimensional semilinear abstract
Cauchy problems. This criterion is based on some weak assumptions on the
non-linear part of the semilinear problem and on the existence of a strongly
continuous semigroup generated by the differential operator. To illustrate a
large variety of applications, we exhibit the feasibility of this criterion
through three examples in mathematical biology: epidemiology, predator-prey
interactions and oncology.
",0,0,1,0,0,0
403,Local Communication Protocols for Learning Complex Swarm Behaviors with Deep Reinforcement Learning,"  Swarm systems constitute a challenging problem for reinforcement learning
(RL) as the algorithm needs to learn decentralized control policies that can
cope with limited local sensing and communication abilities of the agents.
While it is often difficult to directly define the behavior of the agents,
simple communication protocols can be defined more easily using prior knowledge
about the given task. In this paper, we propose a number of simple
communication protocols that can be exploited by deep reinforcement learning to
find decentralized control policies in a multi-robot swarm environment. The
protocols are based on histograms that encode the local neighborhood relations
of the agents and can also transmit task-specific information, such as the
shortest distance and direction to a desired target. In our framework, we use
an adaptation of Trust Region Policy Optimization to learn complex
collaborative tasks, such as formation building and building a communication
link. We evaluate our findings in a simulated 2D-physics environment, and
compare the implications of different communication protocols.
",1,0,0,1,0,0
887,The Spatial Shape of Avalanches,"  In disordered elastic systems, driven by displacing a parabolic confining
potential adiabatically slowly, all advance of the system is in bursts, termed
avalanches. Avalanches have a finite extension in time, which is much smaller
than the waiting-time between them. Avalanches also have a finite extension
$\ell$ in space, i.e. only a part of the interface of size $\ell$ moves during
an avalanche. Here we study their spatial shape $\left< S(x)\right>_{\ell}$
given $\ell$, as well as its fluctuations encoded in the second cumulant
$\left< S^{2}(x)\right>_{\ell}^{\rm c}$. We establish scaling relations
governing the behavior close to the boundary. We then give analytic results for
the Brownian force model, in which the microscopic disorder for each degree of
freedom is a random walk. Finally, we confirm these results with numerical
simulations. To do this properly we elucidate the influence of discretization
effects, which also confirms the assumptions entering into the scaling ansatz.
This allows us to reach the scaling limit already for avalanches of moderate
size. We find excellent agreement for the universal shape, its fluctuations,
including all amplitudes.
",0,1,0,0,0,0
998,Learning the Sparse and Low Rank PARAFAC Decomposition via the Elastic Net,"  In this article, we derive a Bayesian model to learning the sparse and low
rank PARAFAC decomposition for the observed tensor with missing values via the
elastic net, with property to find the true rank and sparse factor matrix which
is robust to the noise. We formulate efficient block coordinate descent
algorithm and admax stochastic block coordinate descent algorithm to solve it,
which can be used to solve the large scale problem. To choose the appropriate
rank and sparsity in PARAFAC decomposition, we will give a solution path by
gradually increasing the regularization to increase the sparsity and decrease
the rank. When we find the sparse structure of the factor matrix, we can fixed
the sparse structure, using a small to regularization to decreasing the
recovery error, and one can choose the proper decomposition from the solution
path with sufficient sparse factor matrix with low recovery error. We test the
power of our algorithm on the simulation data and real data, which show it is
powerful.
",0,0,1,1,0,0
3472,Explaining Parochialism: A Causal Account for Political Polarization in Changing Economic Environments,"  Political and social polarization are a significant cause of conflict and
poor governance in many societies, thus understanding their causes is of
considerable importance. Here we demonstrate that shifts in socialization
strategy similar to political polarization and/or identity politics could be a
constructive response to periods of apparent economic decline. We start from
the observation that economies, like ecologies are seldom at equilibrium.
Rather, they often suffer both negative and positive shocks. We show that even
where in an expanding economy, interacting with diverse out-groups can afford
benefits through innovation and exploration, if that economy contracts, a
strategy of seeking homogeneous groups can be important to maintaining
individual solvency. This is true even where the expected value of out group
interaction exceeds that of in group interactions. Our account unifies what
were previously seen as conflicting explanations: identity threat versus
economic anxiety. Our model indicates that in periods of extreme deprivation,
cooperation with diversity again becomes the best (in fact, only viable)
strategy. However, our model also shows that while polarization may increase
gradually in response to shifts in the economy, gradual decrease of
polarization may not be an available strategy; thus returning to previous
levels of cooperation may require structural change.
",0,0,0,0,1,1
539,"Characterizing videos, audience and advertising in Youtube channels for kids","  Online video services, messaging systems, games and social media services are
tremendously popular among young people and children in many countries. Most of
the digital services offered on the internet are advertising funded, which
makes advertising ubiquitous in children's everyday life. To understand the
impact of advertising-based digital services on children, we study the
collective behavior of users of YouTube for kids channels and present the
demographics of a large number of users. We collected data from 12,848 videos
from 17 channels in US and UK and 24 channels in Brazil. The channels in
English have been viewed more than 37 billion times. We also collected more
than 14 million comments made by users. Based on a combination of text-analysis
and face recognition tools, we show the presence of racial and gender biases in
our large sample of users. We also identify children actively using YouTube,
although the minimum age for using the service is 13 years in most countries.
We provide comparisons of user behavior among the three countries, which
represent large user populations in the global North and the global South.
",1,0,0,0,0,0
871,Variability response functions for statically determinate beams with arbitrary nonlinear constitutive laws,"  The variability response function (VRF) is generalized to statically
determinate Euler Bernoulli beams with arbitrary stress-strain laws following
Cauchy elastic behavior. The VRF is a Green's function that maps the spectral
density function (SDF) of a statistically homogeneous random field describing
the correlation structure of input uncertainty to the variance of a response
quantity. The appeal of such Green's functions is that the variance can be
determined for any correlation structure by a trivial computation of a
convolution integral. The method introduced in this work derives VRFs in closed
form for arbitrary nonlinear Cauchy-elastic constitutive laws and is
demonstrated through three examples. It is shown why and how higher order
spectra of the random field affect the response variance for nonlinear
constitutive laws. In the general sense, the VRF for a statically determinate
beam is found to be a matrix kernel whose inner product by a matrix of higher
order SDFs and statistical moments is integrated to give the response variance.
The resulting VRF matrix is unique regardless of the random field's marginal
probability density function (PDF) and SDFs.
",0,1,0,0,0,0
859,"A Framework for Time-Consistent, Risk-Sensitive Model Predictive Control: Theory and Algorithms","  In this paper we present a framework for risk-sensitive model predictive
control (MPC) of linear systems affected by stochastic multiplicative
uncertainty. Our key innovation is to consider a time-consistent, dynamic risk
evaluation of the cumulative cost as the objective function to be minimized.
This framework is axiomatically justified in terms of time-consistency of risk
assessments, is amenable to dynamic optimization, and is unifying in the sense
that it captures a full range of risk preferences from risk-neutral (i.e.,
expectation) to worst case. Within this framework, we propose and analyze an
online risk-sensitive MPC algorithm that is provably stabilizing. Furthermore,
by exploiting the dual representation of time-consistent, dynamic risk
measures, we cast the computation of the MPC control law as a convex
optimization problem amenable to real-time implementation. Simulation results
are presented and discussed.
",1,0,1,0,0,0
290,Unified Treatment of Spin Torques using a Coupled Magnetisation Dynamics and Three-Dimensional Spin Current Solver,"  A three-dimensional spin current solver based on a generalised spin
drift-diffusion description, including the spin Hall effect, is integrated with
a magnetisation dynamics solver. The resulting model is shown to simultaneously
reproduce the spin-orbit torques generated using the spin Hall effect, spin
pumping torques generated by magnetisation dynamics in multilayers, as well as
the spin transfer torques acting on magnetisation regions with spatial
gradients, whilst field-like and spin-like torques are reproduced in a spin
valve geometry. Two approaches to modelling interfaces are analysed, one based
on the spin mixing conductance and the other based on continuity of spin
currents where the spin dephasing length governs the absorption of transverse
spin components. In both cases analytical formulas are derived for the
spin-orbit torques in a heavy metal / ferromagnet bilayer geometry, showing in
general both field-like and damping-like torques are generated. The limitations
of the analytical approach are discussed, showing that even in a simple bilayer
geometry, due to the non-uniformity of the spin currents, a full
three-dimensional treatment is required. Finally the model is applied to the
quantitative analysis of the spin Hall angle in Pt by reproducing published
experimental data on the ferromagnetic resonance linewidth in the bilayer
geometry.
",0,1,0,0,0,0
3678,Fitting 3D Shapes from Partial and Noisy Point Clouds with Evolutionary Computing,"  Point clouds obtained from photogrammetry are noisy and incomplete models of
reality. We propose an evolutionary optimization methodology that is able to
approximate the underlying object geometry on such point clouds. This approach
assumes a priori knowledge on the 3D structure modeled and enables the
identification of a collection of primitive shapes approximating the scene.
Built-in mechanisms that enforce high shape diversity and adaptive population
size make this method suitable to modeling both simple and complex scenes. We
focus here on the case of cylinder approximations and we describe, test, and
compare a set of mutation operators designed for optimal exploration of their
search space. We assess the robustness and limitations of this algorithm
through a series of synthetic examples, and we finally demonstrate its general
applicability on two real-life cases in vegetation and industrial settings.
",1,0,0,0,1,0
1998,"Pinned, locked, pushed, and pulled traveling waves in structured environments","  Traveling fronts describe the transition between two alternative states in a
great number of physical and biological systems. Examples include the spread of
beneficial mutations, chemical reactions, and the invasions by foreign species.
In homogeneous environments, the alternative states are separated by a smooth
front moving at a constant velocity. This simple picture can break down in
structured environments such as tissues, patchy landscapes, and microfluidic
devices. Habitat fragmentation can pin the front at a particular location or
lock invasion velocities into specific values. Locked velocities are not
sensitive to moderate changes in dispersal or growth and are determined by the
spatial and temporal periodicity of the environment. The synchronization with
the environment results in discontinuous fronts that propagate as periodic
pulses. We characterize the transition from continuous to locked invasions and
show that it is controlled by positive density-dependence in dispersal or
growth. We also demonstrate that velocity locking is robust to demographic and
environmental fluctuations and examine stochastic dynamics and evolution in
locked invasions.
",0,0,0,0,1,0
5005,"Hybrid Sterility Can Only be Primary When Acting as a Reproductive Barrier for Sympatric Speciation,","  Parental gametes unite to form a zygote that develops into an adult with
gonads that, in turn, produce gametes. Interruption of this germinal cycle by
prezygotic or postzygotic reproductive barriers can result in two independent
cycles, each with the potential to evolve into a new species. When the
speciation process is complete, members of each species are fully
reproductively isolated from those of the other. During speciation a primary
barrier may be supported and eventually superceded by a later appearing
secondary barrier. For those holding certain cases of prezygotic isolation to
be primary (e.g. elephant cannot copulate with mouse), the onus is to show that
they had not been preceded over evolutionary time by periods of postzygotic
hybrid inviability (genically determined) or sterility (genically or
chromosomally determined). Likewise, the onus is upon those holding cases of
hybrid inviability to be primary (e.g. Dobzhansky-Muller epistatic
incompatibilities), to show that they had not been preceded by periods, however
brief, of hybrid sterility. The latter, when acting as a sympatric barrier
causing reproductive isolation, can only be primary. In many cases, hybrid
sterility may result from incompatibilities between parental chromosomes that
attempt to pair during meiosis in the gonad of their offspring
(Winge-Crowther-Bateson incompatibilities). While WCB incompatibilities have
long been observed on a microscopic scale, there is growing evidence for a role
of dispersed finer DNA sequence differences.
",0,0,0,0,1,0
729,Reconstructing global fields from dynamics in the abelianized Galois group,"  We study a dynamical system induced by the Artin reciprocity map for a global
field. We translate the conjugacy of such dynamical systems into various
arithmetical properties that are equivalent to field isomorphism, relating it
to anabelian geometry.
",0,0,1,0,0,0
5934,An Introduction to Animal Movement Modeling with Hidden Markov Models using Stan for Bayesian Inference,"  Hidden Markov models (HMMs) are popular time series model in many fields
including ecology, economics and genetics. HMMs can be defined over discrete or
continuous time, though here we only cover the former. In the field of movement
ecology in particular, HMMs have become a popular tool for the analysis of
movement data because of their ability to connect observed movement data to an
underlying latent process, generally interpreted as the animal's unobserved
behavior. Further, we model the tendency to persist in a given behavior over
time. Notation presented here will generally follow the format of Zucchini et
al. (2016) and cover HMMs applied in an unsupervised case to animal movement
data, specifically positional data. We provide Stan code to analyze movement
data of the wild haggis as presented first in Michelot et al. (2016).
",0,0,0,1,1,0
760,"Expropriations, Property Confiscations and New Offshore Entities: Evidence from the Panama Papers","  Using the Panama Papers, we show that the beginning of media reporting on
expropriations and property confiscations in a country increases the
probability that offshore entities are incorporated by agents from the same
country in the same month. This result is robust to the use of country-year
fixed effects and the exclusion of tax havens. Further analysis shows that the
effect is driven by countries with non-corrupt and effective governments, which
supports the notion that offshore entities are incorporated when reasonably
well-intended and well-functioning governments become more serious about
fighting organized crime by confiscating proceeds of crime.
",0,0,0,0,0,1
6435,PIMKL: Pathway Induced Multiple Kernel Learning,"  Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power, limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, and we have very little
understanding about the mechanisms that lead to the prediction. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to reliably classify samples that
can also help gain insights into the molecular mechanisms that underlie the
classification. PIMKL exploits prior knowledge in the form of a molecular
interaction network and annotated gene sets, by optimizing a mixture of
pathway-induced kernels using a Multiple Kernel Learning (MKL) algorithm, an
approach that has demonstrated excellent performance in different machine
learning applications. After optimizing the combination of kernels for
prediction of a specific phenotype, the model provides a stable molecular
signature that can be interpreted in the light of the ingested prior knowledge
and that can be used in transfer learning tasks.
",0,0,0,1,1,0
380,IP determination and 1+1 REMPI spectrum of SiO at 210-220 nm with implications for SiO$^{+}$ ion trap loading,"  The 1+1 REMPI spectrum of SiO in the 210-220 nm range is recorded. Observed
bands are assigned to the $A-X$ vibrational bands $(v``=0-3, v`=5-10)$ and a
tentative assignment is given to the 2-photon transition from $X$ to the
n=12-13 $[X^{2}{\Sigma}^{+},v^{+}=1]$ Rydberg states at 216-217 nm. We estimate
the IP of SiO to be 11.59(1) eV. The SiO$^{+}$ cation has previously been
identified as a molecular candidate amenable to laser control. Our work allows
us to identify an efficient method for loading cold SiO$^{+}$ from an ablated
sample of SiO into an ion trap via the $(5,0)$ $A-X$ band at 213.977 nm.
",0,1,0,0,0,0
392,Empirical determination of the optimum attack for fragmentation of modular networks,"  All possible removals of $n=5$ nodes from networks of size $N=100$ are
performed in order to find the optimal set of nodes which fragments the
original network into the smallest largest connected component. The resulting
attacks are ordered according to the size of the largest connected component
and compared with the state of the art methods of network attacks. We chose
attacks of size $5$ on relative small networks of size $100$ because the number
of all possible attacks, ${100}\choose{5}$ $\approx 10^8$, is at the verge of
the possible to compute with the available standard computers. Besides, we
applied the procedure in a series of networks with controlled and varied
modularity, comparing the resulting statistics with the effect of removing the
same amount of vertices, according to the known most efficient disruption
strategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index
attack (CI), and Modular Based Attack (MBA). Results show that modularity has
an inverse relation with robustness, with $Q_c \approx 0.7$ being the critical
value. For modularities lower than $Q_c$, all heuristic method gives mostly the
same results than with random attacks, while for bigger $Q$, networks are less
robust and highly vulnerable to malicious attacks.
",1,0,0,0,0,0
774,Low energy bands and transport properties of chromium arsenide,"  We apply a method that combines the tight-binding approximation and the
Lowdin down-folding procedure to evaluate the electronic band structure of the
newly discovered pressure-induced superconductor CrAs. By integrating out all
low-lying arsenic degrees of freedom, we derive an effective Hamiltonian model
describing the Cr d bands near the Fermi level. We calculate and make
predictions for the energy spectra, the Fermi surface, the density of states
and transport and magnetic properties of this compound. Our results are
consistent with local-density approximation calculations as well as they show
good agreement with available experimental data for resistivity and Cr magnetic
moment.
",0,1,0,0,0,0
336,Invariance of Weight Distributions in Rectified MLPs,"  An interesting approach to analyzing neural networks that has received
renewed attention is to examine the equivalent kernel of the neural network.
This is based on the fact that a fully connected feedforward network with one
hidden layer, a certain weight distribution, an activation function, and an
infinite number of neurons can be viewed as a mapping into a Hilbert space. We
derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for
all rotationally-invariant weight distributions, generalizing a previous result
that required Gaussian weight distributions. Additionally, the Central Limit
Theorem is used to show that for certain activation functions, kernels
corresponding to layers with weight distributions having $0$ mean and finite
absolute third moment are asymptotically universal, and are well approximated
by the kernel corresponding to layers with spherical Gaussian weights. In deep
networks, as depth increases the equivalent kernel approaches a pathological
fixed point, which can be used to argue why training randomly initialized
networks can be difficult. Our results also have implications for weight
initialization.
",1,0,0,1,0,0
551,Resting-state ASL : Toward an optimal sequence duration,"  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition.
",0,0,0,0,1,0
5295,Mitochondrial network fragmentation modulates mutant mtDNA accumulation independently of absolute fission-fusion rates,"  Mitochondrial DNA (mtDNA) mutations cause severe congenital diseases but may
also be associated with healthy aging. MtDNA is stochastically replicated and
degraded, and exists within organelles which undergo dynamic fusion and
fission. The role of the resulting mitochondrial networks in determining the
time evolution of the cellular proportion of mutated mtDNA molecules
(heteroplasmy), and cell-to-cell variability in heteroplasmy (heteroplasmy
variance), remains incompletely understood. Heteroplasmy variance is
particularly important since it modulates the number of pathological cells in a
tissue. Here, we provide the first wide-reaching mathematical treatment which
bridges mitochondrial network and genetic states. We show that, for a range of
models, the rate of increase in heteroplasmy variance, and the rate of
\textit{de novo} mutation, is proportionately modulated by the fraction of
unfused mitochondria, independently of the absolute fission-fusion rate. In the
context of selective fusion, we show that intermediate fusion/fission ratios
are optimal for the clearance of mtDNA mutants. Our findings imply that
modulating network state, mitophagy rate and copy number to slow down
heteroplasmy dynamics when mean heteroplasmy is low, could have therapeutic
advantages for mitochondrial disease and healthy aging.
",0,0,0,0,1,0
17482,The effect of prudence on the optimal allocation in possibilistic and mixed models,"  In this paper two portfolio choice models are studied: a purely possibilistic
model, in which the return of a risky asset is a fuzzy number, and a mixed
model in which a probabilistic background risk is added. For the two models an
approximate formula of the optimal allocation is computed, with respect to the
possibilistic moments associated with fuzzy numbers and the indicators of the
investor risk preferences (risk aversion, prudence).
",0,0,0,0,0,1
596,Quantum oscillations and a non-trivial Berry phase in the noncentrosymmetric superconductor BiPd,"  We report the measurements of de Haas-van Alphen (dHvA) oscillations in the
noncentrosymmetric superconductor BiPd. Several pieces of a complex multi-sheet
Fermi surface are identified, including a small pocket (frequency 40 T) which
is three dimensional and anisotropic. From the temperature dependence of the
amplitude of the oscillations, the cyclotron effective mass is ($0.18$ $\pm$
0.1) $m_e$. Further analysis showed a non-trivial $\pi$-Berry phase is
associated with the 40 T pocket, which strongly supports the presence of
topological states in bulk BiPd and may result in topological superconductivity
due to the proximity coupling to other bands.
",0,1,0,0,0,0
467,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,"  Recent studies on diffusion-based sampling methods have shown that Langevin
Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and
rigorous theoretical guarantees have been proven for both asymptotic and
finite-time regimes. Algorithmically, LMC-based algorithms resemble the
well-known gradient descent (GD) algorithm, where the GD recursion is perturbed
by an additive Gaussian noise whose variance has a particular form. Fractional
Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the
Gaussian noise is replaced by a heavy-tailed {\alpha}-stable noise. As opposed
to its Gaussian counterpart, these heavy-tailed perturbations can incur large
jumps and it has been empirically demonstrated that the choice of
{\alpha}-stable noise can provide several advantages in modern machine learning
problems, both in optimization and sampling contexts. However, as opposed to
LMC, only asymptotic convergence properties of FLMC have been yet established.
In this study, we analyze the non-asymptotic behavior of FLMC for non-convex
optimization and prove finite-time bounds for its expected suboptimality. Our
results show that the weak-error of FLMC increases faster than LMC, which
suggests using smaller step-sizes in FLMC. We finally extend our results to the
case where the exact gradients are replaced by stochastic gradients and show
that similar results hold in this setting as well.
",1,0,0,1,0,0
4042,ModelFactory: A Matlab/Octave based toolbox to create human body models,"  Background: Model-based analysis of movements can help better understand
human motor control. Here, the models represent the human body as an
articulated multi-body system that reflects the characteristics of the human
being studied.
Results: We present an open-source toolbox that allows for the creation of
human models with easy-to-setup, customizable configurations. The toolbox
scripts are written in Matlab/Octave and provide a command-based interface as
well as a graphical interface to construct, visualize and export models.
Built-in software modules provide functionalities such as automatic scaling of
models based on subject height and weight, custom scaling of segment lengths,
mass and inertia, addition of body landmarks, and addition of motion capture
markers. Users can set up custom definitions of joints, segments and other body
properties using the many included examples as templates. In addition to the
human, any number of objects (e.g. exoskeletons, orthoses, prostheses, boxes)
can be added to the modeling environment.
Conclusions: The ModelFactory toolbox is published as open-source software
under the permissive zLib license. The toolbox fulfills an important function
by making it easier to create human models, and should be of interest to human
movement researchers.
This document is the author's version of this article.
",1,0,0,0,1,0
542,weedNet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming,"  Selective weed treatment is a critical step in autonomous crop management as
related to crop health and yield. However, a key challenge is reliable, and
accurate weed detection to minimize damage to surrounding plants. In this
paper, we present an approach for dense semantic weed classification with
multispectral images collected by a micro aerial vehicle (MAV). We use the
recently developed encoder-decoder cascaded Convolutional Neural Network (CNN),
Segnet, that infers dense semantic classes while allowing any number of input
image channels and class balancing with our sugar beet and weed datasets. To
obtain training datasets, we established an experimental field with varying
herbicide levels resulting in field plots containing only either crop or weed,
enabling us to use the Normalized Difference Vegetation Index (NDVI) as a
distinguishable feature for automatic ground truth generation. We train 6
models with different numbers of input channels and condition (fine-tune) it to
achieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification
metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested
for MAV integration. Dataset used in this paper is released to support the
community and future work.
",1,0,0,0,0,0
15803,Financial density forecasts: A comprehensive comparison of risk-neutral and historical schemes,"  We investigate the forecasting ability of the most commonly used benchmarks
in financial economics. We approach the usual caveats of probabilistic
forecasts studies -small samples, limited models and non-holistic validations-
by performing a comprehensive comparison of 15 predictive schemes during a time
period of over 21 years. All densities are evaluated in terms of their
statistical consistency, local accuracy and forecasting errors. Using a new
composite indicator, the Integrated Forecast Score (IFS), we show that
risk-neutral densities outperform historical-based predictions in terms of
information content. We find that the Variance Gamma model generates the
highest out-of-sample likelihood of observed prices and the lowest predictive
errors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts
across the entire density range. In contrast, lognormal densities, the Heston
model or the Breeden-Litzenberger formula yield biased predictions and are
rejected in statistical tests.
",0,0,0,1,0,1
2738,Aggregating multiple types of complex data in stock market prediction: A model-independent framework,"  The increasing richness in volume, and especially types of data in the
financial domain provides unprecedented opportunities to understand the stock
market more comprehensively and makes the price prediction more accurate than
before. However, they also bring challenges to classic statistic approaches
since those models might be constrained to a certain type of data. Aiming at
aggregating differently sourced information and offering type-free capability
to existing models, a framework for predicting stock market of scenarios with
mixed data, including scalar data, compositional data (pie-like) and functional
data (curve-like), is established. The presented framework is
model-independent, as it serves like an interface to multiple types of data and
can be combined with various prediction models. And it is proved to be
effective through numerical simulations. Regarding to price prediction, we
incorporate the trading volume (scalar data), intraday return series
(functional data), and investors' emotions from social media (compositional
data) through the framework to competently forecast whether the market goes up
or down at opening in the next day. The strong explanatory power of the
framework is further demonstrated. Specifically, it is found that the intraday
returns impact the following opening prices differently between bearish market
and bullish market. And it is not at the beginning of the bearish market but
the subsequent period in which the investors' ""fear"" comes to be indicative.
The framework would help extend existing prediction models easily to scenarios
with multiple types of data and shed light on a more systemic understanding of
the stock market.
",0,0,0,1,0,1
407,Beyond the technical challenges for deploying Machine Learning solutions in a software company,"  Recently software development companies started to embrace Machine Learning
(ML) techniques for introducing a series of advanced functionality in their
products such as personalisation of the user experience, improved search,
content recommendation and automation. The technical challenges for tackling
these problems are heavily researched in literature. A less studied area is a
pragmatic approach to the role of humans in a complex modern industrial
environment where ML based systems are developed. Key stakeholders affect the
system from inception and up to operation and maintenance. Product managers
want to embed ""smart"" experiences for their users and drive the decisions on
what should be built next; software engineers are challenged to build or
utilise ML software tools that require skills that are well outside of their
comfort zone; legal and risk departments may influence design choices and data
access; operations teams are requested to maintain ML systems which are
non-stationary in their nature and change behaviour over time; and finally ML
practitioners should communicate with all these stakeholders to successfully
build a reliable system. This paper discusses some of the challenges we faced
in Atlassian as we started investing more in the ML space.
",1,0,0,1,0,0
611,Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement,"  In this paper, we focus on fully automatic traffic surveillance camera
calibration, which we use for speed measurement of passing vehicles. We improve
over a recent state-of-the-art camera calibration method for traffic
surveillance based on two detected vanishing points. More importantly, we
propose a novel automatic scene scale inference method. The method is based on
matching bounding boxes of rendered 3D models of vehicles with detected
bounding boxes in the image. The proposed method can be used from arbitrary
viewpoints, since it has no constraints on camera placement. We evaluate our
method on the recent comprehensive dataset for speed measurement BrnoCompSpeed.
Experiments show that our automatic camera calibration method by detection of
two vanishing points reduces error by 50% (mean distance ratio error reduced
from 0.18 to 0.09) compared to the previous state-of-the-art method. We also
show that our scene scale inference method is more precise, outperforming both
state-of-the-art automatic calibration method for speed measurement (error
reduction by 86% -- 7.98km/h to 1.10km/h) and manual calibration (error
reduction by 19% -- 1.35km/h to 1.10km/h). We also present qualitative results
of the proposed automatic camera calibration method on video sequences obtained
from real surveillance cameras in various places, and under different lighting
conditions (night, dawn, day).
",1,0,0,0,0,0
915,Riemannian stochastic variance reduced gradient,"  Stochastic variance reduction algorithms have recently become popular for
minimizing the average of a large but finite number of loss functions. In this
paper, we propose a novel Riemannian extension of the Euclidean stochastic
variance reduced gradient algorithm (R-SVRG) to a manifold search space. The
key challenges of averaging, adding, and subtracting multiple gradients are
addressed with retraction and vector transport. We present a global convergence
analysis of the proposed algorithm with a decay step size and a local
convergence rate analysis under a fixed step size under some natural
assumptions. The proposed algorithm is applied to problems on the Grassmann
manifold, such as principal component analysis, low-rank matrix completion, and
computation of the Karcher mean of subspaces, and outperforms the standard
Riemannian stochastic gradient descent algorithm in each case.
",1,0,1,1,0,0
5745,Immigration-induced phase transition in a regulated multispecies birth-death process,"  Power-law-distributed species counts or clone counts arise in many biological
settings such as multispecies cell populations, population genetics, and
ecology. This empirical observation that the number of species $c_{k}$
represented by $k$ individuals scales as negative powers of $k$ is also
supported by a series of theoretical birth-death-immigration (BDI) models that
consistently predict many low-population species, a few intermediate-population
species, and very high-population species. However, we show how a simple global
population-dependent regulation in a neutral BDI model destroys the power law
distributions. Simulation of the regulated BDI model shows a high probability
of observing a high-population species that dominates the total population.
Further analysis reveals that the origin of this breakdown is associated with
the failure of a mean-field approximation for the expected species abundance
distribution. We find an accurate estimate for the expected distribution
$\langle c_k \rangle$ by mapping the problem to a lower-dimensional Moran
process, allowing us to also straightforwardly calculate the covariances
$\langle c_k c_\ell \rangle$. Finally, we exploit the concepts associated with
energy landscapes to explain the failure of the mean-field assumption by
identifying a phase transition in the quasi-steady-state species counts
triggered by a decreasing immigration rate.
",0,0,0,0,1,0
587,Dynamics of cracks in disordered materials,"  Predicting when rupture occurs or cracks progress is a major challenge in
numerous elds of industrial, societal and geophysical importance. It remains
largely unsolved: Stress enhancement at cracks and defects, indeed, makes the
macroscale dynamics extremely sensitive to the microscale material disorder.
This results in giant statistical uctuations and non-trivial behaviors upon
upscaling dicult to assess via the continuum approaches of engineering. These
issues are examined here. We will see: How linear elastic fracture mechanics
sidetracks the diculty by reducing the problem to that of the propagation of a
single crack in an eective material free of defects, How slow cracks sometimes
display jerky dynamics, with sudden violent events incompatible with the
previous approach, and how some paradigms of statistical physics can explain
it, How abnormally fast cracks sometimes emerge due to the formation of
microcracks at very small scales.
",0,1,0,0,0,0
854,HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning,"  In this paper, we introduce a new model for leveraging unlabeled data to
improve generalization performances of image classifiers: a two-branch
encoder-decoder architecture called HybridNet. The first branch receives
supervision signal and is dedicated to the extraction of invariant
class-related representations. The second branch is fully unsupervised and
dedicated to model information discarded by the first branch to reconstruct
input data. To further support the expected behavior of our model, we propose
an original training objective. It favors stability in the discriminative
branch and complementarity between the learned representations in the two
branches. HybridNet is able to outperform state-of-the-art results on CIFAR-10,
SVHN and STL-10 in various semi-supervised settings. In addition,
visualizations and ablation studies validate our contributions and the behavior
of the model on both CIFAR-10 and STL-10 datasets.
",0,0,0,1,0,0
884,Universal and shape dependent features of surface superconductivity,"  We analyze the response of a type II superconducting wire to an external
magnetic field parallel to it in the framework of Ginzburg-Landau theory. We
focus on the surface superconductivity regime of applied field between the
second and third critical values, where the superconducting state survives only
close to the sample's boundary. Our first finding is that, in first
approximation, the shape of the boundary plays no role in determining the
density of superconducting electrons. A second order term is however isolated,
directly proportional to the mean curvature of the boundary. This demonstrates
that points of higher boundary curvature (counted inwards) attract
superconducting electrons.
",0,1,1,0,0,0
15025,Reducing Estimation Risk in Mean-Variance Portfolios with Machine Learning,"  In portfolio analysis, the traditional approach of replacing population
moments with sample counterparts may lead to suboptimal portfolio choices. I
show that optimal portfolio weights can be estimated using a machine learning
(ML) framework, where the outcome to be predicted is a constant and the vector
of explanatory variables is the asset returns. It follows that ML specifically
targets estimation risk when estimating portfolio weights, and that
""off-the-shelf"" ML algorithms can be used to estimate the optimal portfolio in
the presence of parameter uncertainty. The framework nests the traditional
approach and recently proposed shrinkage approaches as special cases. By
relying on results from the ML literature, I derive new insights for existing
approaches and propose new estimation methods. Based on simulation studies and
several datasets, I find that ML significantly reduces estimation risk compared
to both the traditional approach and the equal weight strategy.
",0,0,0,0,0,1
272,Auto-Meta: Automated Gradient Based Meta Learner Search,"  Fully automating machine learning pipelines is one of the key challenges of
current artificial intelligence research, since practical machine learning
often requires costly and time-consuming human-powered processes such as model
design, algorithm development, and hyperparameter tuning. In this paper, we
verify that automated architecture search synergizes with the effect of
gradient-based meta learning. We adopt the progressive neural architecture
search \cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal
architectures for meta-learners. The gradient based meta-learner whose
architecture was automatically found achieved state-of-the-art results on the
5-shot 5-way Mini-ImageNet classification problem with $74.65\%$ accuracy,
which is $11.54\%$ improvement over the result obtained by the first
gradient-based meta-learner called MAML
\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is
the first successful neural architecture search implementation in the context
of meta learning.
",0,0,0,1,0,0
2391,Generalizations of the 'Linear Chain Trick': Incorporating more flexible dwell time distributions into mean field ODE models,"  Mathematical modelers have long known of a ""rule of thumb"" referred to as the
Linear Chain Trick (LCT; aka the Gamma Chain Trick): a technique used to
construct mean field ODE models from continuous-time stochastic state
transition models where the time an individual spends in a given state (i.e.,
the dwell time) is Erlang distributed (i.e., gamma distributed with integer
shape parameter). Despite the LCT's widespread use, we lack general theory to
facilitate the easy application of this technique, especially for complex
models. This has forced modelers to choose between constructing ODE models
using heuristics with oversimplified dwell time assumptions, using time
consuming derivations from first principles, or to instead use non-ODE models
(like integro-differential equations or delay differential equations) which can
be cumbersome to derive and analyze. Here, we provide analytical results that
enable modelers to more efficiently construct ODE models using the LCT or
related extensions. Specifically, we 1) provide novel extensions of the LCT to
various scenarios found in applications; 2) provide formulations of the LCT and
it's extensions that bypass the need to derive ODEs from integral or stochastic
model equations; and 3) introduce a novel Generalized Linear Chain Trick (GLCT)
framework that extends the LCT to a much broader family of distributions,
including the flexible phase-type distributions which can approximate
distributions on $\mathbb{R}^+$ and be fit to data. These results give modelers
more flexibility to incorporate appropriate dwell time assumptions into mean
field ODEs, including conditional dwell time distributions, and these results
help clarify connections between individual-level stochastic model assumptions
and the structure of corresponding mean field ODEs.
",0,0,0,0,1,0
517,The curl operator on odd-dimensional manifolds,"  We study the spectral properties of curl, a linear differential operator of
first order acting on differential forms of appropriate degree on an
odd-dimensional closed oriented Riemannian manifold. In three dimensions its
eigenvalues are the electromagnetic oscillation frequencies in vacuum without
external sources. In general, the spectrum consists of the eigenvalue 0 with
infinite multiplicity and further real discrete eigenvalues of finite
multiplicity. We compute the Weyl asymptotics and study the zeta-function. We
give a sharp lower eigenvalue bound for positively curved manifolds and analyze
the equality case. Finally, we compute the spectrum for flat tori, round
spheres and 3-dimensional spherical space forms.
",0,0,1,0,0,0
605,Learning Sparse Polymatrix Games in Polynomial Time and Sample Complexity,"  We consider the problem of learning sparse polymatrix games from observations
of strategic interactions. We show that a polynomial time method based on
$\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash
equilibria are the $\epsilon$-Nash equilibria of the game from which the data
was generated (true game), in $\mathcal{O}(m^4 d^4 \log (pd))$ samples of
strategy profiles --- where $m$ is the maximum number of pure strategies of a
player, $p$ is the number of players, and $d$ is the maximum degree of the game
graph. Under slightly more stringent separability conditions on the payoff
matrices of the true game, we show that our method learns a game with the exact
same Nash equilibria as the true game. We also show that $\Omega(d \log (pm))$
samples are necessary for any method to consistently recover a game, with the
same Nash-equilibria as the true game, from observations of strategic
interactions. We verify our theoretical results through simulation experiments.
",1,0,0,0,0,0
286,Gini estimation under infinite variance,"  We study the problems related to the estimation of the Gini index in presence
of a fat-tailed data generating process, i.e. one in the stable distribution
class with finite mean but infinite variance (i.e. with tail index
$\alpha\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be
reliably estimated using conventional nonparametric methods, because of a
downward bias that emerges under fat tails. This has important implications for
the ongoing discussion about economic inequality.
We start by discussing how the nonparametric estimator of the Gini index
undergoes a phase transition in the symmetry structure of its asymptotic
distribution, as the data distribution shifts from the domain of attraction of
a light-tailed distribution to that of a fat-tailed one, especially in the case
of infinite variance. We also show how the nonparametric Gini bias increases
with lower values of $\alpha$. We then prove that maximum likelihood estimation
outperforms nonparametric methods, requiring a much smaller sample size to
reach efficiency.
Finally, for fat-tailed data, we provide a simple correction mechanism to the
small sample bias of the nonparametric estimator based on the distance between
the mode and the mean of its asymptotic distribution.
",0,0,0,1,0,0
273,Convergence of the Forward-Backward Algorithm: Beyond the Worst Case with the Help of Geometry,"  We provide a comprehensive study of the convergence of forward-backward
algorithm under suitable geometric conditions leading to fast rates. We present
several new results and collect in a unified view a variety of results
scattered in the literature, often providing simplified proofs. Novel
contributions include the analysis of infinite dimensional convex minimization
problems, allowing the case where minimizers might not exist. Further, we
analyze the relation between different geometric conditions, and discuss novel
connections with a priori conditions in linear inverse problems, including
source conditions, restricted isometry properties and partial smoothness.
",0,0,1,1,0,0
6655,Dimensional Analysis in Economics: A Study of the Neoclassical Economic Growth Model,"  The fundamental purpose of the present research article is to introduce the
basic principles of Dimensional Analysis in the context of the neoclassical
economic theory, in order to apply such principles to the fundamental relations
that underlay most models of economic growth. In particular, basic instruments
from Dimensional Analysis are used to evaluate the analytical consistency of
the Neoclassical economic growth model. The analysis shows that an adjustment
to the model is required in such a way that the principle of dimensional
homogeneity is satisfied.
",0,0,0,0,0,1
425,Analytical solutions for the radial Scarf II potential,"  The real Scarf II potential is discussed as a radial problem. This potential
has been studied extensively as a one-dimensional problem, and now these
results are used to construct its bound and resonance solutions for $l=0$ by
setting the origin at some arbitrary value of the coordinate. The solutions
with appropriate boundary conditions are composed as the linear combination of
the two independent solutions of the Schrödinger equation. The asymptotic
expression of these solutions is used to construct the $S_0(k)$ s-wave
$S$-matrix, the poles of which supply the $k$ values corresponding to the
bound, resonance and anti-bound solutions. The location of the discrete energy
eigenvalues is analyzed, and the relation of the solutions of the radial and
one-dimensional Scarf II potentials is discussed. It is shown that the
generalized Woods--Saxon potential can be generated from the Rosen--Morse II
potential in the same way as the radial Scarf II potential is obtained from its
one-dimensional correspondent. Based on this analogy, possible applications are
also pointed out.
",0,1,0,0,0,0
359,A one-dimensional model for water desalination by flow-through electrode capacitive deionization,"  Capacitive deionization (CDI) is a fast-emerging water desalination
technology in which a small cell voltage of ~1 V across porous carbon
electrodes removes salt from feedwaters via electrosorption. In flow-through
electrode (FTE) CDI cell architecture, feedwater is pumped through macropores
or laser perforated channels in porous electrodes, enabling highly compact
cells with parallel flow and electric field, as well as rapid salt removal. We
here present a one-dimensional model describing water desalination by FTE CDI,
and a comparison to data from a custom-built experimental cell. The model
employs simple cell boundary conditions derived via scaling arguments. We show
good model-to-data fits with reasonable values for fitting parameters such as
the Stern layer capacitance, micropore volume, and attraction energy. Thus, we
demonstrate that from an engineering modeling perspective, an FTE CDI cell may
be described with simpler one-dimensional models, unlike more typical
flow-between electrodes architecture where 2D models are required.
",1,1,0,0,0,0
350,Social media mining for identification and exploration of health-related information from pregnant women,"  Widespread use of social media has led to the generation of substantial
amounts of information about individuals, including health-related information.
Social media provides the opportunity to study health-related information about
selected population groups who may be of interest for a particular study. In
this paper, we explore the possibility of utilizing social media to perform
targeted data collection and analysis from a particular population group --
pregnant women. We hypothesize that we can use social media to identify cohorts
of pregnant women and follow them over time to analyze crucial health-related
information. To identify potentially pregnant women, we employ simple
rule-based searches that attempt to detect pregnancy announcements with
moderate precision. To further filter out false positives and noise, we employ
a supervised classifier using a small number of hand-annotated data. We then
collect their posts over time to create longitudinal health timelines and
attempt to divide the timelines into different pregnancy trimesters. Finally,
we assess the usefulness of the timelines by performing a preliminary analysis
to estimate drug intake patterns of our cohort at different trimesters. Our
rule-based cohort identification technique collected 53,820 users over thirty
months from Twitter. Our pregnancy announcement classification technique
achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user
timelines. Analysis of the timelines revealed that pertinent health-related
information, such as drug-intake and adverse reactions can be mined from the
data. Our approach to using user timelines in this fashion has produced very
encouraging results and can be employed for other important tasks where
cohorts, for which health-related information may not be available from other
sources, are required to be followed over time to derive population-based
estimates.
",1,0,0,0,0,0
2700,Are Bitcoin Bubbles Predictable? Combining a Generalized Metcalfe's Law and the LPPLS Model,"  We develop a strong diagnostic for bubbles and crashes in bitcoin, by
analyzing the coincidence (and its absence) of fundamental and technical
indicators. Using a generalized Metcalfe's law based on network properties, a
fundamental value is quantified and shown to be heavily exceeded, on at least
four occasions, by bubbles that grow and burst. In these bubbles, we detect a
universal super-exponential unsustainable growth. We model this universal
pattern with the Log-Periodic Power Law Singularity (LPPLS) model, which
parsimoniously captures diverse positive feedback phenomena, such as herding
and imitation. The LPPLS model is shown to provide an ex-ante warning of market
instabilities, quantifying a high crash hazard and probabilistic bracket of the
crash time consistent with the actual corrections; although, as always, the
precise time and trigger (which straw breaks the camel's back) being exogenous
and unpredictable. Looking forward, our analysis identifies a substantial but
not unprecedented overvaluation in the price of bitcoin, suggesting many months
of volatile sideways bitcoin prices ahead (from the time of writing, March
2018).
",0,0,0,0,0,1
418,Adversarial Attacks on Neural Network Policies,"  Machine learning classifiers are known to be vulnerable to inputs maliciously
constructed by adversaries to force misclassification. Such adversarial
examples have been extensively studied in the context of computer vision
applications. In this work, we show adversarial attacks are also effective when
targeting neural network policies in reinforcement learning. Specifically, we
show existing adversarial example crafting techniques can be used to
significantly degrade test-time performance of trained policies. Our threat
model considers adversaries capable of introducing small perturbations to the
raw input of the policy. We characterize the degree of vulnerability across
tasks and training algorithms, for a subclass of adversarial-example attacks in
white-box and black-box settings. Regardless of the learned task or training
algorithm, we observe a significant drop in performance, even with small
adversarial perturbations that do not interfere with human perception. Videos
are available at this http URL.
",1,0,0,1,0,0
229,Scholars on Twitter: who and how many are they?,"  In this paper we present a novel methodology for identifying scholars with a
Twitter account. By combining bibliometric data from Web of Science and Twitter
users identified by Altmetric.com we have obtained the largest set of
individual scholars matched with Twitter users made so far. Our methodology
consists of a combination of matching algorithms, considering different
linguistic elements of both author names and Twitter names; followed by a
rule-based scoring system that weights the common occurrence of several
elements related with the names, individual elements and activities of both
Twitter users and scholars matched. Our results indicate that about 2% of the
overall population of scholars in the Web of Science is active on Twitter. By
domain we find a strong presence of researchers from the Social Sciences and
the Humanities. Natural Sciences is the domain with the lowest level of
scholars on Twitter. Researchers on Twitter also tend to be younger than those
that are not on Twitter. As this is a bibliometric-based approach, it is
important to highlight the reliance of the method on the number of publications
produced and tweeted by the scholars, thus the share of scholars on Twitter
ranges between 1% and 5% depending on their level of productivity. Further
research is suggested in order to improve and expand the methodology.
",1,0,0,0,0,0
16145,Optimal continuous-time ALM for insurers: a martingale approach,"  We study a continuous-time asset-allocation problem for a firm in the
insurance industry that backs up the liabilities raised by the insurance
contracts with the underwriting profits and the income resulting from investing
in the financial market. Using the martingale approach and convex duality
techniques we characterize strategies that maximize expected utility from
consumption and final wealth under CRRA preferences. We present numerical
results for some distributions of claims/liabilities with policy limits.
",0,0,0,0,0,1
1592,Variable Prioritization in Nonlinear Black Box Methods: A Genetic Association Case Study,"  The central aim in this paper is to address variable selection questions in
nonlinear and nonparametric regression. Motivated by statistical genetics,
where nonlinear interactions are of particular interest, we introduce a novel
and interpretable way to summarize the relative importance of predictor
variables. Methodologically, we develop the ""RelATive cEntrality"" (RATE)
measure to prioritize candidate genetic variants that are not just marginally
important, but whose associations also stem from significant covarying
relationships with other variants in the data. We illustrate RATE through
Bayesian Gaussian process regression, but the methodological innovations apply
to other ""black box"" methods. It is known that nonlinear models often exhibit
greater predictive accuracy than linear models, particularly for phenotypes
generated by complex genetic architectures. With detailed simulations and two
real data association mapping studies, we show that applying RATE enables an
explanation for this improved performance.
",0,0,0,1,1,0
851,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"  Convolutional Neural Networks (CNNs) are commonly thought to recognise
objects by learning increasingly complex representations of object shapes. Some
recent studies suggest a more important role of image textures. We here put
these conflicting hypotheses to a quantitative test by evaluating CNNs and
human observers on images with a texture-shape cue conflict. We show that
ImageNet-trained CNNs are strongly biased towards recognising textures rather
than shapes, which is in stark contrast to human behavioural evidence and
reveals fundamentally different classification strategies. We then demonstrate
that the same standard architecture (ResNet-50) that learns a texture-based
representation on ImageNet is able to learn a shape-based representation
instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet.
This provides a much better fit for human behavioural performance in our
well-controlled psychophysical lab setting (nine experiments totalling 48,560
psychophysical trials across 97 observers) and comes with a number of
unexpected emergent benefits such as improved object detection performance and
previously unseen robustness towards a wide range of image distortions,
highlighting advantages of a shape-based representation.
",0,0,0,0,1,0
6511,Doing good vs. avoiding bad in prosocial choice: A refined test and extension of the morality preference hypothesis,"  Prosociality is fundamental to human social life, and, accordingly, much
research has attempted to explain human prosocial behavior. Capraro and Rand
(Judgment and Decision Making, 13, 99-111, 2018) recently provided experimental
evidence that prosociality in anonymous, one-shot interactions (such as
Prisoner's Dilemma and Dictator Game experiments) is not driven by
outcome-based social preferences - as classically assumed - but by a
generalized morality preference for ""doing the right thing"". Here we argue that
the key experiments reported in Capraro and Rand (2018) comprise prominent
methodological confounds and open questions that bear on influential
psychological theory. Specifically, their design confounds: (i) preferences for
efficiency with self-interest; and (ii) preferences for action with preferences
for morality. Furthermore, their design fails to dissociate the preference to
do ""good"" from the preference to avoid doing ""bad"". We thus designed and
conducted a preregistered, refined and extended test of the morality preference
hypothesis (N=801). Consistent with this hypothesis, our findings indicate that
prosociality in the anonymous, one-shot Dictator Game is driven by preferences
for doing the morally right thing. Inconsistent with influential psychological
theory, however, our results suggest the preference to do ""good"" was as potent
as the preference to avoid doing ""bad"" in this case.
",0,0,0,0,1,0
7805,Conditional Optimal Stopping: A Time-Inconsistent Optimization,"  Inspired by recent work of P.-L. Lions on conditional optimal control, we
introduce a problem of optimal stopping under bounded rationality: the
objective is the expected payoff at the time of stopping, conditioned on
another event. For instance, an agent may care only about states where she is
still alive at the time of stopping, or a company may condition on not being
bankrupt. We observe that conditional optimization is time-inconsistent due to
the dynamic change of the conditioning probability and develop an equilibrium
approach in the spirit of R. H. Strotz' work for sophisticated agents in
discrete time. Equilibria are found to be essentially unique in the case of a
finite time horizon whereas an infinite horizon gives rise to non-uniqueness
and other interesting phenomena. We also introduce a theory which generalizes
the classical Snell envelope approach for optimal stopping by considering a
pair of processes with Snell-type properties.
",0,0,0,0,0,1
215,Joint Power and Admission Control based on Channel Distribution Information: A Novel Two-Timescale Approach,"  In this letter, we consider the joint power and admission control (JPAC)
problem by assuming that only the channel distribution information (CDI) is
available. Under this assumption, we formulate a new chance (probabilistic)
constrained JPAC problem, where the signal to interference plus noise ratio
(SINR) outage probability of the supported links is enforced to be not greater
than a prespecified tolerance. To efficiently deal with the chance SINR
constraint, we employ the sample approximation method to convert them into
finitely many linear constraints. Then, we propose a convex approximation based
deflation algorithm for solving the sample approximation JPAC problem. Compared
to the existing works, this letter proposes a novel two-timescale JPAC
approach, where admission control is performed by the proposed deflation
algorithm based on the CDI in a large timescale and transmission power is
adapted instantly with fast fadings in a small timescale. The effectiveness of
the proposed algorithm is illustrated by simulations.
",1,0,1,0,0,0
327,Mathematics of Topological Quantum Computing,"  In topological quantum computing, information is encoded in ""knotted"" quantum
states of topological phases of matter, thus being locked into topology to
prevent decay. Topological precision has been confirmed in quantum Hall liquids
by experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum
memory. In this survey, we discuss the conceptual development of this
interdisciplinary field at the juncture of mathematics, physics and computer
science. Our focus is on computing and physical motivations, basic mathematical
notions and results, open problems and future directions related to and/or
inspired by topological quantum computing.
",0,1,1,0,0,0
666,Criteria for the Application of Double Exponential Transformation,"  The double exponential formula was introduced for calculating definite
integrals with singular point oscillation functions and Fourier-integrals. The
double exponential transformation is not only useful for numerical computations
but it is also used in different methods of Sinc theory. In this paper we use
double exponential transformation for calculating particular improper
integrals. By improving integral estimates having singular final points. By
comparison between double exponential transformations and single exponential
transformations it is proved that the error margin of double exponential
transformations is smaller. Finally Fourier-integral and double exponential
transformations are discussed.
",0,0,1,0,0,0
258,Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems,"  Security, privacy, and fairness have become critical in the era of data
science and machine learning. More and more we see that achieving universally
secure, private, and fair systems is practically impossible. We have seen for
example how generative adversarial networks can be used to learn about the
expected private training data; how the exploitation of additional data can
reveal private information in the original one; and how what looks like
unrelated features can teach us about each other. Confronted with this
challenge, in this paper we open a new line of research, where the security,
privacy, and fairness is learned and used in a closed environment. The goal is
to ensure that a given entity (e.g., the company or the government), trusted to
infer certain information with our data, is blocked from inferring protected
information from it. For example, a hospital might be allowed to produce
diagnosis on the patient (the positive task), without being able to infer the
gender of the subject (negative task). Similarly, a company can guarantee that
internally it is not using the provided data for any undesired task, an
important goal that is not contradicting the virtually impossible challenge of
blocking everybody from the undesired task. We design a system that learns to
succeed on the positive task while simultaneously fail at the negative one, and
illustrate this with challenging cases where the positive task is actually
harder than the negative one being blocked. Fairness, to the information in the
negative task, is often automatically obtained as a result of this proposed
approach. The particular framework and examples open the door to security,
privacy, and fairness in very important closed scenarios, ranging from private
data accumulation companies like social networks to law-enforcement and
hospitals.
",1,0,0,1,0,0
722,"Graph Theoretical Models of Closed n-Dimensional Manifolds: Digital Models of a Moebius Strip, a Torus, a Projective Plane a Klein Bottle and n-Dimensional Spheres","  In this paper, we show how to construct graph theoretical models of
n-dimensional continuous objects and manifolds. These models retain topological
properties of their continuous counterparts. An LCL collection of n-cells in
Euclidean space is introduced and investigated. If an LCL collection of n-cells
is a cover of a continuous n-dimensional manifold then the intersection graph
of this cover is a digital closed n-dimensional manifold with the same topology
as its continuous counterpart. As an example, we prove that the digital model
of a continuous n-dimensional sphere is a digital n-sphere with at least 2n+2
points, the digital model of a continuous projective plane is a digital
projective plane with at least eleven points, the digital model of a continuous
Klein bottle is the digital Klein bottle with at least sixteen points, the
digital model of a continuous torus is the digital torus with at least sixteen
points and the digital model of a continuous Moebius band is the digital
Moebius band with at least twelve points.
",1,0,1,0,0,0
292,Rheology of High-Capillary Number Flow in Porous Media,"  Immiscible fluids flowing at high capillary numbers in porous media may be
characterized by an effective viscosity. We demonstrate that the effective
viscosity is well described by the Lichtenecker-Rother equation. The exponent
$\alpha$ in this equation takes either the value 1 or 0.6 in two- and 0.5 in
three-dimensional systems depending on the pore geometry. Our arguments are
based on analytical and numerical methods.
",0,1,0,0,0,0
15925,Day-ahead electricity price forecasting with high-dimensional structures: Univariate vs. multivariate modeling frameworks,"  We conduct an extensive empirical study on short-term electricity price
forecasting (EPF) to address the long-standing question if the optimal model
structure for EPF is univariate or multivariate. We provide evidence that
despite a minor edge in predictive performance overall, the multivariate
modeling framework does not uniformly outperform the univariate one across all
12 considered datasets, seasons of the year or hours of the day, and at times
is outperformed by the latter. This is an indication that combining advanced
structures or the corresponding forecasts from both modeling approaches can
bring a further improvement in forecasting accuracy. We show that this indeed
can be the case, even for a simple averaging scheme involving only two models.
Finally, we also analyze variable selection for the best performing
high-dimensional lasso-type models, thus provide guidelines to structuring
better performing forecasting model designs.
",0,0,0,1,0,1
759,Distances and Isomorphism between Networks and the Stability of Network Invariants,"  We develop the theoretical foundations of a network distance that has
recently been applied to various subfields of topological data analysis, namely
persistent homology and hierarchical clustering. While this network distance
has previously appeared in the context of finite networks, we extend the
setting to that of compact networks. The main challenge in this new setting is
the lack of an easy notion of sampling from compact networks; we solve this
problem in the process of obtaining our results. The generality of our setting
means that we automatically establish results for exotic objects such as
directed metric spaces and Finsler manifolds. We identify readily computable
network invariants and establish their quantitative stability under this
network distance. We also discuss the computational complexity involved in
precisely computing this distance, and develop easily-computable lower bounds
by using the identified invariants. By constructing a wide range of explicit
examples, we show that these lower bounds are effective in distinguishing
between networks. Finally, we provide a simple algorithm that computes a lower
bound on the distance between two networks in polynomial time and illustrate
our metric and invariant constructions on a database of random networks and a
database of simulated hippocampal networks.
",1,0,1,0,0,0
341,Untangling Planar Curves,"  Any generic closed curve in the plane can be transformed into a simple closed
curve by a finite sequence of local transformations called homotopy moves. We
prove that simplifying a planar closed curve with $n$ self-crossings requires
$\Theta(n^{3/2})$ homotopy moves in the worst case. Our algorithm improves the
best previous upper bound $O(n^2)$, which is already implicit in the classical
work of Steinitz; the matching lower bound follows from the construction of
closed curves with large defect, a topological invariant of generic closed
curves introduced by Aicardi and Arnold. Our lower bound also implies that
$\Omega(n^{3/2})$ facial electrical transformations are required to reduce any
plane graph with treewidth $\Omega(\sqrt{n})$ to a single vertex, matching
known upper bounds for rectangular and cylindrical grid graphs. More generally,
we prove that transforming one immersion of $k$ circles with at most $n$
self-crossings into another requires $\Theta(n^{3/2} + nk + k^2)$ homotopy
moves in the worst case. Finally, we prove that transforming one
noncontractible closed curve to another on any orientable surface requires
$\Omega(n^2)$ homotopy moves in the worst case; this lower bound is tight if
the curve is homotopic to a simple closed curve.
",1,0,1,0,0,0
298,Adaptive Diffusion Processes of Time-Varying Local Information on Networks,"  This paper mainly discusses the diffusion on complex networks with
time-varying couplings. We propose a model to describe the adaptive diffusion
process of local topological and dynamical information, and find that the
Barabasi-Albert scale-free network (BA network) is beneficial to the diffusion
and leads nodes to arrive at a larger state value than other networks do. The
ability of diffusion for a node is related to its own degree. Specifically,
nodes with smaller degrees are more likely to change their states and reach
larger values, while those with larger degrees tend to stick to their original
states. We introduce state entropy to analyze the thermodynamic mechanism of
the diffusion process, and interestingly find that this kind of diffusion
process is a minimization process of state entropy. We use the inequality
constrained optimization method to reveal the restriction function of the
minimization and find that it has the same form as the Gibbs free energy. The
thermodynamical concept allows us to understand dynamical processes on complex
networks from a brand-new perspective. The result provides a convenient means
of optimizing relevant dynamical processes on practical circuits as well as
related complex systems.
",1,0,0,0,0,0
909,Forecasting the Impact of Stellar Activity on Transiting Exoplanet Spectra,"  Exoplanet host star activity, in the form of unocculted star spots or
faculae, alters the observed transmission and emission spectra of the
exoplanet. This effect can be exacerbated when combining data from different
epochs if the stellar photosphere varies between observations due to activity.
redHere we present a method to characterize and correct for relative changes
due to stellar activity by exploiting multi-epoch ($\ge$2 visits/transits)
observations to place them in a consistent reference frame. Using measurements
from portions of the planet's orbit where negligible planet transmission or
emission can be assumed, we determine changes to the stellar spectral
amplitude. With the analytical methods described here, we predict the impact of
stellar variability on transit observations. Supplementing these forecasts with
Kepler-measured stellar variabilities for F-, G-, K-, and M-dwarfs, and
predicted transit precisions by JWST's NIRISS, NIRCam, and MIRI, we conclude
that stellar activity does not impact infrared transiting exoplanet
observations of most presently-known or predicted TESS targets by current or
near-future platforms, such as JWST.
",0,1,0,0,0,0
6570,Frictional Effects on RNA Folding: Speed Limit and Kramers Turnover,"  We investigated frictional effects on the folding rates of a human telomerase
hairpin (hTR HP) and H-type pseudoknot from the Beet Western Yellow Virus (BWYV
PK) using simulations of the Three Interaction Site (TIS) model for RNA. The
heat capacity from TIS model simulations, calculated using temperature replica
exchange simulations, reproduces nearly quantitatively the available
experimental data for the hTR HP. The corresponding results for BWYV PK serve
as predictions. We calculated the folding rates ($k_\mathrm{F}$) from more than
100 folding trajectories for each value of the solvent viscosity ($\eta$) at a
fixed salt concentration of 200 mM. By using the theoretical estimate
($\propto$$\sqrt{N}$ where $N$ is the number of nucleotides) for folding free
energy barrier, $k_\mathrm{F}$ data for both the RNAs are quantitatively fit
using one-dimensional Kramers' theory with two parameters specifying the
curvatures in the unfolded basin and the barrier top. In the high-friction
regime ($\eta\gtrsim10^{-5}\,\textrm{Pa\ensuremath{\cdot}s}$), for both HP and
PK, $k_\mathrm{F}$s decrease as $1/\eta$ whereas in the low friction regime,
$k_\mathrm{F}$ values increase as $\eta$ increases, leading to a maximum
folding rate at a moderate viscosity
($\sim10^{-6}\,\textrm{Pa\ensuremath{\cdot}s}$), which is the Kramers turnover.
From the fits, we find that the speed limit to RNA folding at water viscosity
is between 1 and 4 $\mathrm{\mu s}$, which is in accord with our previous
theoretical prediction as well as results from several single molecule
experiments. Both the RNA constructs fold by parallel pathways. Surprisingly,
we find that the flux through the pathways could be altered by changing solvent
viscosity, a prediction that is more easily testable in RNA than in proteins.
",0,0,0,0,1,0
818,Complex Networks: from Classical to Quantum,"  Recent progress in applying complex network theory to problems faced in
quantum information and computation has resulted in a beneficial crossover
between two fields. Complex network methods have successfully been used to
characterize quantum walk and transport models, entangled communication
networks, graph-theoretic models of emergent space-time and in detecting
mesoscale structure in quantum systems. Information physics is setting the
stage for a theory of complex and networked systems with quantum
information-inspired methods appearing in complex network science, including
information-theoretic distance and correlation measures for network
characterization. Novel quantum induced effects have been predicted in random
graphs---where edges represent entangled links---and quantum computer
algorithms have recently been proposed to offer super-polynomial enhancement
for several network and graph theoretic problems. Here we review the results at
the cutting edge, pinpointing the similarities and reconciling the differences
found in the series of results at the intersection of these two fields.
",1,1,0,0,0,0
730,Algebraic models of the Euclidean plane,"  We introduce a new invariant, the real (logarithmic)-Kodaira dimension, that
allows to distinguish smooth real algebraic surfaces up to birational
diffeomorphism. As an application, we construct infinite families of smooth
rational real algebraic surfaces with trivial homology groups, whose real loci
are diffeomorphic to $\mathbb{R}^2$, but which are pairwise not birationally
diffeomorphic. There are thus infinitely many non-trivial models of the
euclidean plane, contrary to the compact case.
",0,0,1,0,0,0
2531,Crawling migration under chemical signalling: a stochastic particle model,"  Cell migration is a fundamental process involved in physiological phenomena
such as the immune response and morphogenesis, but also in pathological
processes, such as the development of tumor metastasis. These functions are
effectively ensured because cells are active systems that adapt to their
environment. In this work, we consider a migrating cell as an active particle,
where its intracellular activity is responsible for motion. Such system was
already modeled in a previous model where the protrusion activity of the cell
was described by a stochastic Markovian jump process. The model was proven able
to capture the diversity in observed trajectories. Here, we add a description
of the effect of an external chemical attractive signal on the protrusion
dynamics, that may vary in time. We show that the resulting stochastic model is
a well-posed non-homogeneous Markovian process, and provide cell trajectories
in different settings, illustrating the effects of the signal on long-term
trajectories.
",0,0,0,0,1,0
908,Phase diagrams of Bose-Hubbard model and antiferromagnetic spin-1/2 models on a honeycomb lattice,"  Motivated by the recent experimental realization of the Haldane model by
ultracold fermions in an optical lattice, we investigate phase diagrams of the
hard-core Bose-Hubbard model on a honeycomb lattice. This model is closely
related with a spin-1/2 antiferromagnetic (AF) quantum spin model.
Nearest-neighbor (NN) hopping amplitude is positive and it prefers an AF
configurations of phases of Bose-Einstein condensates. On the other hand, an
amplitude of the next-NN hopping depends on an angle variable as in the Haldane
model. Phase diagrams are obtained by means of an extended path-integral
Monte-Carlo simulations. Besides the AF state, a 120$^o$-order state, there
appear other phases including a Bose metal in which no long-range orders exist.
",0,1,0,0,0,0
577,Far-field theory for trajectories of magnetic ellipsoids in rectangular and circular channels,"  We report a method to control the positions of ellipsoidal magnets in flowing
channels of rectangular or circular cross section at low Reynolds number.A
static uniform magnetic field is used to pin the particle orientation, and the
particles move with translational drift velocities resulting from hydrodynamic
interactions with the channel walls which can be described using Blake's image
tensor.Building on his insights, we are able to present a far-field theory
predicting the particle motion in rectangular channels, and validate the
accuracy of the theory by comparing to numerical solutions using the boundary
element method.We find that, by changing the direction of the applied magnetic
field, the motion can be controlled so that particles move either to a curved
focusing region or to the channel walls.We also use simulations to show that
the particles are focused to a single line in a circular channel.Our results
suggest ways to focus and segregate magnetic particles in lab-on-a-chip
devices.
",0,1,0,0,0,0
622,Uncoupled isotonic regression via minimum Wasserstein deconvolution,"  Isotonic regression is a standard problem in shape-constrained estimation
where the goal is to estimate an unknown nondecreasing regression function $f$
from independent pairs $(x_i, y_i)$ where $\mathbb{E}[y_i]=f(x_i), i=1, \ldots
n$. While this problem is well understood both statistically and
computationally, much less is known about its uncoupled counterpart where one
is given only the unordered sets $\{x_1, \ldots, x_n\}$ and $\{y_1, \ldots,
y_n\}$. In this work, we leverage tools from optimal transport theory to derive
minimax rates under weak moments conditions on $y_i$ and to give an efficient
algorithm achieving optimal rates. Both upper and lower bounds employ
moment-matching arguments that are also pertinent to learning mixtures of
distributions and deconvolution.
",0,0,0,1,0,0
642,Large Magellanic Cloud Near-Infrared Synoptic Survey. V. Period-Luminosity Relations of Miras,"  We study the near-infrared properties of 690 Mira candidates in the central
region of the Large Magellanic Cloud, based on time-series observations at
JHKs. We use densely-sampled I-band observations from the OGLE project to
generate template light curves in the near infrared and derive robust mean
magnitudes at those wavelengths. We obtain near-infrared Period-Luminosity
relations for Oxygen-rich Miras with a scatter as low as 0.12 mag at Ks. We
study the Period-Luminosity-Color relations and the color excesses of
Carbon-rich Miras, which show evidence for a substantially different reddening
law.
",0,0,0,1,0,0
846,Value added or misattributed? A multi-institution study on the educational benefit of labs for reinforcing physics content,"  Instructional labs are widely seen as a unique, albeit expensive, way to
teach scientific content. We measured the effectiveness of introductory lab
courses at achieving this educational goal across nine different lab courses at
three very different institutions. These institutions and courses encompassed a
broad range of student populations and instructional styles. The nine courses
studied had two key things in common: the labs aimed to reinforce the content
presented in lectures, and the labs were optional. By comparing the performance
of students who did and did not take the labs (with careful normalization for
selection effects), we found universally and precisely no added value to
learning from taking the labs as measured by course exam performance. This work
should motivate institutions and departments to reexamine the goals and conduct
of their lab courses, given their resource-intensive nature. We show why these
results make sense when looking at the comparative mental processes of students
involved in research and instructional labs, and offer alternative goals and
instructional approaches that would make lab courses more educationally
valuable.
",0,1,0,0,0,0
4735,Effects of atrial fibrillation on the arterial fluid dynamics: a modelling perspective,"  Atrial fibrillation (AF) is the most common form of arrhythmia with
accelerated and irregular heart rate (HR), leading to both heart failure and
stroke and being responsible for an increase in cardiovascular morbidity and
mortality. In spite of its importance, the direct effects of AF on the arterial
hemodynamic patterns are not completely known to date. Based on a multiscale
modelling approach, the proposed work investigates the effects of AF on the
local arterial fluid dynamics. AF and normal sinus rhythm (NSR) conditions are
simulated extracting 2000 $\mathrm{RR}$ heartbeats and comparing the most
relevant cardiac and vascular parameters at the same HR (75 bpm). Present
outcomes evidence that the arterial system is not able to completely absorb the
AF-induced variability, which can be even amplified towards the peripheral
circulation. AF is also able to locally alter the wave dynamics, by modifying
the interplay between forward and backward signals. The sole heart rhythm
variation (i.e., from NSR to AF) promotes an alteration of the regular dynamics
at the arterial level which, in terms of pressure and peripheral perfusion,
suggests a modification of the physiological phenomena ruled by periodicity
(e.g., regular organ perfusion)and a possible vascular dysfunction due to the
prolonged exposure to irregular and extreme values. The present study
represents a first modeling approach to characterize the variability of
arterial hemodynamics in presence of AF, which surely deserves further clinical
investigation.
",0,0,0,0,1,0
627,Predicting wind pressures around circular cylinders using machine learning techniques,"  Numerous studies have been carried out to measure wind pressures around
circular cylinders since the early 20th century due to its engineering
significance. Consequently, a large amount of wind pressure data sets have
accumulated, which presents an excellent opportunity for using machine learning
(ML) techniques to train models to predict wind pressures around circular
cylinders. Wind pressures around smooth circular cylinders are a function of
mainly the Reynolds number (Re), turbulence intensity (Ti) of the incident
wind, and circumferential angle of the cylinder. Considering these three
parameters as the inputs, this study trained two ML models to predict mean and
fluctuating pressures respectively. Three machine learning algorithms including
decision tree regressor, random forest, and gradient boosting regression trees
(GBRT) were tested. The GBRT models exhibited the best performance for
predicting both mean and fluctuating pressures, and they are capable of making
accurate predictions for Re ranging from 10^4 to 10^6 and Ti ranging from 0% to
15%. It is believed that the GBRT models provide very efficient and economical
alternative to traditional wind tunnel tests and computational fluid dynamic
simulations for determining wind pressures around smooth circular cylinders
within the studied Re and Ti range.
",1,0,0,1,0,0
504,DeepSaucer: Unified Environment for Verifying Deep Neural Networks,"  In recent years, a number of methods for verifying DNNs have been developed.
Because the approaches of the methods differ and have their own limitations, we
think that a number of verification methods should be applied to a developed
DNN. To apply a number of methods to the DNN, it is necessary to translate
either the implementation of the DNN or the verification method so that one
runs in the same environment as the other. Since those translations are
time-consuming, a utility tool, named DeepSaucer, which helps to retain and
reuse implementations of DNNs, verification methods, and their environments, is
proposed. In DeepSaucer, code snippets of loading DNNs, running verification
methods, and creating their environments are retained and reused as software
assets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer
is confirmed by implementing it on the basis of Anaconda, which provides
virtual environment for loading a DNN and running a verification method. In
addition, the effectiveness of DeepSaucer is demonstrated by usecase examples.
",1,0,0,0,0,0
9357,What are the most important factors that influence the changes in London Real Estate Prices? How to quantify them?,"  In recent years, real estate industry has captured government and public
attention around the world. The factors influencing the prices of real estate
are diversified and complex. However, due to the limitations and one-sidedness
of their respective views, they did not provide enough theoretical basis for
the fluctuation of house price and its influential factors. The purpose of this
paper is to build a housing price model to make the scientific and objective
analysis of London's real estate market trends from the year 1996 to 2016 and
proposes some countermeasures to reasonably control house prices. Specifically,
the paper analyzes eight factors which affect the house prices from two
aspects: housing supply and demand and find out the factor which is of vital
importance to the increase of housing price per square meter. The problem of a
high level of multicollinearity between them is solved by using principal
components analysis.
",0,0,0,1,0,1
842,A Practical Bandit Method with Advantages in Neural Network Tuning,"  Stochastic bandit algorithms can be used for challenging non-convex
optimization problems. Hyperparameter tuning of neural networks is particularly
challenging, necessitating new approaches. To this end, we present a method
that adaptively partitions the combined space of hyperparameters, context, and
training resources (e.g., total number of training iterations). By adaptively
partitioning the space, the algorithm is able to focus on the portions of the
hyperparameter search space that are most relevant in a practical way. By
including the resources in the combined space, the method tends to use fewer
training resources overall. Our experiments show that this method can surpass
state-of-the-art methods in tuning neural networks on benchmark datasets. In
some cases, our implementations can achieve the same levels of accuracy on
benchmark datasets as existing state-of-the-art approaches while saving over
50% of our computational resources (e.g. time, training iterations).
",1,0,0,1,0,0
838,Multi-view Low-rank Sparse Subspace Clustering,"  Most existing approaches address multi-view subspace clustering problem by
constructing the affinity matrix on each view separately and afterwards propose
how to extend spectral clustering algorithm to handle multi-view data. This
paper presents an approach to multi-view subspace clustering that learns a
joint subspace representation by constructing affinity matrix shared among all
views. Relying on the importance of both low-rank and sparsity constraints in
the construction of the affinity matrix, we introduce the objective that
balances between the agreement across different views, while at the same time
encourages sparsity and low-rankness of the solution. Related low-rank and
sparsity constrained optimization problem is for each view solved using the
alternating direction method of multipliers. Furthermore, we extend our
approach to cluster data drawn from nonlinear subspaces by solving the
corresponding problem in a reproducing kernel Hilbert space. The proposed
algorithm outperforms state-of-the-art multi-view subspace clustering
algorithms on one synthetic and four real-world datasets.
",1,0,0,1,0,0
5783,Fine-scale population structure analysis in Armadillidium vulgare (Isopoda: Oniscidea) reveals strong female philopatry,"  In the last decades, dispersal studies have benefitted from the use of
molecular markers for detecting patterns differing between categories of
individuals, and have highlighted sex-biased dispersal in several species. To
explain this phenomenon, sex-related handicaps such as parental care have been
recently proposed as a hypothesis. Herein we tested this hypothesis in
Armadillidium vulgare, a terrestrial isopod in which females bear the totality
of the high parental care costs. We performed a fine-scale analysis of
sex-specific dispersal patterns, using males and females originating from five
sampling points located within 70 meters of each other. Based on microsatellite
markers and both F-statistics and spatial autocorrelation analyses, our results
revealed that while males did not present a significant structure at this
geographic scale, females were significantly more similar to each other when
they were collected in the same sampling point. These results support the
sex-handicap hypothesis, and we suggest that widening dispersal studies to
other isopods or crustaceans, displaying varying levels of parental care but
differing in their ecology or mating system, might shed light on the processes
underlying the evolution of sex-biased dispersal.
",0,0,0,0,1,0
362,Making Sense of Physics through Stories: High School Students Narratives about Electric Charges and Interactions,"  Educational research has shown that narratives are useful tools that can help
young students make sense of scientific phenomena. Based on previous research,
I argue that narratives can also become tools for high school students to make
sense of concepts such as the electric field. In this paper I examine high
school students visual and oral narratives in which they describe the
interaction among electric charges as if they were characters of a cartoon
series. The study investigates: given the prompt to produce narratives for
electrostatic phenomena during a classroom activity prior to receiving formal
instruction, (1) what ideas of electrostatics do students attend to in their
narratives?; (2) what role do students narratives play in their understanding
of electrostatics? The participants were a group of high school students
engaged in an open-ended classroom activity prior to receiving formal
instruction about electrostatics. During the activity, the group was asked to
draw comic strips for electric charges. In addition to individual work,
students shared their work within small groups as well as with the whole group.
Post activity, six students from a small group were interviewed individually
about their work. In this paper I present two cases in which students produced
narratives to express their ideas about electrostatics in different ways. In
each case, I present student work for the comic strip activity (visual
narratives), their oral descriptions of their work (oral narratives) during the
interview and/or to their peers during class, and the their ideas of the
electric interactions expressed through their narratives.
",0,1,0,0,0,0
4673,Entropy Production Rate is Maximized in Non-Contractile Actomyosin,"  The actin cytoskeleton is an active semi-flexible polymer network whose
non-equilibrium properties coordinate both stable and contractile behaviors to
maintain or change cell shape. While myosin motors drive the actin cytoskeleton
out-of-equilibrium, the role of myosin-driven active stresses in the
accumulation and dissipation of mechanical energy is unclear. To investigate
this, we synthesize an actomyosin material in vitro whose active stress content
can tune the network from stable to contractile. Each increment in activity
determines a characteristic spectrum of actin filament fluctuations which is
used to calculate the total mechanical work and the production of entropy in
the material. We find that the balance of work and entropy does not increase
monotonically and, surprisingly, the entropy production rate is maximized in
the non-contractile, stable state. Our study provides evidence that the origins
of system entropy production and activity-dependent dissipation arise from
disorder in the molecular interactions between actin and myosin
",0,0,0,0,1,0
888,Multiple Topological Electronic Phases in Superconductor MoC,"  The search for a superconductor with non-s-wave pairing is important not only
for understanding unconventional mechanisms of superconductivity but also for
finding new types of quasiparticles such as Majorana bound states. Materials
with both topological band structure and superconductivity are promising
candidates as $p+ip$ superconducting states can be generated through pairing
the spin-polarized topological surface states. In this work, the electronic and
phonon properties of the superconductor molybdenum carbide (MoC) are studied
with first-principles methods. Our calculations show that nontrivial band
topology and superconductivity coexist in both structural phases of MoC,
namely, the cubic $\alpha$ and hexagonal $\gamma$ phases. The $\alpha$ phase is
a strong topological insulator and the $\gamma$ phase is a topological nodal
line semimetal with drumhead surface states. In addition, hole doping can
stabilize the crystal structure of the $\alpha$ phase and elevate the
transition temperature in the $\gamma$ phase. Therefore, MoC in different
structural forms can be a practical material platform for studying topological
superconductivity and elusive Majorana fermions.
",0,1,0,0,0,0
228,A Practical Approach for Successive Omniscience,"  The system that we study in this paper contains a set of users that observe a
discrete memoryless multiple source and communicate via noise-free channels
with the aim of attaining omniscience, the state that all users recover the
entire multiple source. We adopt the concept of successive omniscience (SO),
i.e., letting the local omniscience in some user subset be attained before the
global omniscience in the entire system, and consider the problem of how to
efficiently attain omniscience in a successive manner. Based on the existing
results on SO, we propose a CompSetSO algorithm for determining a complimentary
set, a user subset in which the local omniscience can be attained first without
increasing the sum-rate, the total number of communications, for the global
omniscience. We also derive a sufficient condition for a user subset to be
complimentary so that running the CompSetSO algorithm only requires a lower
bound, instead of the exact value, of the minimum sum-rate for attaining global
omniscience. The CompSetSO algorithm returns a complimentary user subset in
polynomial time. We show by example how to recursively apply the CompSetSO
algorithm so that the global omniscience can be attained by multi-stages of SO.
",1,0,0,0,0,0
954,Hölder continuous solutions of the Monge-Ampère equation on compact Hermitian manifolds,"  We show that a positive Borel measure of positive finite total mass, on
compact Hermitian manifolds, admits a Holder continuous quasi-plurisubharmonic
solution to the Monge-Ampere equation if and only if it is dominated locally by
Monge-Ampere measures of Holder continuous plurisubharmonic functions.
",0,0,1,0,0,0
617,Holography and thermalization in optical pump-probe spectroscopy,"  Using holography, we model experiments in which a 2+1D strange metal is
pumped by a laser pulse into a highly excited state, after which the time
evolution of the optical conductivity is probed. We consider a finite-density
state with mildly broken translation invariance and excite it by oscillating
electric field pulses. At zero density, the optical conductivity would assume
its thermalized value immediately after the pumping has ended. At finite
density, pulses with significant DC components give rise to slow exponential
relaxation, governed by a vector quasinormal mode. In contrast, for
high-frequency pulses the amplitude of the quasinormal mode is strongly
suppressed, so that the optical conductivity assumes its thermalized value
effectively instantaneously. This surprising prediction may provide a stimulus
for taking up the challenge to realize these experiments in the laboratory.
Such experiments would test a crucial open question faced by applied
holography: Are its predictions artefacts of the large $N$ limit or do they
enjoy sufficient UV independence to hold at least qualitatively in real-world
systems?
",0,1,0,0,0,0
698,Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access,"  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms
for uncoordinated spectrum access. The number of users is assumed to be unknown
to each user. A stochastic setting is first considered, where the rewards on a
channel are the same for each user. In contrast to prior work, it is assumed
that the number of users can possibly exceed the number of channels, and that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
in particular, even when the number of users is less than the number of
channels. Next, an adversarial multi-user MAB framework is considered, where
the rewards on the channels are user-dependent. It is assumed that the number
of users is less than the number of channels, and that the users receive zero
reward on collision. The proposed algorithm combines the Exp3.P algorithm
developed in prior work for single user adversarial bandits with a collision
resolution mechanism to achieve sub-linear regret. It is shown that if every
user employs the proposed algorithm, the system wide regret is of the order
$O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both
stochastic and adversarial scenarios are extended to the dynamic case where the
number of users in the system evolves over time and are shown to lead to
sub-linear regret.
",0,0,0,1,0,0
265,Automated Synthesis of Safe Digital Controllers for Sampled-Data Stochastic Nonlinear Systems,"  We present a new method for the automated synthesis of digital controllers
with formal safety guarantees for systems with nonlinear dynamics, noisy output
measurements, and stochastic disturbances. Our method derives digital
controllers such that the corresponding closed-loop system, modeled as a
sampled-data stochastic control system, satisfies a safety specification with
probability above a given threshold. The proposed synthesis method alternates
between two steps: generation of a candidate controller pc, and verification of
the candidate. pc is found by maximizing a Monte Carlo estimate of the safety
probability, and by using a non-validated ODE solver for simulating the system.
Such a candidate is therefore sub-optimal but can be generated very rapidly. To
rule out unstable candidate controllers, we prove and utilize Lyapunov's
indirect method for instability of sampled-data nonlinear systems. In the
subsequent verification step, we use a validated solver based on SMT
(Satisfiability Modulo Theories) to compute a numerically and statistically
valid confidence interval for the safety probability of pc. If the probability
so obtained is not above the threshold, we expand the search space for
candidates by increasing the controller degree. We evaluate our technique on
three case studies: an artificial pancreas model, a powertrain control model,
and a quadruple-tank process.
",1,0,0,0,0,0
358,A short-orbit spectrometer for low-energy pion detection in electroproduction experiments at MAMI,"  A new Short-Orbit Spectrometer (SOS) has been constructed and installed
within the experimental facility of the A1 collaboration at Mainz Microtron
(MAMI), with the goal to detect low-energy pions. It is equipped with a
Browne-Buechner magnet and a detector system consisting of two helium-ethane
based drift chambers and a scintillator telescope made of five layers. The
detector system allows detection of pions in the momentum range of 50 - 147
MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
be placed at a distance range of 54 - 66 cm from the target center. Two
collimators are available for the measurements, one having 1.8 msr aperture and
the other having 7 msr aperture. The Short-Orbit Spectrometer has been
successfully calibrated and used in coincidence measurements together with the
standard magnetic spectrometers of the A1 collaboration.
",0,1,0,0,0,0
716,The X-ray reflection spectrum of the radio-loud quasar 4C 74.26,"  The relativistic jets created by some active galactic nuclei are important
agents of AGN feedback. In spite of this, our understanding of what produces
these jets is still incomplete. X-ray observations, which can probe the
processes operating in the central regions in immediate vicinity of the
supermassive black hole, the presumed jet launching point, are potentially
particularly valuable in illuminating the jet formation process. Here, we
present the hard X-ray NuSTAR observations of the radio-loud quasar 4C 74.26 in
a joint analysis with quasi-simultaneous, soft X-ray Swift observations. Our
spectral analysis reveals a high-energy cut-off of 183$_{-35}^{+51}$ keV and
confirms the presence of ionized reflection in the source. From the average
spectrum we detect that the accretion disk is mildly recessed with an inner
radius of $R_\mathrm{in}=4-180\,R_\mathrm{g}$. However, no significant
evolution of the inner radius is seen during the three months covered by our
NuSTAR campaign. This lack of variation could mean that the jet formation in
this radio-loud quasar differs from what is observed in broad-line radio
galaxies.
",0,1,0,0,0,0
4303,A model for random fire induced tree-grass coexistence in savannas,"  Tree-grass coexistence in savanna ecosystems depends strongly on
environmental disturbances out of which crucial is fire. Most modeling attempts
in the literature lack stochastic approach to fire occurrences which is
essential to reflect their unpredictability. Existing models that actually
include stochasticity of fire are usually analyzed only numerically. We
introduce new minimalistic model of tree-grass coexistence where fires occur
according to stochastic process. We use the tools of linear semigroup theory to
provide more careful mathematical analysis of the model. Essentially we show
that there exists a unique stationary distribution of tree and grass biomasses.
",0,0,0,0,1,0
811,A Universal Marginalizer for Amortized Inference in Generative Models,"  We consider the problem of inference in a causal generative model where the
set of available observations differs between data instances. We show how
combining samples drawn from the graphical model with an appropriate masking
function makes it possible to train a single neural network to approximate all
the corresponding conditional marginal distributions and thus amortize the cost
of inference. We further demonstrate that the efficiency of importance sampling
may be improved by basing proposals on the output of the neural network. We
also outline how the same network can be used to generate samples from an
approximate joint posterior via a chain decomposition of the graph.
",1,0,0,1,0,0
736,Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field,"  Even- and odd-frequency superconductivity coexist due to broken time-reversal
symmetry under magnetic field. In order to describe this mixing, we extend the
linearized Eliashberg equation for the spin and charge fluctuation mechanism in
strongly correlated electron systems. We apply this extended Eliashberg
equation to the odd-frequency superconductivity on a quasi-one-dimensional
isosceles triangular lattice under in-plane magnetic field and examine the
effect of the even-frequency component.
",0,1,0,0,0,0
15651,Voting power of political parties in the Senate of Chile during the whole binomial system period: 1990-2017,"  The binomial system is an electoral system unique in the world. It was used
to elect the senators and deputies of Chile during 27 years, from the return of
democracy in 1990 until 2017. In this paper we study the real voting power of
the different political parties in the Senate of Chile during the whole
binomial period. We not only consider the different legislative periods, but
also any party changes between one period and the next. The real voting power
is measured by considering power indices from cooperative game theory, which
are based on the capability of the political parties to form winning
coalitions. With this approach, we can do an analysis that goes beyond the
simple count of parliamentary seats.
",0,0,0,0,0,1
964,Separation of time scales and direct computation of weights in deep neural networks,"  Artificial intelligence is revolutionizing our lives at an ever increasing
pace. At the heart of this revolution is the recent advancements in deep neural
networks (DNN), learning to perform sophisticated, high-level tasks. However,
training DNNs requires massive amounts of data and is very computationally
intensive. Gaining analytical understanding of the solutions found by DNNs can
help us devise more efficient training algorithms, replacing the commonly used
mthod of stochastic gradient descent (SGD). We analyze the dynamics of SGD and
show that, indeed, direct computation of the solutions is possible in many
cases. We show that a high performing setup used in DNNs introduces a
separation of time-scales in the training dynamics, allowing SGD to train
layers from the lowest (closest to input) to the highest. We then show that for
each layer, the distribution of solutions found by SGD can be estimated using a
class-based principal component analysis (PCA) of the layer's input. This
finding allows us to forgo SGD entirely and directly derive the DNN parameters
using this class-based PCA, which can be well estimated using significantly
less data than SGD. We implement these results on image datasets MNIST, CIFAR10
and CIFAR100 and find that, in fact, layers derived using our class-based PCA
perform comparable or superior to neural networks of the same size and
architecture trained using SGD. We also confirm that the class-based PCA often
converges using a fraction of the data required for SGD. Thus, using our method
training time can be reduced both by requiring less training data than SGD, and
by eliminating layers in the costly backpropagation step of the training.
",1,0,0,1,0,0
450,The Tu--Deng Conjecture holds almost surely,"  The Tu--Deng Conjecture is concerned with the sum of digits $w(n)$ of $n$ in
base~$2$ (the Hamming weight of the binary expansion of $n$) and states the
following: assume that $k$ is a positive integer and $1\leq t<2^k-1$. Then
\[\Bigl \lvert\Bigl\{(a,b)\in\bigl\{0,\ldots,2^k-2\bigr\}^2:a+b\equiv t\bmod
2^k-1, w(a)+w(b)<k\Bigr\}\Bigr \rvert\leq 2^{k-1}.\]
We prove that the Tu--Deng Conjecture holds almost surely in the following
sense: the proportion of $t\in[1,2^k-2]$ such that the above inequality holds
approaches $1$ as $k\rightarrow\infty$.
Moreover, we prove that the Tu--Deng Conjecture implies a conjecture due to
T.~W.~Cusick concerning the sum of digits of $n$ and $n+t$.
",1,0,1,0,0,0
582,Stability of casein micelles cross-linked with genipin: a physicochemical study as a function of pH,"  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their
stability against dissociating agents. In this paper, a comparative study of
stability between native CMs and CMs cross-linked with genipin (CMs-GP) as a
function of pH is described. Stability to temperature and ethanol were
investigated in the pH range 2.0-7.0. The size and the charge
($\zeta$-potential) of the particles were determined by dynamic light
scattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH
3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.
The isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability
against heat and ethanol was observed for CMs-GP at pH 2, where visible
coagulation was determined only after 800 s at 140 $^\circ$C or 87.5% (v/v) of
ethanol. These results confirmed the hypothesis that cross-linking by GP
increased the stability of CMs.
",0,1,0,0,0,0
3545,Local White Matter Architecture Defines Functional Brain Dynamics,"  Large bundles of myelinated axons, called white matter, anatomically connect
disparate brain regions together and compose the structural core of the human
connectome. We recently proposed a method of measuring the local integrity
along the length of each white matter fascicle, termed the local connectome. If
communication efficiency is fundamentally constrained by the integrity along
the entire length of a white matter bundle, then variability in the functional
dynamics of brain networks should be associated with variability in the local
connectome. We test this prediction using two statistical approaches that are
capable of handling the high dimensionality of data. First, by performing
statistical inference on distance-based correlations, we show that similarity
in the local connectome between individuals is significantly correlated with
similarity in their patterns of functional connectivity. Second, by employing
variable selection using sparse canonical correlation analysis and
cross-validation, we show that segments of the local connectome are predictive
of certain patterns of functional brain dynamics. These results are consistent
with the hypothesis that structural variability along axon bundles constrains
communication between disparate brain regions.
",0,0,0,1,1,0
430,Robot human interface for housekepeer with wireless capabilities,"  This paper presents the design and implementation of a Human Interface for a
housekeeper robot. It bases on the idea of making the robot understand the
human needs without making the human go through the details of robots work, for
example, the way that the robot implements the work or the method that the
robot uses to plan the path in order to reach the work area. The interface
commands based on idioms of the natural human language and designed in a manner
that the user gives the robot several commands with their execution date/time.
",1,0,0,0,0,0
526,Deuterium fractionation and H2D+ evolution in turbulent and magnetized cloud cores,"  High-mass stars are expected to form from dense prestellar cores. Their
precise formation conditions are widely discussed, including their virial
condition, which results in slow collapse for super-virial cores with strong
support by turbulence or magnetic fields, or fast collapse for sub-virial
sources. To disentangle their formation processes, measurements of the
deuterium fractions are frequently employed to approximately estimate the ages
of these cores and to obtain constraints on their dynamical evolution. We here
present 3D magneto-hydrodynamical simulations including for the first time an
accurate non-equilibrium chemical network with 21 gas-phase species plus dust
grains and 213 reactions. With this network we model the deuteration process in
fully depleted prestellar cores in great detail and determine its response to
variations in the initial conditions. We explore the dependence on the initial
gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio
and the distribution of the magnetic field, as well as the initial
ortho-to-para ratio of H2. We find excellent agreement with recent observations
of deuterium fractions in quiescent sources. Our results show that deuteration
is rather efficient, even when assuming a conservative ortho-to-para ratio of 3
and highly sub-virial initial conditions, leading to large deuterium fractions
already within roughly a free-fall time. We discuss the implications of our
results and give an outlook to relevant future investigations.
",0,1,0,0,0,0
4665,"Connecting the dots between mechanosensitive channel abundance, osmotic shock, and survival at single-cell resolution","  Rapid changes in extracellular osmolarity are one of many insults microbial
cells face on a daily basis. To protect against such shocks, Escherichia coli
and other microbes express several types of transmembrane channels which open
and close in response to changes in membrane tension. In E. coli, one of the
most abundant channels is the mechanosensitive channel of large conductance
(MscL). While this channel has been heavily characterized through structural
methods, electrophysiology, and theoretical modeling, our understanding of its
physiological role in preventing cell death by alleviating high membrane
tension remains tenuous. In this work, we examine the contribution of MscL
alone to cell survival after osmotic shock at single cell resolution using
quantitative fluorescence microscopy. We conduct these experiments in an E.
coli strain which is lacking all mechanosensitive channel genes save for MscL
whose expression is tuned across three orders of magnitude through
modifications of the Shine-Dalgarno sequence. While theoretical models suggest
that only a few MscL channels would be needed to alleviate even large changes
in osmotic pressure, we find that between 500 and 700 channels per cell are
needed to convey upwards of 80% survival. This number agrees with the average
MscL copy number measured in wild-type E. coli cells through proteomic studies
and quantitative Western blotting. Furthermore, we observe zero survival events
in cells with less than 100 channels per cell. This work opens new questions
concerning the contribution of other mechanosensitive channels to survival as
well as regulation of their activity.
",0,0,0,0,1,0
4905,Harnessing functional segregation across brain rhythms as a means to detect EEG oscillatory multiplexing during music listening,"  Music, being a multifaceted stimulus evolving at multiple timescales,
modulates brain function in a manifold way that encompasses not only the
distinct stages of auditory perception but also higher cognitive processes like
memory and appraisal. Network theory is apparently a promising approach to
describe the functional reorganization of brain oscillatory dynamics during
music listening. However, the music induced changes have so far been examined
within the functional boundaries of isolated brain rhythms. Using naturalistic
music, we detected the functional segregation patterns associated with
different cortical rhythms, as these were reflected in the surface EEG
measurements. The emerged structure was compared across frequency bands to
quantify the interplay among rhythms. It was also contrasted against the
structure from the rest and noise listening conditions to reveal the specific
components stemming from music listening. Our methodology includes an efficient
graph-partitioning algorithm, which is further utilized for mining prototypical
modular patterns, and a novel algorithmic procedure for identifying switching
nodes that consistently change module during music listening. Our results
suggest the multiplex character of the music-induced functional reorganization
and particularly indicate the dependence between the networks reconstructed
from the {\delta} and {\beta}H rhythms. This dependence is further justified
within the framework of nested neural oscillations and fits perfectly within
the context of recently introduced cortical entrainment to music. Considering
its computational efficiency, and in conjunction with the flexibility of in
situ electroencephalography, it may lead to novel assistive tools for real-life
applications.
",0,0,0,0,1,0
469,Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems,"  We consider bilinear optimal control problems, whose objective functionals do
not depend on the controls. Hence, bang-bang solutions will appear. We
investigate sufficient second-order conditions for bang-bang controls, which
guarantee local quadratic growth of the objective functional in $L^1$. In
addition, we prove that for controls that are not bang-bang, no such growth can
be expected. Finally, we study the finite-element discretization, and prove
error estimates of bang-bang controls in $L^1$-norms.
",0,0,1,0,0,0
10204,A Community Microgrid Architecture with an Internal Local Market,"  This work fits in the context of community microgrids, where members of a
community can exchange energy and services among themselves, without going
through the usual channels of the public electricity grid. We introduce and
analyze a framework to operate a community microgrid, and to share the
resulting revenues and costs among its members. A market-oriented pricing of
energy exchanges within the community is obtained by implementing an internal
local market based on the marginal pricing scheme. The market aims at
maximizing the social welfare of the community, thanks to the more efficient
allocation of resources, the reduction of the peak power to be paid, and the
increased amount of reserve, achieved at an aggregate level. A community
microgrid operator, acting as a benevolent planner, redistributes revenues and
costs among the members, in such a way that the solution achieved by each
member within the community is not worse than the solution it would achieve by
acting individually. In this way, each member is incentivized to participate in
the community on a voluntary basis. The overall framework is formulated in the
form of a bilevel model, where the lower level problem clears the market, while
the upper level problem plays the role of the community microgrid operator.
Numerical results obtained on a real test case implemented in Belgium show
significant cost savings on a yearly scale for the community members, as
compared to the case when they act individually.
",1,0,0,0,0,1
6862,PT-Spike: A Precise-Time-Dependent Single Spike Neuromorphic Architecture with Efficient Supervised Learning,"  One of the most exciting advancements in AI over the last decade is the wide
adoption of ANNs, such as DNN and CNN, in many real-world applications.
However, the underlying massive amounts of computation and storage requirement
greatly challenge their applicability in resource-limited platforms like the
drone, mobile phone, and IoT devices etc. The third generation of neural
network model--Spiking Neural Network (SNN), inspired by the working mechanism
and efficiency of human brain, has emerged as a promising solution for
achieving more impressive computing and power efficiency within light-weighted
devices (e.g. single chip). However, the relevant research activities have been
narrowly carried out on conventional rate-based spiking system designs for
fulfilling the practical cognitive tasks, underestimating SNN's energy
efficiency, throughput, and system flexibility. Although the time-based SNN can
be more attractive conceptually, its potentials are not unleashed in realistic
applications due to lack of efficient coding and practical learning schemes. In
this work, a Precise-Time-Dependent Single Spike Neuromorphic Architecture,
namely ""PT-Spike"", is developed to bridge this gap. Three constituent
hardware-favorable techniques: precise single-spike temporal encoding,
efficient supervised temporal learning, and fast asymmetric decoding are
proposed accordingly to boost the energy efficiency and data processing
capability of the time-based SNN at a more compact neural network model size
when executing real cognitive tasks. Simulation results show that ""PT-Spike""
demonstrates significant improvements in network size, processing efficiency
and power consumption with marginal classification accuracy degradation when
compared with the rate-based SNN and ANN under the similar network
configuration.
",0,0,0,0,1,0
781,Fine Structure and Lifetime of Dark Excitons in Transition Metal Dichalcogenide Monolayers,"  The intricate interplay between optically dark and bright excitons governs
the light-matter interaction in transition metal dichalcogenide monolayers. We
have performed a detailed investigation of the ""spin-forbidden"" dark excitons
in WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field
Bz. In agreement with the theoretical predictions deduced from group theory
analysis, magneto-photoluminescence experiments reveal a zero field splitting
$\delta=0.6 \pm 0.1$ meV between two dark exciton states. The low energy state
being strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state
is partially coupled to light with z polarization (""grey"" exciton). The first
determination of the dark neutral exciton lifetime $\tau_D$ in a transition
metal dichalcogenide monolayer is obtained by time-resolved photoluminescence.
We measure $\tau_D \sim 110 \pm 10$ ps for the grey exciton state, i.e. two
orders of magnitude longer than the radiative lifetime of the bright neutral
exciton at T=12 K.
",0,1,0,0,0,0
379,Gravitational wave production from preheating: parameter dependence,"  Parametric resonance is among the most efficient phenomena generating
gravitational waves (GWs) in the early Universe. The dynamics of parametric
resonance, and hence of the GWs, depend exclusively on the resonance parameter
$q$. The latter is determined by the properties of each scenario: the initial
amplitude and potential curvature of the oscillating field, and its coupling to
other species. Previous works have only studied the GW production for fixed
value(s) of $q$. We present an analytical derivation of the GW amplitude
dependence on $q$, valid for any scenario, which we confront against numerical
results. By running lattice simulations in an expanding grid, we study for a
wide range of $q$ values, the production of GWs in post-inflationary preheating
scenarios driven by parametric resonance. We present simple fits for the final
amplitude and position of the local maxima in the GW spectrum. Our
parametrization allows to predict the location and amplitude of the GW
background today, for an arbitrary $q$. The GW signal can be rather large, as
$h^2\Omega_{\rm GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high
frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of
spectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,
or the Standard Model Higgs.
",0,1,0,0,0,0
994,Anomaly detecting and ranking of the cloud computing platform by multi-view learning,"  Anomaly detecting as an important technical in cloud computing is applied to
support smooth running of the cloud platform. Traditional detecting methods
based on statistic, analysis, etc. lead to the high false-alarm rate due to
non-adaptive and sensitive parameters setting. We presented an online model for
anomaly detecting using machine learning theory. However, most existing methods
based on machine learning linked all features from difference sub-systems into
a long feature vector directly, which is difficult to both exploit the
complement information between sub-systems and ignore multi-view features
enhancing the classification performance. Aiming to this problem, the proposed
method automatic fuses multi-view features and optimize the discriminative
model to enhance the accuracy. This model takes advantage of extreme learning
machine (ELM) to improve detection efficiency. ELM is the single hidden layer
neural network, which is transforming iterative solution the output weights to
solution of linear equations and avoiding the local optimal solution. Moreover,
we rank anomies according to the relationship between samples and the
classification boundary, and then assigning weights for ranked anomalies,
retraining the classification model finally. Our method exploits the complement
information between sub-systems sufficiently, and avoids the influence from
imbalance dataset, therefore, deal with various challenges from the cloud
computing platform. We deploy the privately cloud platform by Openstack,
verifying the proposed model and comparing results to the state-of-the-art
methods with better efficiency and simplicity.
",1,0,0,1,0,0
892,Pressure-tuning of bond-directional exchange interactions and magnetic frustration in hyperhoneycomb iridate $β$-$\mathrm{Li_2IrO_3}$,"  We explore the response of Ir $5d$ orbitals to pressure in
$\beta$-$\mathrm{Li_2IrO_3}$, a hyperhoneycomb iridate in proximity to a Kitaev
quantum spin liquid (QSL) ground state. X-ray absorption spectroscopy reveals a
reconstruction of the electronic ground state below 2 GPa, the same pressure
range where x-ray magnetic circular dichroism shows an apparent collapse of
magnetic order. The electronic reconstruction, which manifests a reduction in
the effective spin-orbit (SO) interaction in $5d$ orbitals, pushes
$\beta$-$\mathrm{Li_2IrO_3}$ further away from the pure $J_{\rm eff}=1/2$
limit. Although lattice symmetry is preserved across the electronic transition,
x-ray diffraction shows a highly anisotropic compression of the hyperhoneycomb
lattice which affects the balance of bond-directional Ir-Ir exchange
interactions driven by spin-orbit coupling at Ir sites. An enhancement of
symmetric anisotropic exchange over Kitaev and Heisenberg exchange interactions
seen in theoretical calculations that use precisely this anisotropic Ir-Ir bond
compression provides one possible route to realization of a QSL state in this
hyperhoneycomb iridate at high pressures.
",0,1,0,0,0,0
213,Seifert surgery on knots via Reidemeister torsion and Casson-Walker-Lescop invariant III,"  For a knot $K$ in a homology $3$-sphere $\Sigma$, let $M$ be the result of
$2/q$-surgery on $K$, and let $X$ be the universal abelian covering of $M$. Our
first theorem is that if the first homology of $X$ is finite cyclic and $M$ is
a Seifert fibered space with $N\ge 3$ singular fibers, then $N\ge 4$ if and
only if the first homology of the universal abelian covering of $X$ is
infinite. Our second theorem is that under an appropriate assumption on the
Alexander polynomial of $K$, if $M$ is a Seifert fibered space, then $q=\pm 1$
(i.e.\ integral surgery).
",0,0,1,0,0,0
4484,Fast swaption pricing in Gaussian term structure models,"  We propose a fast and accurate numerical method for pricing European
swaptions in multi-factor Gaussian term structure models. Our method can be
used to accelerate the calibration of such models to the volatility surface.
The pricing of an interest rate option in such a model involves evaluating a
multi-dimensional integral of the payoff of the claim on a domain where the
payoff is positive. In our method, we approximate the exercise boundary of the
state space by a hyperplane tangent to the maximum probability point on the
boundary and simplify the multi-dimensional integration into an analytical
form. The maximum probability point can be determined using the gradient
descent method. We demonstrate that our method is superior to previous methods
by comparing the results to the price obtained by numerical integration.
",0,0,0,0,0,1
4601,A Parsimonious Dynamical Model for Structural Learning in the Human Brain,"  The human brain is capable of diverse feats of intelligence. A particularly
salient example is the ability to deduce structure from time-varying auditory
and visual stimuli, enabling humans to master the rules of language and to
build rich expectations of their physical environment. The broad relevance of
this ability for human cognition motivates the need for a first-principles
model explicating putative mechanisms. Here we propose a general framework for
structural learning in the brain, composed of an evolving, high-dimensional
dynamical system driven by external stimuli or internal processes. We
operationalize the scenario in which humans learn the rules that generate a
sequence of stimuli, rather than the exemplar stimuli themselves. We model
external stimuli as seemingly disordered chaotic time series generated by
complex dynamical systems; the underlying structure being deduced is then that
of the corresponding chaotic attractor. This approach allows us to demonstrate
and theoretically explain the emergence of five distinct phenomena reminiscent
of cognitive functions: (i) learning the structure of a chaotic system purely
from time series, (ii) generating new streams of stimuli from a chaotic system,
(iii) switching stream generation among multiple learned chaotic systems,
either spontaneously or in response to external perturbations, (iv) inferring
missing data from sparse observations of the chaotic system, and (v)
deciphering superimposed input from different chaotic systems. Numerically, we
show that these phenomena emerge naturally from a recurrent neural network of
Erdos-Renyi topology in which the synaptic strengths adapt in a Hebbian-like
manner. Broadly, our work blends chaotic theory and artificial neural networks
to answer the long standing question of how neural systems can learn the
structure underlying temporal sequences of stimuli.
",0,0,0,0,1,0
2703,A unifying framework for the modelling and analysis of STR DNA samples arising in forensic casework,"  This paper presents a new framework for analysing forensic DNA samples using
probabilistic genotyping. Specifically it presents a mathematical framework for
specifying and combining the steps in producing forensic casework
electropherograms of short tandem repeat loci from DNA samples. It is
applicable to both high and low template DNA samples, that is, samples
containing either high or low amounts DNA. A specific model is developed within
the framework, by way of particular modelling assumptions and approximations,
and its interpretive power presented on examples using simulated data and data
from a publicly available dataset. The framework relies heavily on the use of
univariate and multivariate probability generating functions. It is shown that
these provide a succinct and elegant mathematical scaffolding to model the key
steps in the process. A significant development in this paper is that of new
numerical methods for accurately and efficiently evaluating the probability
distribution of amplicons arising from the polymerase chain reaction process,
which is modelled as a discrete multi-type branching process. Source code in
the scripting languages Python, R and Julia is provided for illustration of
these methods. These new developments will be of general interest to persons
working outside the province of forensic DNA interpretation that this paper
focuses on.
",0,0,0,1,1,0
10215,Strong and Weak Equilibria for Time-Inconsistent Stochastic Control in Continuous Time,"  A new definition of continuous-time equilibrium controls is introduced. As
opposed to the standard definition, which involves a derivative-type operation,
the new definition parallels how a discrete-time equilibrium is defined, and
allows for unambiguous economic interpretation. The terms ""strong equilibria""
and ""weak equilibria"" are coined for controls under the new and the standard
definitions, respectively. When the state process is a time-homogeneous
continuous-time Markov chain, a careful asymptotic analysis gives complete
characterizations of weak and strong equilibria. Thanks to Kakutani-Fan's
fixed-point theorem, general existence of weak and strong equilibria is also
established, under additional compactness assumption. Our theoretic results are
applied to a two-state model under non-exponential discounting. In particular,
we demonstrate explicitly that there can be incentive to deviate from a weak
equilibrium, which justifies the need for strong equilibria. Our analysis also
provides new results for the existence and characterization of discrete-time
equilibria under infinite horizon.
",0,0,0,0,0,1
1099,Functional importance of noise in neuronal information processing,"  Noise is an inherent part of neuronal dynamics, and thus of the brain. It can
be observed in neuronal activity at different spatiotemporal scales, including
in neuronal membrane potentials, local field potentials,
electroencephalography, and magnetoencephalography. A central research topic in
contemporary neuroscience is to elucidate the functional role of noise in
neuronal information processing. Experimental studies have shown that a
suitable level of noise may enhance the detection of weak neuronal signals by
means of stochastic resonance. In response, theoretical research, based on the
theory of stochastic processes, nonlinear dynamics, and statistical physics,
has made great strides in elucidating the mechanism and the many benefits of
stochastic resonance in neuronal systems. In this perspective, we review recent
research dedicated to neuronal stochastic resonance in biophysical mathematical
models. We also explore the regulation of neuronal stochastic resonance, and we
outline important open questions and directions for future research. A deeper
understanding of neuronal stochastic resonance may afford us new insights into
the highly impressive information processing in the brain.
",0,0,0,0,1,0
790,"Heads or tails in zero gravity: an example of a classical contextual ""measurement""","  Playing the game of heads or tails in zero gravity demonstrates that there
exists a contextual ""measurement"" in classical mechanics. When the coin is
flipped, its orientation is a continuous variable. However, the ""measurement""
that occurs when the coin is caught by clapping two hands together gives a
discrete value (heads or tails) that depends on the context (orientation of the
hands). It is then shown that there is a strong analogy with the spin
measurement of the Stern-Gerlach experiment, and in particular with Stern and
Gerlach's sequential measurements. Finally, we clarify the analogy by recalling
how the de Broglie-Bohm interpretation simply explains the spin ""measurement"".
",0,1,0,0,0,0
415,Helmholtz decomposition theorem and Blumenthal's extension by regularization,"  Helmholtz decomposition theorem for vector fields is usually presented with
too strong restrictions on the fields and only for time independent fields.
Blumenthal showed in 1905 that decomposition is possible for any asymptotically
weakly decreasing vector field. He used a regularization method in his proof
which can be extended to prove the theorem even for vector fields
asymptotically increasing sublinearly. Blumenthal's result is then applied to
the time-dependent fields of the dipole radiation and an artificial sublinearly
increasing field.
",0,1,0,0,0,0
457,"Mutual Information, Relative Entropy and Estimation Error in Semi-martingale Channels","  Fundamental relations between information and estimation have been
established in the literature for the continuous-time Gaussian and Poisson
channels, in a long line of work starting from the classical representation
theorems by Duncan and Kabanov respectively. In this work, we demonstrate that
such relations hold for a much larger family of continuous-time channels. We
introduce the family of semi-martingale channels where the channel output is a
semi-martingale stochastic process, and the channel input modulates the
characteristics of the semi-martingale. For these channels, which includes as a
special case the continuous time Gaussian and Poisson models, we establish new
representations relating the mutual information between the channel input and
output to an optimal causal filtering loss, thereby unifying and considerably
extending results from the Gaussian and Poisson settings. Extensions to the
setting of mismatched estimation are also presented where the relative entropy
between the laws governing the output of the channel under two different input
distributions is equal to the cumulative difference between the estimation loss
incurred by using the mismatched and optimal causal filters respectively. The
main tool underlying these results is the Doob--Meyer decomposition of a class
of likelihood ratio sub-martingales. The results in this work can be viewed as
the continuous-time analogues of recent generalizations for relations between
information and estimation for discrete-time Lévy channels.
",1,0,0,0,0,0
531,A Data-Driven Sparse-Learning Approach to Model Reduction in Chemical Reaction Networks,"  In this paper, we propose an optimization-based sparse learning approach to
identify the set of most influential reactions in a chemical reaction network.
This reduced set of reactions is then employed to construct a reduced chemical
reaction mechanism, which is relevant to chemical interaction network modeling.
The problem of identifying influential reactions is first formulated as a
mixed-integer quadratic program, and then a relaxation method is leveraged to
reduce the computational complexity of our approach. Qualitative and
quantitative validation of the sparse encoding approach demonstrates that the
model captures important network structural properties with moderate
computational load.
",1,0,0,0,0,0
5395,Evaluation of equity-based debt obligations,"  We consider a class of participation rights, i.e. obligations issued by a
company to investors who are interested in performance-based compensation.
Albeit having desirable economic properties equity-based debt obligations
(EbDO) pose challenges in accounting and contract pricing. We formulate and
solve the associated mathematical problem in a discrete time, as well as a
continuous time setting. In the latter case the problem is reduced to a
forward-backward stochastic differential equation (FBSDE) and solved using the
method of decoupling fields.
",0,0,0,0,0,1
5497,Exact Combinatorial Inference for Brain Images,"  The permutation test is known as the exact test procedure in statistics.
However, often it is not exact in practice and only an approximate method since
only a small fraction of every possible permutation is generated. Even for a
small sample size, it often requires to generate tens of thousands
permutations, which can be a serious computational bottleneck. In this paper,
we propose a novel combinatorial inference procedure that enumerates all
possible permutations combinatorially without any resampling. The proposed
method is validated against the standard permutation test in simulation studies
with the ground truth. The method is further applied in twin DTI study in
determining the genetic contribution of the minimum spanning tree of the
structural brain connectivity.
",0,0,0,1,1,0
930,Learning convex bounds for linear quadratic control policy synthesis,"  Learning to make decisions from observed data in dynamic environments remains
a problem of fundamental importance in a number of fields, from artificial
intelligence and robotics, to medicine and finance. This paper concerns the
problem of learning control policies for unknown linear dynamical systems so as
to maximize a quadratic reward function. We present a method to optimize the
expected value of the reward over the posterior distribution of the unknown
system parameters, given data. The algorithm involves sequential convex
programing, and enjoys reliable local convergence and robust stability
guarantees. Numerical simulations and stabilization of a real-world inverted
pendulum are used to demonstrate the approach, with strong performance and
robustness properties observed in both.
",0,0,0,1,0,0
794,The Time Dimension of Science: Connecting the Past to the Future,"  A central question in science of science concerns how time affects citations.
Despite the long-standing interests and its broad impact, we lack systematic
answers to this simple yet fundamental question. By reviewing and classifying
prior studies for the past 50 years, we find a significant lack of consensus in
the literature, primarily due to the coexistence of retrospective and
prospective approaches to measuring citation age distributions. These two
approaches have been pursued in parallel, lacking any known connections between
the two. Here we developed a new theoretical framework that not only allows us
to connect the two approaches through precise mathematical relationships, it
also helps us reconcile the interplay between temporal decay of citations and
the growth of science, helping us uncover new functional forms characterizing
citation age distributions. We find retrospective distribution follows a
lognormal distribution with exponential cutoff, while prospective distribution
is governed by the interplay between a lognormal distribution and the growth in
the number of references. Most interestingly, the two approaches can be
connected once rescaled by the growth of publications and citations. We further
validate our framework using both large-scale citation datasets and analytical
models capturing citation dynamics. Together this paper presents a
comprehensive analysis of the time dimension of science, representing a new
empirical and theoretical basis for all future studies in this area.
",1,1,0,0,0,0
733,Homology theory formulas for generalized Riemann-Hurwitz and generalized monoidal transformations,"  In the context of orientable circuits and subcomplexes of these as
representing certain singular spaces, we consider characteristic class formulas
generalizing those classical results as seen for the Riemann-Hurwitz formula
for regulating the topology of branched covering maps and that for monoidal
transformations which include the standard blowing-up process. Here the results
are presented as cap product pairings, which will be elements of a suitable
homology theory, rather than characteristic numbers as would be the case when
taking Kronecker products once Poincaré duality is defined. We further
consider possible applications and examples including branched covering maps,
singular varieties involving virtual tangent bundles, the
Chern-Schwartz-MacPherson class, the homology L-class, generalized signature,
and the cohomology signature class.
",0,0,1,0,0,0
1687,A Lattice Model of Charge-Pattern-Dependent Polyampholyte Phase Separation,"  In view of recent intense experimental and theoretical interests in the
biophysics of liquid-liquid phase separation (LLPS) of intrinsically disordered
proteins (IDPs), heteropolymer models with chain molecules configured as
self-avoiding walks on the simple cubic lattice are constructed to study how
phase behaviors depend on the sequence of monomers along the chains. To address
pertinent general principles, we focus primarily on two fully charged
50-monomer sequences with significantly different charge patterns. Each monomer
in our models occupies a single lattice site and all monomers interact via a
screened pairwise Coulomb potential. Phase diagrams are obtained by extensive
Monte Carlo sampling performed at multiple temperatures on ensembles of 300
chains in boxes of sizes ranging from $52\times 52\times 52$ to $246\times
246\times 246$ to simulate a large number of different systems with the overall
polymer volume fraction $\phi$ in each system varying from $0.001$ to $0.1$.
Phase separation in the model systems is characterized by the emergence of a
large cluster connected by inter-monomer nearest-neighbor lattice contacts and
by large fluctuations in local polymer density. The simulated critical
temperatures, $T_{\rm cr}$, of phase separation for the two sequences differ
significantly, whereby the sequence with a more ""blocky"" charge pattern
exhibits a substantially higher propensity to phase separate. The trend is
consistent with our sequence-specific random-phase-approximation (RPA) polymer
theory, but the variation of the simulated $T_{\rm cr}$ with a previously
proposed ""sequence charge decoration"" pattern parameter is milder than that
predicted by RPA. Ramifications of our findings for the development of
analytical theory and simulation protocols of IDP LLPS are discussed.
",0,0,0,0,1,0
521,Towards a Service-oriented Platform for Intelligent Apps in Intermediate Cities,"  Smart cities are a growing trend in many cities in Argentina. In particular,
the so-called intermediate cities present a context and requirements different
from those of large cities with respect to smart cities. One aspect of
relevance is to encourage the development of applications (generally for mobile
devices) that enable citizens to take advantage of data and services normally
associated with the city, for example, in the urban mobility domain. In this
work, a platform is proposed for intermediate cities that provide ""high level""
services and that allow the construction of software applications that consume
those services. Our platform-centric strategy focused aims to integrate systems
and heterogeneous data sources, and provide ""intelligent"" services to different
applications. Examples of these services include: construction of user
profiles, recommending local events, and collaborative sensing based on data
mining techniques, among others. In this work, the design of this platform
(currently in progress) is described, and experiences of applications for urban
mobility are discussed, which are being migrated in the form of reusable
services provided by the platform
",1,0,0,0,0,0
797,Kernel Approximation Methods for Speech Recognition,"  We study large-scale kernel methods for acoustic modeling in speech
recognition and compare their performance to deep neural networks (DNNs). We
perform experiments on four speech recognition datasets, including the TIMIT
and Broadcast News benchmark tasks, and compare these two types of models on
frame-level performance metrics (accuracy, cross-entropy), as well as on
recognition metrics (word/character error rate). In order to scale kernel
methods to these large datasets, we use the random Fourier feature method of
Rahimi and Recht (2007). We propose two novel techniques for improving the
performance of kernel acoustic models. First, in order to reduce the number of
random features required by kernel models, we propose a simple but effective
method for feature selection. The method is able to explore a large number of
non-linear features while maintaining a compact model more efficiently than
existing approaches. Second, we present a number of frame-level metrics which
correlate very strongly with recognition performance when computed on the
heldout set; we take advantage of these correlations by monitoring these
metrics during training in order to decide when to stop learning. This
technique can noticeably improve the recognition performance of both DNN and
kernel models, while narrowing the gap between them. Additionally, we show that
the linear bottleneck method of Sainath et al. (2013) improves the performance
of our kernel models significantly, in addition to speeding up training and
making the models more compact. Together, these three methods dramatically
improve the performance of kernel acoustic models, making their performance
comparable to DNNs on the tasks we explored.
",1,0,0,1,0,0
4027,Gradient Sensing via Cell Communication,"  The chemotactic dynamics of cells and organisms that have no specialized
gradient sensing organelles is not well understood. In fact, chemotaxis of this
sort of organism is especially challenging to explain when the external
chemical gradient is so small as to make variations of concentrations minute
over the length of each of the organisms. Experimental evidence lends support
to the conjecture that chemotactic behavior of chains of cells can be achieved
via cell-to-cell communication. This is the chemotactic basis for the Local
Excitation, Global Inhibition (LEGI) model.
A generalization of the model for the communication component of the LEGI
model is proposed. Doing so permits us to study in detail how gradient sensing
changes as a function of the structure of the communication term. The key
findings of this study are, an accounting of how gradient sensing is affected
by the competition of communication and diffusive processes; the determination
of the scale dependence of the model outcomes; the sensitivity of communication
to parameters in the model. Together with an essential analysis of the dynamics
of the model, these findings can prove useful in suggesting experiments aimed
at determining the viability of a communication mechanism in chemotactic
dynamics of chains and networks of cells exposed to a chemical concentration
gradient.
",0,0,0,0,1,0
324,Bayesian Optimization for Probabilistic Programs,"  We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.
",1,0,0,1,0,0
3155,A Biologically Plausible Supervised Learning Method for Spiking Neural Networks Using the Symmetric STDP Rule,"  Spiking neural networks (SNNs) possess energy-efficient potential due to
event-based computation. However, supervised training of SNNs remains a
challenge as spike activities are non-differentiable. Previous SNNs training
methods can basically be categorized into two classes, backpropagation-like
training methods and plasticity-based learning methods. The former methods are
dependent on energy-inefficient real-valued computation and non-local
transmission, as also required in artificial neural networks (ANNs), while the
latter either be considered biologically implausible or exhibit poor
performance. Hence, biologically plausible (bio-plausible) high-performance
supervised learning (SL) methods for SNNs remain deficient. In this paper, we
proposed a novel bio-plausible SNN model for SL based on the symmetric
spike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By
combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic
plasticity of the dynamic threshold, our SNN model implemented SL well and
achieved good performance in the benchmark recognition task (MNIST). To reveal
the underlying mechanism of our SL model, we visualized both layer-based
activities and synaptic weights using the t-distributed stochastic neighbor
embedding (t-SNE) method after training and found that they were well
clustered, thereby demonstrating excellent classification ability. As the
learning rules were bio-plausible and based purely on local spike events, our
model could be easily applied to neuromorphic hardware for online training and
may be helpful for understanding SL information processing at the synaptic
level in biological neural systems.
",0,0,0,0,1,0
4372,On the Analysis of Bacterial Cooperation with a Characterization of 2D Signal Propagation,"  The exchange of small molecular signals within microbial populations is
generally referred to as quorum sensing (QS). QS is ubiquitous in nature and
enables microorganisms to respond to fluctuations of living environments by
working together. In this work, a QS-based communication system within a
microbial population in a two-dimensional (2D) environment is analytically
modeled. Notably, the diffusion and degradation of signaling molecules within
the population is characterized. Microorganisms are randomly distributed on a
2D circle where each one releases molecules at random times. The number of
molecules observed at each randomly-distributed bacterium is analyzed. Using
this analysis and some approximation, the expected density of cooperating
bacteria is derived. The analytical results are validated via a particle-based
simulation method. The model can be used to predict and control behavioral
dynamics of microscopic populations that have imperfect signal propagation.
",0,0,0,0,1,0
933,Heisenberg Modules over Quantum 2-tori are metrized quantum vector bundles,"  The modular Gromov-Hausdorff propinquity is a distance on classes of modules
endowed with quantum metric information, in the form of a metric form of a
connection and a left Hilbert module structure. This paper proves that the
family of Heisenberg modules over quantum two tori, when endowed with their
canonical connections, form a family of metrized quantum vector bundles, as a
first step in proving that Heisenberg modules form a continuous family for the
modular Gromov-Hausdorff propinquity.
",0,0,1,0,0,0
484,Performance Analysis of Ultra-Dense Networks with Elevated Base Stations,"  This paper analyzes the downlink performance of ultra-dense networks with
elevated base stations (BSs). We consider a general dual-slope pathloss model
with distance-dependent probability of line-of-sight (LOS) transmission between
BSs and receivers. Specifically, we consider the scenario where each link may
be obstructed by randomly placed buildings. Using tools from stochastic
geometry, we show that both coverage probability and area spectral efficiency
decay to zero as the BS density grows large. Interestingly, we show that the BS
height alone has a detrimental effect on the system performance even when the
standard single-slope pathloss model is adopted.
",1,0,0,0,0,0
454,Diffusion Maps meet Nyström,"  Diffusion maps are an emerging data-driven technique for non-linear
dimensionality reduction, which are especially useful for the analysis of
coherent structures and nonlinear embeddings of dynamical systems. However, the
computational complexity of the diffusion maps algorithm scales with the number
of observations. Thus, long time-series data presents a significant challenge
for fast and efficient embedding. We propose integrating the Nyström method
with diffusion maps in order to ease the computational demand. We achieve a
speedup of roughly two to four times when approximating the dominant diffusion
map components.
",0,0,0,1,0,0
515,"Investigation on different physical aspects such as structural, elastic, mechanical, optical properties and Debye temperature of Fe2ScM (M = P and As) semiconductors: a DFT based first principles study","  With the help of first principles calculation method based on the density
functional theory we have investigated the structural, elastic, mechanical
properties and Debye temperature of Fe2ScM (M = P and As) compounds under
pressure up to 60 GPa. The optical properties have been investigated under zero
pressure. Our calculated optimized structural parameters of both the compounds
are in good agreement with the other theoretical results. The calculated
elastic constants show that Fe2ScM (M = P and As) compounds are mechanically
stable up to 60 GPa.
",0,1,0,0,0,0
978,A simple recipe for making accurate parametric inference in finite sample,"  Constructing tests or confidence regions that control over the error rates in
the long-run is probably one of the most important problem in statistics. Yet,
the theoretical justification for most methods in statistics is asymptotic. The
bootstrap for example, despite its simplicity and its widespread usage, is an
asymptotic method. There are in general no claim about the exactness of
inferential procedures in finite sample. In this paper, we propose an
alternative to the parametric bootstrap. We setup general conditions to
demonstrate theoretically that accurate inference can be claimed in finite
sample.
",0,0,1,1,0,0
448,Direct estimation of density functionals using a polynomial basis,"  A number of fundamental quantities in statistical signal processing and
information theory can be expressed as integral functions of two probability
density functions. Such quantities are called density functionals as they map
density functions onto the real line. For example, information divergence
functions measure the dissimilarity between two probability density functions
and are useful in a number of applications. Typically, estimating these
quantities requires complete knowledge of the underlying distribution followed
by multi-dimensional integration. Existing methods make parametric assumptions
about the data distribution or use non-parametric density estimation followed
by high-dimensional integration. In this paper, we propose a new alternative.
We introduce the concept of ""data-driven basis functions"" - functions of
distributions whose value we can estimate given only samples from the
underlying distributions without requiring distribution fitting or direct
integration. We derive a new data-driven complete basis that is similar to the
deterministic Bernstein polynomial basis and develop two methods for performing
basis expansions of functionals of two distributions. We also show that the new
basis set allows us to approximate functions of distributions as closely as
desired. Finally, we evaluate the methodology by developing data driven
estimators for the Kullback-Leibler divergences and the Hellinger distance and
by constructing empirical estimates of tight bounds on the Bayes error rate.
",1,0,0,1,0,0
770,Regularising Non-linear Models Using Feature Side-information,"  Very often features come with their own vectorial descriptions which provide
detailed information about their properties. We refer to these vectorial
descriptions as feature side-information. In the standard learning scenario,
input is represented as a vector of features and the feature side-information
is most often ignored or used only for feature selection prior to model
fitting. We believe that feature side-information which carries information
about features intrinsic property will help improve model prediction if used in
a proper way during learning process. In this paper, we propose a framework
that allows for the incorporation of the feature side-information during the
learning of very general model families to improve the prediction performance.
We control the structures of the learned models so that they reflect features
similarities as these are defined on the basis of the side-information. We
perform experiments on a number of benchmark datasets which show significant
predictive performance gains, over a number of baselines, as a result of the
exploitation of the side-information.
",1,0,0,1,0,0
12360,Herding behavior in cryptocurrency markets,"  There are no solid arguments to sustain that digital currencies are the
future of online payments or the disruptive technology that some of its former
participants declared when used to face critiques. This paper aims to solve the
cryptocurrency puzzle from a behavioral finance perspective by finding the
parallelism between biases present in financial markets that could be applied
to cryptomarkets. Moreover, it is suggested that cryptocurrencies' prices are
driven by herding, hence this study test herding behavior under asymmetric and
symmetric conditions and the existence of different herding regimes by
employing the Markov-Switching approach.
",0,0,0,0,0,1
528,Ensemble Sampling,"  Thompson sampling has emerged as an effective heuristic for a broad range of
online decision problems. In its basic form, the algorithm requires computing
and sampling from a posterior distribution over models, which is tractable only
for simple special cases. This paper develops ensemble sampling, which aims to
approximate Thompson sampling while maintaining tractability even in the face
of complex models such as neural networks. Ensemble sampling dramatically
expands on the range of applications for which Thompson sampling is viable. We
establish a theoretical basis that supports the approach and present
computational results that offer further insight.
",1,0,0,1,0,0
15945,Markov cubature rules for polynomial processes,"  We study discretizations of polynomial processes using finite state Markov
processes satisfying suitable moment matching conditions. The states of these
Markov processes together with their transition probabilities can be
interpreted as Markov cubature rules. The polynomial property allows us to
study such rules using algebraic techniques. Markov cubature rules aid the
tractability of path-dependent tasks such as American option pricing in models
where the underlying factors are polynomial processes.
",0,0,0,0,0,1
205,Learning Transferable Architectures for Scalable Image Recognition,"  Developing neural network image classification models often requires
significant architecture engineering. In this paper, we study a method to learn
the model architectures directly on the dataset of interest. As this approach
is expensive when the dataset is large, we propose to search for an
architectural building block on a small dataset and then transfer the block to
a larger dataset. The key contribution of this work is the design of a new
search space (the ""NASNet search space"") which enables transferability. In our
experiments, we search for the best convolutional layer (or ""cell"") on the
CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
together more copies of this cell, each with their own parameters to design a
convolutional architecture, named ""NASNet architecture"". We also introduce a
new regularization technique called ScheduledDropPath that significantly
improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
the best human-invented architectures while having 9 billion fewer FLOPS - a
reduction of 28% in computational demand from the previous state-of-the-art
model. When evaluated at different levels of computational cost, accuracies of
NASNets exceed those of the state-of-the-art human-designed models. For
instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
3.1% better than equivalently-sized, state-of-the-art models for mobile
platforms. Finally, the learned features by NASNet used with the Faster-RCNN
framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
dataset.
",1,0,0,0,0,0
3086,Portfolio Construction Matters,"  The role of portfolio construction in the implementation of equity market
neutral factors is often underestimated. Taking the classical momentum strategy
as an example, we show that one can significantly improve the main strategy's
features by properly taking care of this key step. More precisely, an optimized
portfolio construction algorithm allows one to significantly improve the Sharpe
Ratio, reduce sector exposures and volatility fluctuations, and mitigate the
strategy's skewness and tail correlation with the market. These results are
supported by long-term, world-wide simulations and will be shown to be
universal. Our findings are quite general and hold true for a number of other
""equity factors"". Finally, we discuss the details of a more realistic set-up
where we also deal with transaction costs.
",0,0,0,0,0,1
601,Regrasping by Fixtureless Fixturing,"  This paper presents a fixturing strategy for regrasping that does not require
a physical fixture. To regrasp an object in a gripper, a robot pushes the
object against external contact/s in the environment such that the external
contact keeps the object stationary while the fingers slide over the object. We
call this manipulation technique fixtureless fixturing. Exploiting the
mechanics of pushing, we characterize a convex polyhedral set of pushes that
results in fixtureless fixturing. These pushes are robust against uncertainty
in the object inertia, grasping force, and the friction at the contacts. We
propose a sampling-based planner that uses the sets of robust pushes to rapidly
build a tree of reachable grasps. A path in this tree is a pushing strategy,
possibly involving pushes from different sides, to regrasp the object. We
demonstrate the experimental validity and robustness of the proposed
manipulation technique with different regrasp examples on a manipulation
platform. Such a fast and flexible regrasp planner facilitates versatile and
flexible automation solutions.
",1,0,0,0,0,0
12438,Inferring short-term volatility indicators from Bitcoin blockchain,"  In this paper, we study the possibility of inferring early warning indicators
(EWIs) for periods of extreme bitcoin price volatility using features obtained
from Bitcoin daily transaction graphs. We infer the low-dimensional
representations of transaction graphs in the time period from 2012 to 2017
using Bitcoin blockchain, and demonstrate how these representations can be used
to predict extreme price volatility events. Our EWI, which is obtained with a
non-negative decomposition, contains more predictive information than those
obtained with singular value decomposition or scalar value of the total Bitcoin
transaction volume.
",1,0,0,0,0,1
15594,"Market Self-Learning of Signals, Impact and Optimal Trading: Invisible Hand Inference with Free Energy","  We present a simple model of a non-equilibrium self-organizing market where
asset prices are partially driven by investment decisions of a bounded-rational
agent. The agent acts in a stochastic market environment driven by various
exogenous ""alpha"" signals, agent's own actions (via market impact), and noise.
Unlike traditional agent-based models, our agent aggregates all traders in the
market, rather than being a representative agent. Therefore, it can be
identified with a bounded-rational component of the market itself, providing a
particular implementation of an Invisible Hand market mechanism. In such
setting, market dynamics are modeled as a fictitious self-play of such
bounded-rational market-agent in its adversarial stochastic environment. As
rewards obtained by such self-playing market agent are not observed from market
data, we formulate and solve a simple model of such market dynamics based on a
neuroscience-inspired Bounded Rational Information Theoretic Inverse
Reinforcement Learning (BRIT-IRL). This results in effective asset price
dynamics with a non-linear mean reversion - which in our model is generated
dynamically, rather than being postulated. We argue that our model can be used
in a similar way to the Black-Litterman model. In particular, it represents, in
a simple modeling framework, market views of common predictive signals, market
impacts and implied optimal dynamic portfolio allocations, and can be used to
assess values of private signals. Moreover, it allows one to quantify a
""market-implied"" optimal investment strategy, along with a measure of market
rationality. Our approach is numerically light, and can be implemented using
standard off-the-shelf software such as TensorFlow.
",0,0,0,0,0,1
6920,Non-equilibrium time dynamics of genetic evolution,"  Biological systems are typically highly open, non-equilibrium systems that
are very challenging to understand from a statistical mechanics perspective.
While statistical treatments of evolutionary biological systems have a long and
rich history, examination of the time-dependent non-equilibrium dynamics has
been less studied. In this paper we first derive a generalized master equation
in the genotype space for diploid organisms incorporating the processes of
selection, mutation, recombination, and reproduction. The master equation is
defined in terms of continuous time and can handle an arbitrary number of gene
loci and alleles, and can be defined in terms of an absolute population or
probabilities. We examine and analytically solve several prototypical cases
which illustrate the interplay of the various processes and discuss the
timescales of their evolution. The entropy production during the evolution
towards steady state is calculated and we find that it agrees with predictions
from non-equilibrium statistical mechanics where it is large when the
population distribution evolves towards a more viable genotype. The stability
of the non-equilibrium steady state is confirmed using the Glansdorff-Prigogine
criterion.
",0,0,0,0,1,0
14734,The cooling-off effect of price limits in the Chinese stock markets,"  In this paper, we investigate the cooling-off effect (opposite to the magnet
effect) from two aspects. Firstly, from the viewpoint of dynamics, we study the
existence of the cooling-off effect by following the dynamical evolution of
some financial variables over a period of time before the stock price hits its
limit. Secondly, from the probability perspective, we investigate, with the
logit model, the existence of the cooling-off effect through analyzing the
high-frequency data of all A-share common stocks traded on the Shanghai Stock
Exchange and the Shenzhen Stock Exchange from 2000 to 2011 and inspecting the
trading period from the opening phase prior to the moment that the stock price
hits its limits. A comparison is made of the properties between up-limit hits
and down-limit hits, and the possible difference will also be compared between
bullish and bearish market state by dividing the whole period into three
alternating bullish periods and three bearish periods. We find that the
cooling-off effect emerges for both up-limit hits and down-limit hits, and the
cooling-off effect of the down-limit hits is stronger than that of the up-limit
hits. The difference of the cooling-off effect between bullish period and
bearish period is quite modest. Moreover, we examine the sub-optimal orders
effect, and infer that the professional individual investors and institutional
investors play a positive role in the cooling-off effects. All these findings
indicate that the price limit trading rule exerts a positive effect on
maintaining the stability of the Chinese stock markets.
",0,0,0,0,0,1
671,The Future of RICH Detectors through the Light of the LHCb RICH,"  The limitations in performance of the present RICH system in the LHCb
experiment are given by the natural chromatic dispersion of the gaseous
Cherenkov radiator, the aberrations of the optical system and the pixel size of
the photon detectors. Moreover, the overall PID performance can be affected by
high detector occupancy as the pattern recognition becomes more difficult with
high particle multiplicities. This paper shows a way to improve performance by
systematically addressing each of the previously mentioned limitations. These
ideas are applied in the present and future upgrade phases of the LHCb
experiment. Although applied to specific circumstances, they are used as a
paradigm on what is achievable in the development and realisation of high
precision RICH detectors.
",0,1,0,0,0,0
1963,Affine forward variance models,"  We introduce the class of affine forward variance (AFV) models of which both
the conventional Heston model and the rough Heston model are special cases. We
show that AFV models can be characterized by the affine form of their cumulant
generating function, which can be obtained as solution of a convolution Riccati
equation. We further introduce the class of affine forward order flow intensity
(AFI) models, which are structurally similar to AFV models, but driven by jump
processes, and which include Hawkes-type models. We show that the cumulant
generating function of an AFI model satisfies a generalized convolution Riccati
equation and that a high-frequency limit of AFI models converges in
distribution to the AFV model.
",0,0,0,0,0,1
1799,Efficient algorithms to discover alterations with complementary functional association in cancer,"  Recent large cancer studies have measured somatic alterations in an
unprecedented number of tumours. These large datasets allow the identification
of cancer-related sets of genetic alterations by identifying relevant
combinatorial patterns. Among such patterns, mutual exclusivity has been
employed by several recent methods that have shown its effectivenes in
characterizing gene sets associated to cancer. Mutual exclusivity arises
because of the complementarity, at the functional level, of alterations in
genes which are part of a group (e.g., a pathway) performing a given function.
The availability of quantitative target profiles, from genetic perturbations or
from clinical phenotypes, provides additional information that can be leveraged
to improve the identification of cancer related gene sets by discovering groups
with complementary functional associations with such targets.
In this work we study the problem of finding groups of mutually exclusive
alterations associated with a quantitative (functional) target. We propose a
combinatorial formulation for the problem, and prove that the associated
computation problem is computationally hard. We design two algorithms to solve
the problem and implement them in our tool UNCOVER. We provide analytic
evidence of the effectiveness of UNCOVER in finding high-quality solutions and
show experimentally that UNCOVER finds sets of alterations significantly
associated with functional targets in a variety of scenarios. In addition, our
algorithms are much faster than the state-of-the-art, allowing the analysis of
large datasets of thousands of target profiles from cancer cell lines. We show
that on one such dataset from project Achilles our methods identify several
significant gene sets with complementary functional associations with targets.
",0,0,0,0,1,0
16691,The Price of BitCoin: GARCH Evidence from High Frequency Data,"  This is the first paper that estimates the price determinants of BitCoin in a
Generalised Autoregressive Conditional Heteroscedasticity framework using high
frequency data. Derived from a theoretical model, we estimate BitCoin
transaction demand and speculative demand equations in a GARCH framework using
hourly data for the period 2013-2018. In line with the theoretical model, our
empirical results confirm that both the BitCoin transaction demand and
speculative demand have a statistically significant impact on the BitCoin price
formation. The BitCoin price responds negatively to the BitCoin velocity,
whereas positive shocks to the BitCoin stock, interest rate and the size of the
BitCoin economy exercise an upward pressure on the BitCoin price.
",0,0,0,0,0,1
7743,A Hilbert Space of Stationary Ergodic Processes,"  Identifying meaningful signal buried in noise is a problem of interest
arising in diverse scenarios of data-driven modeling. We present here a
theoretical framework for exploiting intrinsic geometry in data that resists
noise corruption, and might be identifiable under severe obfuscation. Our
approach is based on uncovering a valid complete inner product on the space of
ergodic stationary finite valued processes, providing the latter with the
structure of a Hilbert space on the real field. This rigorous construction,
based on non-standard generalizations of the notions of sum and scalar
multiplication of finite dimensional probability vectors, allows us to
meaningfully talk about ""angles"" between data streams and data sources, and,
make precise the notion of orthogonal stochastic processes. In particular, the
relative angles appear to be preserved, and identifiable, under severe noise,
and will be developed in future as the underlying principle for robust
classification, clustering and unsupervised featurization algorithms.
",0,0,0,1,0,1
1589,Gene regulatory networks: a primer in biological processes and statistical modelling,"  Modelling gene regulatory networks not only requires a thorough understanding
of the biological system depicted but also the ability to accurately represent
this system from a mathematical perspective. Throughout this chapter, we aim to
familiarise the reader with the biological processes and molecular factors at
play in the process of gene expression regulation.We first describe the
different interactions controlling each step of the expression process, from
transcription to mRNA and protein decay. In the second section, we provide
statistical tools to accurately represent this biological complexity in the
form of mathematical models. Amongst other considerations, we discuss the
topological properties of biological networks, the application of deterministic
and stochastic frameworks and the quantitative modelling of regulation. We
particularly focus on the use of such models for the simulation of expression
data that can serve as a benchmark for the testing of network inference
algorithms.
",0,0,0,1,1,0
742,A levitated nanoparticle as a classical two-level atom,"  The center-of-mass motion of a single optically levitated nanoparticle
resembles three uncoupled harmonic oscillators. We show how a suitable
modulation of the optical trapping potential can give rise to a coupling
between two of these oscillators, such that their dynamics are governed by a
classical equation of motion that resembles the Schrödinger equation for a
two-level system. Based on experimental data, we illustrate the dynamics of
this parametrically coupled system both in the frequency and in the time
domain. We discuss the limitations and differences of the mechanical analogue
in comparison to a true quantum mechanical system.
",0,1,0,0,0,0
3963,Deep SNP: An End-to-end Deep Neural Network with Attention-based Localization for Break-point Detection in SNP Array Genomic data,"  Diagnosis and risk stratification of cancer and many other diseases require
the detection of genomic breakpoints as a prerequisite of calling copy number
alterations (CNA). This, however, is still challenging and requires
time-consuming manual curation. As deep-learning methods outperformed classical
state-of-the-art algorithms in various domains and have also been successfully
applied to life science problems including medicine and biology, we here
propose Deep SNP, a novel Deep Neural Network to learn from genomic data.
Specifically, we used a manually curated dataset from 12 genomic single
nucleotide polymorphism array (SNPa) profiles as truth-set and aimed at
predicting the presence or absence of genomic breakpoints, an indicator of
structural chromosomal variations, in windows of 40,000 probes. We compare our
results with well-known neural network models as well as Rawcopy though this
tool is designed to predict breakpoints and in addition genomic segments with
high sensitivity. We show, that Deep SNP is capable of successfully predicting
the presence or absence of a breakpoint in large genomic windows and
outperforms state-of-the-art neural network models. Qualitative examples
suggest that integration of a localization unit may enable breakpoint detection
and prediction of genomic segments, even if the breakpoint coordinates were not
provided for network training. These results warrant further evaluation of
DeepSNP for breakpoint localization and subsequent calling of genomic segments.
",0,0,0,0,1,0
654,Highly accurate model for prediction of lung nodule malignancy with CT scans,"  Computed tomography (CT) examinations are commonly used to predict lung
nodule malignancy in patients, which are shown to improve noninvasive early
diagnosis of lung cancer. It remains challenging for computational approaches
to achieve performance comparable to experienced radiologists. Here we present
NoduleX, a systematic approach to predict lung nodule malignancy from CT data,
based on deep learning convolutional neural networks (CNN). For training and
validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort.
All nodules were identified and classified by four experienced thoracic
radiologists who participated in the LIDC project. NoduleX achieves high
accuracy for nodule malignancy classification, with an AUC of ~0.99. This is
commensurate with the analysis of the dataset by experienced radiologists. Our
approach, NoduleX, provides an effective framework for highly accurate nodule
malignancy prediction with the model trained on a large patient population. Our
results are replicable with software available at
this http URL.
",0,0,0,1,1,0
4848,Non-equilibrium statistical mechanics of continuous attractors,"  Continuous attractors have been used to understand recent neuroscience
experiments where persistent activity patterns encode internal representations
of external attributes like head direction or spatial location. However, the
conditions under which the emergent bump of neural activity in such networks
can be manipulated by space and time-dependent external sensory or motor
signals are not understood. Here, we find fundamental limits on how rapidly
internal representations encoded along continuous attractors can be updated by
an external signal. We apply these results to place cell networks to derive a
velocity-dependent non-equilibrium memory capacity in neural networks.
",0,0,0,0,1,0
426,Gated Multimodal Units for Information Fusion,"  This paper presents a novel model for multimodal learning based on gated
neural networks. The Gated Multimodal Unit (GMU) model is intended to be used
as an internal unit in a neural network architecture whose purpose is to find
an intermediate representation based on a combination of data from different
modalities. The GMU learns to decide how modalities influence the activation of
the unit using multiplicative gates. It was evaluated on a multilabel scenario
for genre classification of movies using the plot and the poster. The GMU
improved the macro f-score performance of single-modality approaches and
outperformed other fusion strategies, including mixture of experts models.
Along with this work, the MM-IMDb dataset is released which, to the best of our
knowledge, is the largest publicly available multimodal dataset for genre
prediction on movies.
",0,0,0,1,0,0
402,Efficient Online Bandit Multiclass Learning with $\tilde{O}(\sqrt{T})$ Regret,"  We present an efficient second-order algorithm with
$\tilde{O}(\frac{1}{\eta}\sqrt{T})$ regret for the bandit online multiclass
problem. The regret bound holds simultaneously with respect to a family of loss
functions parameterized by $\eta$, for a range of $\eta$ restricted by the norm
of the competitor. The family of loss functions ranges from hinge loss
($\eta=0$) to squared hinge loss ($\eta=1$). This provides a solution to the
open problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for
$\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our
algorithm experimentally, showing that it also performs favorably against
earlier algorithms.
",0,0,0,1,0,0
248,Bayesian nonparametric inference for the M/G/1 queueing systems based on the marked departure process,"  In the present work we study Bayesian nonparametric inference for the
continuous-time M/G/1 queueing system. In the focus of the study is the
unobservable service time distribution. We assume that the only available data
of the system are the marked departure process of customers with the marks
being the queue lengths just after departure instants. These marks constitute
an embedded Markov chain whose distribution may be parametrized by stochastic
matrices of a special delta form. We develop the theory in order to obtain
integral mixtures of Markov measures with respect to suitable prior
distributions. We have found a sufficient statistic with a distribution of a
so-called S-structure sheding some new light on the inner statistical structure
of the M/G/1 queue. Moreover, it allows to update suitable prior distributions
to the posterior. Our inference methods are validated by large sample results
as posterior consistency and posterior normality.
",0,0,1,1,0,0
17325,Econometric modelling and forecasting of intraday electricity prices,"  In the following paper we analyse the ID$_3$-Price on German Intraday
Continuous Electricity Market using an econometric time series model. A
multivariate approach is conducted for hourly and quarter-hourly products
separately. We estimate the model using lasso and elastic net techniques and
perform an out-of-sample very short-term forecasting study. The model's
performance is compared with benchmark models and is discussed in detail.
Forecasting results provide new insights to the German Intraday Continuous
Electricity Market regarding its efficiency and to the ID$_3$-Price behaviour.
The supplementary materials are available online.
",0,0,0,0,0,1
1183,Mining Illegal Insider Trading of Stocks: A Proactive Approach,"  Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
",0,0,0,1,0,1
1896,Status maximization as a source of fairness in a networked dictator game,"  Human behavioural patterns exhibit selfish or competitive, as well as
selfless or altruistic tendencies, both of which have demonstrable effects on
human social and economic activity. In behavioural economics, such effects have
traditionally been illustrated experimentally via simple games like the
dictator and ultimatum games. Experiments with these games suggest that, beyond
rational economic thinking, human decision-making processes are influenced by
social preferences, such as an inclination to fairness. In this study we
suggest that the apparent gap between competitive and altruistic human
tendencies can be bridged by assuming that people are primarily maximising
their status, i.e., a utility function different from simple profit
maximisation. To this end we analyse a simple agent-based model, where
individuals play the repeated dictator game in a social network they can
modify. As model parameters we consider the living costs and the rate at which
agents forget infractions by others. We find that individual strategies used in
the game vary greatly, from selfish to selfless, and that both of the above
parameters determine when individuals form complex and cohesive social
networks.
",1,0,0,0,0,1
664,Cryptoasset Factor Models,"  We propose factor models for the cross-section of daily cryptoasset returns
and provide source code for data downloads, computing risk factors and
backtesting them out-of-sample. In ""cryptoassets"" we include all
cryptocurrencies and a host of various other digital assets (coins and tokens)
for which exchange market data is available. Based on our empirical analysis,
we identify the leading factor that appears to strongly contribute into daily
cryptoasset returns. Our results suggest that cross-sectional statistical
arbitrage trading may be possible for cryptoassets subject to efficient
executions and shorting.
",0,0,0,0,0,1
413,Oblivious Routing via Random Walks,"  We present novel oblivious routing algorithms for both splittable and
unsplittable multicommodity flow. Our algorithm for minimizing congestion for
\emph{unsplittable} multicommodity flow is the first oblivious routing
algorithm for this setting. As an intermediate step towards this algorithm, we
present a novel generalization of Valiant's classical load balancing scheme for
packet-switched networks to arbitrary graphs, which is of independent interest.
Our algorithm for minimizing congestion for \emph{splittable} multicommodity
flow improves upon the state-of-the-art, in terms of both running time and
performance, for graphs that exhibit good expansion guarantees. Our algorithms
rely on diffusing traffic via iterative applications of the random walk
operator. Consequently, the performance guarantees of our algorithms are
derived from the convergence of the random walk operator to the stationary
distribution and are expressed in terms of the spectral gap of the graph (which
dominates the mixing time).
",1,0,0,0,0,0
1728,Membrane Trafficking in the Yeast Saccharomyces cerevisiae Model,"  The yeast Saccharomyces cerevisiae is one of the best characterized
eukaryotic models. The secretory pathway was the first trafficking pathway
clearly understood mainly thanks to the work done in the laboratory of Randy
Schekman in the 1980s. They have isolated yeast sec mutants unable to secrete
an extracellular enzyme and these SEC genes were identified as encoding key
effectors of the secretory machinery. For this work, the 2013 Nobel Prize in
Physiology and Medicine has been awarded to Randy Schekman; the prize is shared
with James Rothman and Thomas S{ü}dhof. Here, we present the different
trafficking pathways of yeast S. cerevisiae. At the Golgi apparatus newly
synthesized proteins are sorted between those transported to the plasma
membrane (PM), or the external medium, via the exocytosis or secretory pathway
(SEC), and those targeted to the vacuole either through endosomes (vacuolar
protein sorting or VPS pathway) or directly (alkaline phosphatase or ALP
pathway). Plasma membrane proteins can be internalized by endocytosis (END) and
transported to endosomes where they are sorted between those targeted for
vacuolar degradation and those redirected to the Golgi (recycling or RCY
pathway). Studies in yeast S. cerevisiae allowed the identification of most of
the known effectors, protein complexes, and trafficking pathways in eukaryotic
cells, and most of them are conserved among eukaryotes.
",0,0,0,0,1,0
494,Constraints on Super-Earths Interiors from Stellar Abundances,"  Modeling the interior of exoplanets is essential to go further than the
conclusions provided by mean density measurements. In addition to the still
limited precision on the planets' fundamental parameters, models are limited by
the existence of degeneracies on their compositions. Here we present a model of
internal structure dedicated to the study of solid planets up to ~10 Earth
masses, i.e. Super-Earths. When the measurement is available, the assumption
that the bulk Fe/Si ratio of a planet is similar to that of its host star
allows us to significantly reduce the existing degeneracy and more precisely
constrain the planet's composition. Based on our model, we provide an update of
the mass-radius relationships used to provide a first estimate of a planet's
composition from density measurements. Our model is also applied to the cases
of two well-known exoplanets, CoRoT-7b and Kepler-10b, using their recently
updated parameters. The core mass fractions of CoRoT-7b and Kepler-10b are
found to lie within the 10-37% and 10-33% ranges, respectively, allowing both
planets to be compatible with an Earth-like composition. We also extend the
recent study of Proxima Centauri b, and show that its radius may reach 1.94
Earth radii in the case of a 5 Earth masses planet, as there is a 96.7%
probability that the real mass of Proxima Centauri b is below this value.
",0,1,0,0,0,0
756,Asynchronous Byzantine Machine Learning (the case of SGD),"  Asynchronous distributed machine learning solutions have proven very
effective so far, but always assuming perfectly functioning workers. In
practice, some of the workers can however exhibit Byzantine behavior, caused by
hardware failures, software bugs, corrupt data, or even malicious attacks. We
introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient
descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of
two complementary components: a filtering and a dampening component. The first
is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers.
Essentially, this filter leverages the Lipschitzness of cost functions and acts
as a self-stabilizer against Byzantine workers that would attempt to corrupt
the progress of SGD. The dampening component bounds the convergence rate by
adjusting to stale information through a generic gradient weighting scheme. We
prove that Kardam guarantees almost sure convergence in the presence of
asynchrony and Byzantine behavior, and we derive its convergence rate. We
evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead
with respect to non Byzantine-resilient solutions. We empirically show that
Kardam does not introduce additional noise to the learning procedure but does
induce a slowdown (the cost of Byzantine resilience) that we both theoretically
and empirically show to be less than $f/n$, where $f$ is the number of
Byzantine failures tolerated and $n$ the total number of workers.
Interestingly, we also empirically observe that the dampening component is
interesting in its own right for it enables to build an SGD algorithm that
outperforms alternative staleness-aware asynchronous competitors in
environments with honest workers.
",0,0,0,1,0,0
673,Higgs mode and its decay in a two dimensional antiferromagnet,"  Condensed-matter analogs of the Higgs boson in particle physics allow
insights into its behavior in different symmetries and dimensionalities.
Evidence for the Higgs mode has been reported in a number of different
settings, including ultracold atomic gases, disordered superconductors, and
dimerized quantum magnets. However, decay processes of the Higgs mode (which
are eminently important in particle physics) have not yet been studied in
condensed matter due to the lack of a suitable material system coupled to a
direct experimental probe. A quantitative understanding of these processes is
particularly important for low-dimensional systems where the Higgs mode decays
rapidly and has remained elusive to most experimental probes. Here, we discover
and study the Higgs mode in a two-dimensional antiferromagnet using
spin-polarized inelastic neutron scattering. Our spin-wave spectra of
Ca$_2$RuO$_4$ directly reveal a well-defined, dispersive Higgs mode, which
quickly decays into transverse Goldstone modes at the antiferromagnetic
ordering wavevector. Through a complete mapping of the transverse modes in the
reciprocal space, we uniquely specify the minimal model Hamiltonian and
describe the decay process. We thus establish a novel condensed matter platform
for research on the dynamics of the Higgs mode.
",0,1,0,0,0,0
11590,Geert Hofstede et al's set of national cultural dimensions - popularity and criticisms,"  This article outlines different stages in development of the national culture
model, created by Geert Hofstede and his affiliates. This paper reveals and
synthesizes the contemporary review of the application spheres of this
framework. Numerous applications of the dimensions set are used as a source of
identifying significant critiques, concerning different aspects in model's
operation. These critiques are classified and their underlying reasons are also
outlined by means of a fishbone diagram.
",0,0,0,0,0,1
2756,Recurrent Deep Embedding Networks for Genotype Clustering and Ethnicity Prediction,"  The understanding of variations in genome sequences assists us in identifying
people who are predisposed to common diseases, solving rare diseases, and
finding the corresponding population group of the individuals from a larger
population group. Although classical machine learning techniques allow
researchers to identify groups (i.e. clusters) of related variables, the
accuracy, and effectiveness of these methods diminish for large and
high-dimensional datasets such as the whole human genome. On the other hand,
deep neural network architectures (the core of deep learning) can better
exploit large-scale datasets to build complex models. In this paper, we use the
K-means clustering approach for scalable genomic data analysis aiming towards
clustering genotypic variants at the population scale. Finally, we train a deep
belief network (DBN) for predicting the geographic ethnicity. We used the
genotype data from the 1000 Genomes Project, which covers the result of genome
sequencing for 2504 individuals from 26 different ethnic origins and comprises
84 million variants. Our experimental results, with a focus on accuracy and
scalability, show the effectiveness and superiority compared to the
state-of-the-art.
",0,0,0,0,1,0
374,"L lines, C points and Chern numbers: understanding band structure topology using polarization fields","  Topology has appeared in different physical contexts. The most prominent
application is topologically protected edge transport in condensed matter
physics. The Chern number, the topological invariant of gapped Bloch
Hamiltonians, is an important quantity in this field. Another example of
topology, in polarization physics, are polarization singularities, called L
lines and C points. By establishing a connection between these two theories, we
develop a novel technique to visualize and potentially measure the Chern
number: it can be expressed either as the winding of the polarization azimuth
along L lines in reciprocal space, or in terms of the handedness and the index
of the C points. For mechanical systems, this is directly connected to the
visible motion patterns.
",0,1,0,0,0,0
2990,On smile properties of volatility derivatives and exotic products: understanding the VIX skew,"  We develop a method to study the implied volatility for exotic options and
volatility derivatives with European payoffs such as VIX options. Our approach,
based on Malliavin calculus techniques, allows us to describe the properties of
the at-the-money implied volatility (ATMI) in terms of the Malliavin
derivatives of the underlying process. More precisely, we study the short-time
behaviour of the ATMI level and skew. As an application, we describe the
short-term behavior of the ATMI of VIX and realized variance options in terms
of the Hurst parameter of the model, and most importantly we describe the class
of volatility processes that generate a positive skew for the VIX implied
volatility. In addition, we find that our ATMI asymptotic formulae perform very
well even for large maturities. Several numerical examples are provided to
support our theoretical results.
",0,0,0,0,0,1
905,A Simple Convolutional Generative Network for Next Item Recommendation,"  Convolutional Neural Networks (CNNs) have been recently introduced in the
domain of session-based next item recommendation. An ordered collection of past
items the user has interacted with in a session (or sequence) are embedded into
a 2-dimensional latent matrix, and treated as an image. The convolution and
pooling operations are then applied to the mapped item embeddings. In this
paper, we first examine the typical session-based CNN recommender and show that
both the generative model and network architecture are suboptimal when modeling
long-range dependencies in the item sequence. To address the issues, we
introduce a simple, but very effective generative model that is capable of
learning high-level representation from both short- and long-range item
dependencies. The network architecture of the proposed model is formed of a
stack of \emph{holed} convolutional layers, which can efficiently increase the
receptive fields without relying on the pooling operation. Another contribution
is the effective use of residual block structure in recommender systems, which
can ease the optimization for much deeper networks. The proposed generative
model attains state-of-the-art accuracy with less training time in the next
item recommendation task. It accordingly can be used as a powerful
recommendation baseline to beat in future, especially when there are long
sequences of user feedback.
",0,0,0,1,0,0
4206,Network-based protein structural classification,"  Experimental determination of protein function is resource-consuming. As an
alternative, computational prediction of protein function has received
attention. In this context, protein structural classification (PSC) can help,
by allowing for determining structural classes of currently unclassified
proteins based on their features, and then relying on the fact that proteins
with similar structures have similar functions. Existing PSC approaches rely on
sequence-based or direct (""raw"") 3-dimensional (3D) structure-based protein
features. In contrast, we first model 3D structures as protein structure
networks (PSNs). Then, we use (""processed"") network-based features for PSC. We
propose the use of graphlets, state-of-the-art features in many domains of
network science, in the task of PSC. Moreover, because graphlets can deal only
with unweighted PSNs, and because accounting for edge weights when constructing
PSNs could improve PSC accuracy, we also propose a deep learning framework that
automatically learns network features from the weighted PSNs. When evaluated on
a large set of ~9,400 CATH and ~12,800 SCOP protein domains (spanning 36 PSN
sets), our proposed approaches are superior to existing PSC approaches in terms
of accuracy, with comparable running time.
",0,0,0,1,1,0
728,"Precise measurement of hyperfine structure in the $ \rm {3\,S_{1/2}} $ state of $ \rm{^7Li} $","  We report a precise measurement of hyperfine structure in the $ \rm
{3\,S_{1/2}} $ state of the odd isotope of Li, namely $ \rm {^7Li} $. The state
is excited from the ground $ \rm {2\,S_{1/2}} $ state (which has the same
parity) using two single-photon transitions via the intermediate $ \rm
{2\,P_{3/2}} $ state. The value of the hyperfine constant we measure is $ A =
93.095(52)$ MHz, which resolves two discrepant values reported in the
literature measured using other techniques. Our value is also consistent with
theoretical calculations.
",0,1,0,0,0,0
287,Training Neural Networks Using Features Replay,"  Training a neural network using backpropagation algorithm requires passing
error gradients sequentially through the network. The backward locking prevents
us from updating network layers in parallel and fully leveraging the computing
resources. Recently, there are several works trying to decouple and parallelize
the backpropagation algorithm. However, all of them suffer from severe accuracy
loss or memory explosion when the neural network is deep. To address these
challenging issues, we propose a novel parallel-objective formulation for the
objective function of the neural network. After that, we introduce features
replay algorithm and prove that it is guaranteed to converge to critical points
for the non-convex problem under certain conditions. Finally, we apply our
method to training deep convolutional neural networks, and the experimental
results show that the proposed method achieves {faster} convergence, {lower}
memory consumption, and {better} generalization error than compared methods.
",0,0,0,1,0,0
900,Faster Clustering via Non-Backtracking Random Walks,"  This paper presents VEC-NBT, a variation on the unsupervised graph clustering
technique VEC, which improves upon the performance of the original algorithm
significantly for sparse graphs. VEC employs a novel application of the
state-of-the-art word2vec model to embed a graph in Euclidean space via random
walks on the nodes of the graph. In VEC-NBT, we modify the original algorithm
to use a non-backtracking random walk instead of the normal backtracking random
walk used in VEC. We introduce a modification to a non-backtracking random
walk, which we call a begrudgingly-backtracking random walk, and show
empirically that using this model of random walks for VEC-NBT requires shorter
walks on the graph to obtain results with comparable or greater accuracy than
VEC, especially for sparser graphs.
",1,0,0,1,0,0
2686,Model compression for faster structural separation of macromolecules captured by Cellular Electron Cryo-Tomography,"  Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule
structure inside single cells. Macromolecule classification approaches based on
convolutional neural networks (CNN) were developed to separate millions of
macromolecules captured from ECT systematically. However, given the fast
accumulation of ECT data, it will soon become necessary to use CNN models to
efficiently and accurately separate substantially more macromolecules at the
prediction stage, which requires additional computational costs. To speed up
the prediction, we compress classification models into compact neural networks
with little in accuracy for deployment. Specifically, we propose to perform
model compression through knowledge distillation. Firstly, a complex teacher
network is trained to generate soft labels with better classification
feasibility followed by training of customized student networks with simple
architectures using the soft label to compress model complexity. Our tests
demonstrate that our compressed models significantly reduce the number of
parameters and time cost while maintaining similar classification accuracy.
",0,0,0,1,1,0
2449,Methodological variations in lagged regression for detecting physiologic drug effects in EHR data,"  We studied how lagged linear regression can be used to detect the physiologic
effects of drugs from data in the electronic health record (EHR). We
systematically examined the effect of methodological variations ((i) time
series construction, (ii) temporal parameterization, (iii) intra-subject
normalization, (iv) differencing (lagged rates of change achieved by taking
differences between consecutive measurements), (v) explanatory variables, and
(vi) regression models) on performance of lagged linear methods in this
context. We generated two gold standards (one knowledge-base derived, one
expert-curated) for expected pairwise relationships between 7 drugs and 4 labs,
and evaluated how the 64 unique combinations of methodological perturbations
reproduce gold standards. Our 28 cohorts included patients in Columbia
University Medical Center/NewYork-Presbyterian Hospital clinical database. The
most accurate methods achieved AUROC of 0.794 for knowledge-base derived gold
standard (95%CI [0.741, 0.847]) and 0.705 for expert-curated gold standard (95%
CI [0.629, 0.781]). We observed a 0.633 mean AUROC (95%CI [0.610, 0.657],
expert-curated gold standard) across all methods that re-parameterize time
according to sequence and use either a joint autoregressive model with
differencing or an independent lag model without differencing. The complement
of this set of methods achieved a mean AUROC close to 0.5, indicating the
importance of these choices. We conclude that time- series analysis of EHR data
will likely rely on some of the beneficial pre-processing and modeling
methodologies identified, and will certainly benefit from continued careful
analysis of methodological perturbations. This study found that methodological
variations, such as pre-processing and representations, significantly affect
results, exposing the importance of evaluating these components when comparing
machine-learning methods.
",0,0,0,1,1,0
497,On the self-duality of rings of integers in tame and abelian extensions,"  Let $L/K$ be a tame and Galois extension of number fields with group $G$. It
is well-known that any ambiguous ideal in $L$ is locally free over
$\mathcal{O}_KG$ (of rank one), and so it defines a class in the locally free
class group of $\mathcal{O}_KG$, where $\mathcal{O}_K$ denotes the ring of
integers of $K$. In this paper, we shall study the relationship among the
classes arising from the ring of integers $\mathcal{O}_L$ of $L$, the inverse
different $\mathfrak{D}_{L/K}^{-1}$ of $L/K$, and the square root of the
inverse different $A_{L/K}$ of $L/K$ (if it exists), in the case that $G$ is
abelian. They are naturally related because $A_{L/K}^2 =
\mathfrak{D}_{L/K}^{-1} = \mathcal{O}_L^*$, and $A_{L/K}$ is special because
$A_{L/K} = A_{L/K}^*$, where $*$ denotes dual with respect to the trace of
$L/K$.
",0,0,1,0,0,0
9824,On approximations of Value at Risk and Expected Shortfall involving kurtosis,"  We derive new approximations for the Value at Risk and the Expected Shortfall
at high levels of loss distributions with positive skewness and excess
kurtosis, and we describe their precisions for notable ones such as for
exponential, Pareto type I, lognormal and compound (Poisson) distributions. Our
approximations are motivated by extensions of the so-called Normal Power
Approximation, used for approximating the cumulative distribution function of a
random variable, incorporating not only the skewness but the kurtosis of the
random variable in question as well. We show the performance of our
approximations in numerical examples and we also give comparisons with some
known ones in the literature.
",0,0,0,0,0,1
553,Anisotropic exchange and spin-wave damping in pure and electron-doped Sr$_2$IrO$_4$,"  The collective magnetic excitations in the spin-orbit Mott insulator
(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by
means of resonant inelastic x-ray scattering. We report significant magnon
energy gaps at both the crystallographic and antiferromagnetic zone centers at
all doping levels, along with a remarkably pronounced momentum-dependent
lifetime broadening. The spin-wave gap is accounted for by a significant
anisotropy in the interactions between $J_\text{eff}=1/2$ isospins, thus
marking the departure of Sr$_2$IrO$_4$ from the essentially isotropic
Heisenberg model appropriate for the superconducting cuprates.
",0,1,0,0,0,0
13751,"A dynamic network model with persistent links and node-specific latent variables, with an application to the interbank market","  We propose a dynamic network model where two mechanisms control the
probability of a link between two nodes: (i) the existence or absence of this
link in the past, and (ii) node-specific latent variables (dynamic fitnesses)
describing the propensity of each node to create links. Assuming a Markov
dynamics for both mechanisms, we propose an Expectation-Maximization algorithm
for model estimation and inference of the latent variables. The estimated
parameters and fitnesses can be used to forecast the presence of a link in the
future. We apply our methodology to the e-MID interbank network for which the
two linkage mechanisms are associated with two different trading behaviors in
the process of network formation, namely preferential trading and trading
driven by node-specific characteristics. The empirical results allow to
recognise preferential lending in the interbank market and indicate how a
method that does not account for time-varying network topologies tends to
overestimate preferential linkage.
",1,0,0,1,0,1
755,Properties of Kinetic Transition Networks for Atomic Clusters and Glassy Solids,"  A database of minima and transition states corresponds to a network where the
minima represent nodes and the transition states correspond to edges between
the pairs of minima they connect via steepest-descent paths. Here we construct
networks for small clusters bound by the Morse potential for a selection of
physically relevant parameters, in two and three dimensions. The properties of
these unweighted and undirected networks are analysed to examine two features:
whether they are small-world, where the shortest path between nodes involves
only a small number or edges; and whether they are scale-free, having a degree
distribution that follows a power law. Small-world character is present, but
statistical tests show that a power law is not a good fit, so the networks are
not scale-free. These results for clusters are compared with the corresponding
properties for the molecular and atomic structural glass formers
ortho-terphenyl and binary Lennard-Jones. These glassy systems do not show
small-world properties, suggesting that such behaviour is linked to the
structure-seeking landscapes of the Morse clusters.
",0,1,0,1,0,0
6340,Kinetic Trans-assembly of DNA Nanostructures,"  The central dogma of molecular biology is the principal framework for
understanding how nucleic acid information is propagated and used by living
systems to create complex biomolecules. Here, by integrating the structural and
dynamic paradigms of DNA nanotechnology, we present a rationally designed
synthetic platform which functions in an analogous manner to create complex DNA
nanostructures. Starting from one type of DNA nanostructure, DNA strand
displacement circuits were designed to interact and pass along the information
encoded in the initial structure to mediate the self-assembly of a different
type of structure, the final output structure depending on the type of circuit
triggered. Using this concept of a DNA structure ""trans-assembling"" a different
DNA structure through non-local strand displacement circuitry, four different
schemes were implemented. Specifically, 1D ladder and 2D double-crossover (DX)
lattices were designed to kinetically trigger DNA circuits to activate
polymerization of either ring structures or another type of DX lattice under
enzyme-free, isothermal conditions. In each scheme, the desired multilayer
reaction pathway was activated, among multiple possible pathways, ultimately
leading to the downstream self-assembly of the correct output structure.
",0,0,0,0,1,0
4004,Spreading of an infectious disease between different locations,"  The endogenous adaptation of agents, that may adjust their local contact
network in response to the risk of being infected, can have the perverse effect
of increasing the overall systemic infectiveness of a disease. We study a
dynamical model over two geographically distinct but interacting locations, to
better understand theoretically the mechanism at play. Moreover, we provide
empirical motivation from the Italian National Bovine Database, for the period
2006-2013.
",0,0,0,0,0,1
525,Proactive Edge Computing in Latency-Constrained Fog Networks,"  In this paper, the fundamental problem of distribution and proactive caching
of computing tasks in fog networks is studied under latency and reliability
constraints. In the proposed scenario, computing can be executed either locally
at the user device or offloaded to an edge cloudlet. Moreover, cloudlets
exploit both their computing and storage capabilities by proactively caching
popular task computation results to minimize computing latency. To this end, a
clustering method to group spatially proximate user devices with mutual task
popularity interests and their serving cloudlets is proposed. Then, cloudlets
can proactively cache the popular tasks' computations of their cluster members
to minimize computing latency. Additionally, the problem of distributing tasks
to cloudlets is formulated as a matching game in which a cost function of
computing delay is minimized under latency and reliability constraints.
Simulation results show that the proposed scheme guarantees reliable
computations with bounded latency and achieves up to 91% decrease in computing
latency as compared to baseline schemes.
",1,0,0,0,0,0
1878,The Wisdom of a Kalman Crowd,"  The Kalman Filter has been called one of the greatest inventions in
statistics during the 20th century. Its purpose is to measure the state of a
system by processing the noisy data received from different electronic sensors.
In comparison, a useful resource for managers in their effort to make the right
decisions is the wisdom of crowds. This phenomenon allows managers to combine
judgments by different employees to get estimates that are often more accurate
and reliable than estimates, which managers produce alone. Since harnessing the
collective intelligence of employees, and filtering signals from multiple noisy
sensors appear related, we looked at the possibility of using the Kalman Filter
on estimates by people. Our predictions suggest, and our findings based on the
Survey of Professional Forecasters reveal, that the Kalman Filter can help
managers solve their decision-making problems by giving them stronger signals
before they choose. Indeed, when used on a subset of forecasters identified by
the Contribution Weighted Model, the Kalman Filter beat that rule clearly,
across all the forecasting horizons in the survey.
",0,0,0,0,0,1
268,Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions,"  We experimentally confirmed the threshold behavior and scattering length
scaling law of the three-body loss coefficients in an ultracold spin-polarized
gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the
three-body loss coefficients as functions of temperature and scattering volume,
and found that the threshold law and the scattering length scaling law hold in
limited temperature and magnetic field regions. We also found that the
breakdown of the scaling laws is due to the emergence of the effective-range
term. This work is an important first step toward full understanding of the
loss of identical fermions with $p$-wave interactions.
",0,1,0,0,0,0
701,Mutual Interpretability of Robinson Arithmetic and Adjunctive Set Theory with Extensionality,"  An elementary rheory of concatenation is introduced and used to establish
mutual interpretability of Robinson arithmetic, Minimal Predicative Set Theory,
the quantifier-free part of Kirby's finitary set theory, and Adjunctive Set
Theory, with or without extensionality.
",0,0,1,0,0,0
409,Dynamical system analysis of dark energy models in scalar coupled metric-torsion theories,"  We study the phase space dynamics of cosmological models in the theoretical
formulations of non-minimal metric-torsion couplings with a scalar field, and
investigate in particular the critical points which yield stable solutions
exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is
defined in a way that it effectively has no direct interaction with the
cosmological fluid, although in an equivalent scalar-tensor cosmological setup
the scalar field interacts with the fluid (which we consider to be the
pressureless dust). Determining the conditions for the existence of the stable
critical points we check their physical viability, in both Einstein and Jordan
frames. We also verify that in either of these frames, the evolution of the
universe at the corresponding stable points matches with that given by the
respective exact solutions we have found in an earlier work (arXiv: 1611.00654
[gr-qc]). We not only examine the regions of physical relevance for the
trajectories in the phase space when the coupling parameter is varied, but also
demonstrate the evolution profiles of the cosmological parameters of interest
along fiducial trajectories in the effectively non-interacting scenarios, in
both Einstein and Jordan frames.
",0,1,0,0,0,0
11790,Distributions of Historic Market Data -- Implied and Realized Volatility,"  We undertake a systematic comparison between implied volatility, as
represented by VIX (new methodology) and VXO (old methodology), and realized
volatility. We compare visually and statistically distributions of realized and
implied variance (volatility squared) and study the distribution of their
ratio. We find that the ratio is best fitted by heavy-tailed -- lognormal and
fat-tailed (power-law) -- distributions, depending on whether preceding or
concurrent month of realized variance is used. We do not find substantial
difference in accuracy between VIX and VXO. Additionally, we study the variance
of theoretical realized variance for Heston and multiplicative models of
stochastic volatility and compare those with realized variance obtained from
historic market data.
",0,0,0,0,0,1
644,Deep Multi-User Reinforcement Learning for Distributed Dynamic Spectrum Access,"  We consider the problem of dynamic spectrum access for network utility
maximization in multichannel wireless networks. The shared bandwidth is divided
into K orthogonal channels. In the beginning of each time slot, each user
selects a channel and transmits a packet with a certain transmission
probability. After each time slot, each user that has transmitted a packet
receives a local observation indicating whether its packet was successfully
delivered or not (i.e., ACK signal). The objective is a multi-user strategy for
accessing the spectrum that maximizes a certain network utility in a
distributed manner without online coordination or message exchanges between
users. Obtaining an optimal solution for the spectrum access problem is
computationally expensive in general due to the large state space and partial
observability of the states. To tackle this problem, we develop a novel
distributed dynamic spectrum access algorithm based on deep multi-user
reinforcement leaning. Specifically, at each time slot, each user maps its
current state to spectrum access actions based on a trained deep-Q network used
to maximize the objective function. Game theoretic analysis of the system
dynamics is developed for establishing design principles for the implementation
of the algorithm. Experimental results demonstrate strong performance of the
algorithm.
",1,0,0,0,0,0
2367,Algorithmic Trading with Fitted Q Iteration and Heston Model,"  We present the use of the fitted Q iteration in algorithmic trading. We show
that the fitted Q iteration helps alleviate the dimension problem that the
basic Q-learning algorithm faces in application to trading. Furthermore, we
introduce a procedure including model fitting and data simulation to enrich
training data as the lack of data is often a problem in realistic application.
We experiment our method on both simulated environment that permits arbitrage
opportunity and real-world environment by using prices of 450 stocks. In the
former environment, the method performs well, implying that our method works in
theory. To perform well in the real-world environment, the agents trained might
require more training (iteration) and more meaningful variables with predictive
value.
",0,0,0,0,0,1
4408,A Biomechanical Study on the Use of Curved Drilling Technique for Treatment of Osteonecrosis of Femoral Head,"  Osteonecrosis occurs due to the loss of blood supply to the bone, leading to
spontaneous death of the trabecular bone. Delayed treatment of the involved
patients results in collapse of the femoral head, which leads to a need for
total hip arthroplasty surgery. Core decompression, as the most popular
technique for treatment of the osteonecrosis, includes removal of the lesion
area by drilling a straight tunnel to the lesion, debriding the dead bone and
replacing it with bone substitutes. However, there are two drawbacks for this
treatment method. First, due to the rigidity of the instruments currently used
during core decompression, lesions cannot be completely removed and/or
excessive healthy bone may also be removed with the lesion. Second, the use of
bone substitutes, despite its biocompatibility and osteoconductivity, may not
provide sufficient mechanical strength and support for the bone. To address
these shortcomings, a novel robot-assisted curved core decompression (CCD)
technique is introduced to provide surgeons with direct access to the lesions
causing minimal damage to the healthy bone. In this study, with the aid of
finite element (FE) simulations, we investigate biomechanical performance of
core decompression using the curved drilling technique in the presence of
normal gait loading. In this regard, we compare the result of the CCD using
bone substitutes and flexible implants with other conventional core
decompression techniques. The study finding shows that the maximum principal
stress occurring at the superior domain of the neck is smaller in the CCD
techniques (i.e. 52.847 MPa) compared to the other core decompression methods.
",0,0,0,0,1,0
2499,Deep Learning in Pharmacogenomics: From Gene Regulation to Patient Stratification,"  This Perspective provides examples of current and future applications of deep
learning in pharmacogenomics, including: (1) identification of novel regulatory
variants located in noncoding domains and their function as applied to
pharmacoepigenomics; (2) patient stratification from medical records; and (3)
prediction of drugs, targets, and their interactions. Deep learning
encapsulates a family of machine learning algorithms that over the last decade
has transformed many important subfields of artificial intelligence (AI) and
has demonstrated breakthrough performance improvements on a wide range of tasks
in biomedicine. We anticipate that in the future deep learning will be widely
used to predict personalized drug response and optimize medication selection
and dosing, using knowledge extracted from large and complex molecular,
epidemiological, clinical, and demographic datasets.
",0,0,0,1,1,0
2080,Linear compartmental models: input-output equations and operations that preserve identifiability,"  This work focuses on the question of how identifiability of a mathematical
model, that is, whether parameters can be recovered from data, is related to
identifiability of its submodels. We look specifically at linear compartmental
models and investigate when identifiability is preserved after adding or
removing model components. In particular, we examine whether identifiability is
preserved when an input, output, edge, or leak is added or deleted. Our
approach, via differential algebra, is to analyze specific input-output
equations of a model and the Jacobian of the associated coefficient map. We
clarify a prior determinantal formula for these equations, and then use it to
prove that, under some hypotheses, a model's input-output equations can be
understood in terms of certain submodels we call ""output-reachable"". Our proofs
use algebraic and combinatorial techniques.
",0,0,0,0,1,0
3932,New indicators for assessing the quality of in silico produced biomolecules: the case study of the aptamer-Angiopoietin-2 complex,"  Computational procedures to foresee the 3D structure of aptamers are in
continuous progress. They constitute a crucial input to research, mainly when
the crystallographic counterpart of the structures in silico produced is not
present. At now, many codes are able to perform structure and binding
prediction, although their ability in scoring the results remains rather weak.
In this paper, we propose a novel procedure to complement the ranking outcomes
of free docking code, by applying it to a set of anti-angiopoietin aptamers,
whose performances are known. We rank the in silico produced configurations,
adopting a maximum likelihood estimate, based on their topological and
electrical properties. From the analysis, two principal kinds of conformers are
identified, whose ability to mimick the binding features of the natural
receptor is discussed. The procedure is easily generalizable to many biological
biomolecules, useful for increasing chances of success in designing
high-specificity biosensors (aptasensors).
",0,0,0,0,1,0
652,Multi-hop assortativities for networks classification,"  Several social, medical, engineering and biological challenges rely on
discovering the functionality of networks from their structure and node
metadata, when it is available. For example, in chemoinformatics one might want
to detect whether a molecule is toxic based on structure and atomic types, or
discover the research field of a scientific collaboration network. Existing
techniques rely on counting or measuring structural patterns that are known to
show large variations from network to network, such as the number of triangles,
or the assortativity of node metadata. We introduce the concept of multi-hop
assortativity, that captures the similarity of the nodes situated at the
extremities of a randomly selected path of a given length. We show that
multi-hop assortativity unifies various existing concepts and offers a
versatile family of 'fingerprints' to characterize networks. These fingerprints
allow in turn to recover the functionalities of a network, with the help of the
machine learning toolbox. Our method is evaluated empirically on established
social and chemoinformatic network benchmarks. Results reveal that our
assortativity based features are competitive providing highly accurate results
often outperforming state of the art methods for the network classification
task.
",1,0,0,1,0,0
2409,Microfluidics for Chemical Synthesis: Flow Chemistry,"  Klavs F. Jensen is Warren K. Lewis Professor in Chemical Engineering and
Materials Science and Engineering at the Massachusetts Institute of Technology.
Here he describes the use of microfluidics for chemical synthesis, from the
early demonstration examples to the current efforts with automated droplet
microfluidic screening and optimization techniques.
",0,0,0,0,1,0
2173,Flux cost functions and the choice of metabolic fluxes,"  Metabolic fluxes in cells are governed by physical, biochemical,
physiological, and economic principles. Cells may show ""economical"" behaviour,
trading metabolic performance against the costly side-effects of high enzyme or
metabolite concentrations. Some constraint-based flux prediction methods score
fluxes by heuristic flux costs as proxies of enzyme investments. However,
linear cost functions ignore enzyme kinetics and the tight coupling between
fluxes, metabolite levels and enzyme levels. To derive more realistic cost
functions, I define an apparent ""enzymatic flux cost"" as the minimal enzyme
cost at which the fluxes can be realised in a given kinetic model, and a
""kinetic flux cost"", which includes metabolite cost. I discuss the mathematical
properties of such flux cost functions, their usage for flux prediction, and
their importance for cells' metabolic strategies. The enzymatic flux cost
scales linearly with the fluxes and is a concave function on the flux polytope.
The costs of two flows are usually not additive, due to an additional
""compromise cost"". Between flux polytopes, where fluxes change their
directions, the enzymatic cost shows a jump. With strictly concave flux cost
functions, cells can reduce their enzymatic cost by running different fluxes in
different cell compartments or at different moments in time. The enzymactic
flux cost can be translated into an approximated cell growth rate, a convex
function on the flux polytope. Growth-maximising metabolic states can be
predicted by Flux Cost Minimisation (FCM), a variant of FBA based on general
flux cost functions. The solutions are flux distributions in corners of the
flux polytope, i.e. typically elementary flux modes. Enzymatic flux costs can
be linearly or nonlinearly approximated, providing model parameters for linear
FBA based on kinetic parameters and extracellular concentrations, and justified
by a kinetic model.
",0,0,0,0,1,0
608,Counting Dominating Sets of Graphs,"  Counting dominating sets in a graph $G$ is closely related to the
neighborhood complex of $G$. We exploit this relation to prove that the number
of dominating sets $d(G)$ of a graph is determined by the number of complete
bipartite subgraphs of its complement. More precisely, we state the following.
Let $G$ be a simple graph of order $n$ such that its complement has exactly
$a(G)$ subgraphs isomorphic to $K_{2p,2q}$ and exactly $b(G)$ subgraphs
isomorphic to $K_{2p+1,2q+1}$. Then $d(G) = 2^n -1 + 2[a(G)-b(G)]$. We also
show some new relations between the domination polynomial and the neighborhood
polynomial of a graph.
",0,0,1,0,0,0
647,Scaling laws and bounds for the turbulent G.O. Roberts dynamo,"  Numerical simulations of the G.O. Roberts dynamo are presented. Dynamos both
with and without a significant mean field are obtained. Exact bounds are
derived for the total energy which conform with the Kolmogorov phenomenology of
turbulence. Best fits to numerical data show the same functional dependences as
the inequalities obtained from optimum theory.
",0,1,0,0,0,0
491,Handover analysis of the Improved Phantom Cells,"  Improved Phantom cell is a new scenario which has been introduced recently to
enhance the capacity of Heterogeneous Networks (HetNets). The main trait of
this scenario is that, besides maximizing the total network capacity in both
indoor and outdoor environments, it claims to reduce the handover number
compared to the conventional scenarios. In this paper, by a comprehensive
review of the Improved Phantom cells structure, an appropriate algorithm will
be introduced for the handover procedure of this scenario. To reduce the number
of handover in the proposed algorithm, various parameters such as the received
Signal to Interference plus Noise Ratio (SINR) at the user equipment (UE),
users access conditions to the phantom cells, and users staying time in the
target cell based on its velocity, has been considered. Theoretical analyses
and simulation results show that applying the suggested algorithm the improved
phantom cell structure has a much better performance than conventional HetNets
in terms of the number of handover.
",1,0,0,0,0,0
464,Fast Meta-Learning for Adaptive Hierarchical Classifier Design,"  We propose a new splitting criterion for a meta-learning approach to
multiclass classifier design that adaptively merges the classes into a
tree-structured hierarchy of increasingly difficult binary classification
problems. The classification tree is constructed from empirical estimates of
the Henze-Penrose bounds on the pairwise Bayes misclassification rates that
rank the binary subproblems in terms of difficulty of classification. The
proposed empirical estimates of the Bayes error rate are computed from the
minimal spanning tree (MST) of the samples from each pair of classes. Moreover,
a meta-learning technique is presented for quantifying the one-vs-rest Bayes
error rate for each individual class from a single MST on the entire dataset.
Extensive simulations on benchmark datasets show that the proposed hierarchical
method can often be learned much faster than competing methods, while achieving
competitive accuracy.
",1,0,0,1,0,0
6108,End-to-end distance and contour length distribution functions of DNA helices,"  We present a computational method to evaluate the end-to-end and the contour
length distribution functions of short DNA molecules described by a mesoscopic
Hamiltonian. The method generates a large statistical ensemble of possible
configurations for each dimer in the sequence, selects the global equilibrium
twist conformation for the molecule and determines the average base pair
distances along the molecule backbone. Integrating over the base pair radial
and angular fluctuations, we derive the room temperature distribution functions
as a function of the sequence length. The obtained values for the most probable
end-to-end distance and contour length distance, providing a measure of the
global molecule size, are used to examine the DNA flexibility at short length
scales. It is found that, also in molecules with less than $\sim 60$ base
pairs, coiled configurations maintain a large statistical weight and,
consistently, the persistence lengths may be much smaller than in kilo-base
DNA.
",0,0,0,0,1,0
651,Phonon-Induced Topological Transition to a Type-II Weyl Semimetal,"  Given the importance of crystal symmetry for the emergence of topological
quantum states, we have studied, as exemplified in NbNiTe2, the interplay of
crystal symmetry, atomic displacements (lattice vibration), band degeneracy,
and band topology. For NbNiTe2 structure in space group 53 (Pmna) - having an
inversion center arising from two glide planes and one mirror plane with a
2-fold rotation and screw axis - a full gap opening exists between two band
manifolds near the Fermi energy. Upon atomic displacements by optical phonons,
the symmetry lowers to space group 28 (Pma2), eliminating one glide plane along
c, the associated rotation and screw axis, and the inversion center. As a
result, twenty Weyl points emerge, including four type-II Weyl points in the
G-X direction at the boundary between a pair of adjacent electron and hole
bands. Thus, optical phonons may offer control of the transition to a Weyl
fermion state.
",0,1,0,0,0,0
10216,Trading algorithms with learning in latent alpha models,"  Alpha signals for statistical arbitrage strategies are often driven by latent
factors. This paper analyses how to optimally trade with latent factors that
cause prices to jump and diffuse. Moreover, we account for the effect of the
trader's actions on quoted prices and the prices they receive from trading.
Under fairly general assumptions, we demonstrate how the trader can learn the
posterior distribution over the latent states, and explicitly solve the latent
optimal trading problem. We provide a verification theorem, and a methodology
for calibrating the model by deriving a variation of the
expectation-maximization algorithm. To illustrate the efficacy of the optimal
strategy, we demonstrate its performance through simulations and compare it to
strategies which ignore learning in the latent factors. We also provide
calibration results for a particular model using Intel Corporation stock as an
example.
",0,0,0,1,0,1
899,Biocompatible Writing of Data into DNA,"  A simple DNA-based data storage scheme is demonstrated in which information
is written using ""addressing"" oligonucleotides. In contrast to other methods
that allow arbitrary code to be stored, the resulting DNA is suitable for
downstream enzymatic and biological processing. This capability is crucial for
DNA computers, and may allow for a diverse array of computational operations to
be carried out using this DNA. Although here we use gel-based methods for
information readout, we also propose more advanced methods involving
protein/DNA complexes and atomic force microscopy/nano-pore schemes for data
readout.
",1,1,0,0,0,0
16467,Reverse Quantum Annealing Approach to Portfolio Optimization Problems,"  We investigate a hybrid quantum-classical solution method to the
mean-variance portfolio optimization problems. Starting from real financial
data statistics and following the principles of the Modern Portfolio Theory, we
generate parametrized samples of portfolio optimization problems that can be
related to quadratic binary optimization forms programmable in the analog
D-Wave Quantum Annealer 2000Q. The instances are also solvable by an
industry-established Genetic Algorithm approach, which we use as a classical
benchmark. We investigate several options to run the quantum computation
optimally, ultimately discovering that the best results in terms of expected
time-to-solution as a function of number of variables for the hardest instances
set are obtained by seeding the quantum annealer with a solution candidate
found by a greedy local search and then performing a reverse annealing
protocol. The optimized reverse annealing protocol is found to be more than 100
times faster than the corresponding forward quantum annealing on average.
",0,0,0,0,0,1
639,Inverse Kinematics for Control of Tensegrity Soft Robots: Existence and Optimality of Solutions,"  Tension-network (`tensegrity') robots encounter many control challenges as
articulated soft robots, due to the structures' high-dimensional nonlinear
dynamics. Control approaches have been developed which use the inverse
kinematics of tensegrity structures, either for open-loop control or as
equilibrium inputs for closed-loop controllers. However, current formulations
of the tensegrity inverse kinematics problem are limited in robotics
applications: first, they can lead to higher than needed cable tensions, and
second, may lack solutions when applied to robots with high node-to-cable
ratios. This work provides progress in both directions. To address the first
limitation, the objective function for the inverse kinematics optimization
problem is modified to produce cable tensions as low or lower than before, thus
reducing the load on the robots' motors. For the second, a reformulation of the
static equilibrium constraint is proposed, which produces solutions independent
of the number of nodes within each rigid body. Simulation results using the
second reformulation on a specific tensegrity spine robot show reasonable
open-loop control results, whereas the previous formulation could not produce
any solution.
",1,0,0,0,0,0
247,From Natural to Artificial Camouflage: Components and Systems,"  We identify the components of bio-inspired artificial camouflage systems
including actuation, sensing, and distributed computation. After summarizing
recent results in understanding the physiology and system-level performance of
a variety of biological systems, we describe computational algorithms that can
generate similar patterns and have the potential for distributed
implementation. We find that the existing body of work predominately treats
component technology in an isolated manner that precludes a material-like
implementation that is scale-free and robust. We conclude with open research
challenges towards the realization of integrated camouflage solutions.
",1,0,0,0,1,0
7263,Using Stock Prices as Ground Truth in Sentiment Analysis to Generate Profitable Trading Signals,"  The increasing availability of ""big"" (large volume) social media data has
motivated a great deal of research in applying sentiment analysis to predict
the movement of prices within financial markets. Previous work in this field
investigates how the true sentiment of text (i.e. positive or negative
opinions) can be used for financial predictions, based on the assumption that
sentiments expressed online are representative of the true market sentiment.
Here we consider the converse idea, that using the stock price as the
ground-truth in the system may be a better indication of sentiment. Tweets are
labelled as Buy or Sell dependent on whether the stock price discussed rose or
fell over the following hour, and from this, stock-specific dictionaries are
built for individual companies. A Bayesian classifier is used to generate stock
predictions, which are input to an automated trading algorithm. Placing 468
trades over a 1 month period yields a return rate of 5.18%, which annualises to
approximately 83% per annum. This approach performs significantly better than
random chance and outperforms two baseline sentiment analysis methods tested.
",0,0,0,0,0,1
2399,Aggressive Economic Incentives and Physical Activity: The Role of Choice and Technology Decision Aids,"  Aggressive incentive schemes that allow individuals to impose economic
punishment on themselves if they fail to meet health goals present a promising
approach for encouraging healthier behavior. However, the element of choice
inherent in these schemes introduces concerns that only non-representative
sectors of the population will select aggressive incentives, leaving value on
the table for those who don't opt in. In a field experiment conducted over a 29
week period on individuals wearing Fitbit activity trackers, we find modest and
short lived increases in physical activity for those provided the choice of
aggressive incentives. In contrast, we find significant and persistent
increases for those assigned (oftentimes against their stated preference) to
the same aggressive incentives. The modest benefits for those provided a choice
seems to emerge because those who benefited most from the aggressive incentives
were the least likely to choose them, and it was those who did not need them
who opted in. These results are confirmed in a follow up lab experiment. We
also find that benefits to individuals assigned to aggressive incentives were
pronounced if they also updated their step target in the Fitbit mobile
application to match the new activity goal we provided them. Our findings have
important implications for incentive based interventions to improve health
behavior. For firms and policy makers, our results suggest that one effective
strategy for encouraging sustained healthy behavior combines exposure to
aggressive incentive schemes to jolt individuals out of their comfort zones
with technology decision aids that help individuals sustain this behavior after
incentives end.
",0,0,0,0,0,1
849,Macro diversity in Cellular Networks with Random Blockages,"  Blocking objects (blockages) between a transmitter and receiver cause
wireless communication links to transition from line-of-sight (LOS) to
non-line-of-sight (NLOS) propagation, which can greatly reduce the received
power, particularly at higher frequencies such as millimeter wave (mmWave). We
consider a cellular network in which a mobile user attempts to connect to two
or more base stations (BSs) simultaneously, to increase the probability of at
least one LOS link, which is a form of macrodiversity. We develop a framework
for determining the LOS probability as a function of the number of BSs, when
taking into account the correlation between blockages: for example, a single
blockage close to the device -- including the user's own body -- could block
multiple BSs. We consider the impact of the size of blocking objects on the
system reliability probability and show that macrodiversity gains are higher
when the blocking objects are small. We also show that the BS density must
scale as the square of the blockage density to maintain a given level of
reliability.
",1,0,1,0,0,0
204,Chain effects of clean water: The Mills-Reincke phenomenon in early twentieth-century Japan,"  This study explores the validity of chain effects of clean water, which are
known as the ""Mills-Reincke phenomenon,"" in early twentieth-century Japan.
Recent studies have reported that water purifications systems are responsible
for huge contributions to human capital. Although a few studies have
investigated the short-term effects of water-supply systems in pre-war Japan,
little is known about the benefits associated with these systems. By analyzing
city-level cause-specific mortality data from the years 1922-1940, we found
that eliminating typhoid fever infections decreased the risk of deaths due to
non-waterborne diseases. Our estimates show that for one additional typhoid
death, there were approximately one to three deaths due to other causes, such
as tuberculosis and pneumonia. This suggests that the observed Mills-Reincke
phenomenon could have resulted from the prevention typhoid fever in a
previously-developing Asian country.
",0,0,0,1,1,0
606,Superposition solutions to the extended KdV equation for water surface waves,"  The KdV equation can be derived in the shallow water limit of the Euler
equations. Over the last few decades, this equation has been extended to
include higher order effects. Although this equation has only one conservation
law, exact periodic and solitonic solutions exist. Khare and Saxena
\cite{KhSa,KhSa14,KhSa15} demonstrated the possibility of generating new exact
solutions by combining known ones for several fundamental equations (e.g.,
Korteweg - de Vries, Nonlinear Schrödinger). Here we find that this
construction can be repeated for higher order, non-integrable extensions of
these equations. Contrary to many statements in the literature, there seems to
be no correlation between integrability and the number of nonlinear one
variable wave solutions.
",0,1,0,0,0,0
472,Anisotropy and multiband superconductivity in Sr2RuO4,"  Despite numerous studies the exact nature of the order parameter in
superconducting Sr2RuO4 remains unresolved. We have extended previous
small-angle neutron scattering studies of the vortex lattice in this material
to a wider field range, higher temperatures, and with the field applied close
to both the <100> and <110> basal plane directions. Measurements at high field
were made possible by the use of both spin polarization and analysis to improve
the signal-to-noise ratio. Rotating the field towards the basal plane causes a
distortion of the square vortex lattice observed for H // <001>, and also a
symmetry change to a distorted triangular symmetry for fields close to <100>.
The vortex lattice distortion allows us to determine the intrinsic
superconducting anisotropy between the c-axis and the Ru-O basal plane,
yielding a value of ~60 at low temperature and low to intermediate fields. This
greatly exceeds the upper critical field anisotropy of ~20 at low temperature,
reminiscent of Pauli limiting. Indirect evidence for Pauli paramagnetic effects
on the unpaired quasiparticles in the vortex cores are observed, but a direct
detection lies below the measurement sensitivity. The superconducting
anisotropy is found to be independent of temperature but increases for fields >
1 T, indicating multiband superconductvity in Sr2RuO4. Finally, the temperature
dependence of the scattered intensity provides further support for gap nodes or
deep minima in the superconducting gap.
",0,1,0,0,0,0
6463,The Frequent Network Neighborhood Mapping of the Human Hippocampus Shows Much More Frequent Neighbor Sets in Males Than in Females,"  In the study of the human connectome, the vertices and the edges of the
network of the human brain are analyzed: the vertices of the graphs are the
anatomically identified gray matter areas of the subjects; this set is exactly
the same for all the subjects. The edges of the graphs correspond to the axonal
fibers, connecting these areas. In the biological applications of graph theory,
it happens very rarely that scientists examine numerous large graphs on the
very same, labeled vertex set. Exactly this is the case in the study of the
connectomes. Because of the particularity of these sets of graphs, novel,
robust methods need to be developed for their analysis. Here we introduce the
new method of the Frequent Network Neighborhood Mapping for the connectome,
which serves as a robust identification of the neighborhoods of given vertices
of special interest in the graph. We apply the novel method for mapping the
neighborhoods of the human hippocampus and discover strong statistical
asymmetries between the connectomes of the sexes, computed from the Human
Connectome Project. We analyze 413 braingraphs, each with 463 nodes. We show
that the hippocampi of men have much more significantly frequent neighbor sets
than women; therefore, in a sense, the connections of the hippocampi are more
regularly distributed in men and more varied in women. Our results are in
contrast to the volumetric studies of the human hippocampus, where it was shown
that the relative volume of the hippocampus is the same in men and women.
",0,0,0,0,1,0
545,A Generalized Framework for the Estimation of Causal Moderation Effects with Randomized Treatments and Non-Randomized Moderators,"  Researchers are often interested in analyzing conditional treatment effects.
One variant of this is ""causal moderation,"" which implies that intervention
upon a third (moderator) variable would alter the treatment effect. This study
presents a generalized, non-parametric framework for estimating causal
moderation effects given randomized treatments and non-randomized moderators
that achieves a number of goals. First, it highlights how conventional
approaches do not constitute unbiased or consistent estimators of causal
moderation effects. Second, it offers researchers a simple, transparent
approach for estimating causal moderation effects and lays out the assumptions
under which this can be performed consistently and/or without bias. Third, as
part of the estimation process, it allows researchers to implement their
preferred method of covariate adjustment, including parametric and
non-parametric methods, or alternative identification strategies of their
choosing. Fourth, it provides a set-up whereby sensitivity analysis designed
for the average-treatment-effect context can be extended to the moderation
context. An original application is also presented.
",0,0,0,1,0,0
1976,Dihedral angle prediction using generative adversarial networks,"  Several dihedral angles prediction methods were developed for protein
structure prediction and their other applications. However, distribution of
predicted angles would not be similar to that of real angles. To address this
we employed generative adversarial networks (GAN). Generative adversarial
networks are composed of two adversarially trained networks: a discriminator
and a generator. A discriminator distinguishes samples from a dataset and
generated samples while a generator generates realistic samples. Although the
discriminator of GANs is trained to estimate density, GAN model is intractable.
On the other hand, noise-contrastive estimation (NCE) was introduced to
estimate a normalization constant of an unnormalized statistical model and thus
the density function. In this thesis, we introduce noise-contrastive estimation
generative adversarial networks (NCE-GAN) which enables explicit density
estimation of a GAN model. And a new loss for the generator is proposed. We
also propose residue-wise variants of auxiliary classifier GAN (AC-GAN) and
Semi-supervised GAN to handle sequence information in a window. In our
experiment, the conditional generative adversarial network (C-GAN), AC-GAN and
Semi-supervised GAN were compared. And experiments done with improved
conditions were invested. We identified a phenomenon of AC-GAN that
distribution of its predicted angles is composed of unusual clusters. The
distribution of the predicted angles of Semi-supervised GAN was most similar to
the Ramachandran plot. We found that adding the output of the NCE as an
additional input of the discriminator is helpful to stabilize the training of
the GANs and to capture the detailed structures. Adding regression loss and
using predicted angles by regression loss only model could improve the
conditional generation performance of the C-GAN and AC-GAN.
",0,0,0,1,1,0
441,Enhancing the Spectral Hardening of Cosmic TeV Photons by Mixing with Axionlike Particles in the Magnetized Cosmic Web,"  Large-scale extragalactic magnetic fields may induce conversions between
very-high-energy photons and axionlike particles (ALPs), thereby shielding the
photons from absorption on the extragalactic background light. However, in
simplified ""cell"" models, used so far to represent extragalactic magnetic
fields, this mechanism would be strongly suppressed by current astrophysical
bounds. Here we consider a recent model of extragalactic magnetic fields
obtained from large-scale cosmological simulations. Such simulated magnetic
fields would have large enhancement in the filaments of matter. As a result,
photon-ALP conversions would produce a significant spectral hardening for
cosmic TeV photons. This effect would be probed with the upcoming Cherenkov
Telescope Array detector. This possible detection would give a unique chance to
perform a tomography of the magnetized cosmic web with ALPs.
",0,1,0,0,0,0
1819,Causal Mediation Analysis Leveraging Multiple Types of Summary Statistics Data,"  Summary statistics of genome-wide association studies (GWAS) teach causal
relationship between millions of genetic markers and tens and thousands of
phenotypes. However, underlying biological mechanisms are yet to be elucidated.
We can achieve necessary interpretation of GWAS in a causal mediation
framework, looking to establish a sparse set of mediators between genetic and
downstream variables, but there are several challenges. Unlike existing methods
rely on strong and unrealistic assumptions, we tackle practical challenges
within a principled summary-based causal inference framework. We analyzed the
proposed methods in extensive simulations generated from real-world genetic
data. We demonstrated only our approach can accurately redeem causal genes,
even without knowing actual individual-level data, despite the presence of
competing non-causal trails.
",1,0,0,1,1,0
573,Photo-Induced Bandgap Renormalization Governs the Ultrafast Response of Single-Layer MoS2,"  Transition metal dichalcogenides (TMDs) are emerging as promising
two-dimensional (2d) semiconductors for optoelectronic and flexible devices.
However, a microscopic explanation of their photophysics -- of pivotal
importance for the understanding and optimization of device operation -- is
still lacking. Here we use femtosecond transient absorption spectroscopy, with
pump pulse tunability and broadband probing, to monitor the relaxation dynamics
of single-layer MoS2 over the entire visible range, upon photoexcitation of
different excitonic transitions. We find that, irrespective of excitation
photon energy, the transient absorption spectrum shows the simultaneous
bleaching of all excitonic transitions and corresponding red-shifted
photoinduced absorption bands. First-principle modeling of the ultrafast
optical response reveals that a transient bandgap renormalization, caused by
the presence of photo-excited carriers, is primarily responsible for the
observed features. Our results demonstrate the strong impact of many-body
effects in the transient optical response of TMDs even in the
low-excitation-density regime.
",0,1,0,0,0,0
2558,Universal kinetics for engagement of mechanosensing pathways in cell adhesion,"  When plated onto substrates, cell morphology and even stem cell
differentiation are influenced by the stiffness of their environment. Stiffer
substrates give strongly spread (eventually polarized) cells with strong focal
adhesions, and stress fibers; very soft substrates give a less developed
cytoskeleton, and much lower cell spreading. The kinetics of this process of
cell spreading is studied extensively, and important universal relationships
are established on how the cell area grows with time. Here we study the
population dynamics of spreading cells, investigating the characteristic
processes involved in cell response to the substrate. We show that unlike the
individual cell morphology, this population dynamics does not depend on the
substrate stiffness. Instead, a strong activation temperature dependence is
observed. Different cell lines on different substrates all have long-time
statistics controlled by the thermal activation over a single energy barrier
dG=19 kcal/mol, while the early-time kinetics follows a power law $t^5$. This
implies that the rate of spreading depends on an internal process of
adhesion-mechanosensing complex assembly and activation: the operational
complex must have 5 component proteins, and the last process in the sequence
(which we believe is the activation of focal adhesion kinase) is controlled by
the binding energy dG.
",0,0,0,0,1,0
9260,A time change strategy to model reporting delay dynamics in claims reserving,"  This paper considers the problem of predicting the number of claims that have
already incurred in past exposure years, but which have not yet been reported
to the insurer. This is an important building block in the risk management
strategy of an insurer since the company should be able to fulfill its
liabilities with respect to such claims. Our approach puts emphasis on modeling
the time between the occurrence and reporting of claims, the so-called
reporting delay. Using data at a daily level we propose a micro-level model for
the heterogeneity in reporting delay caused by calendar day effects in the
reporting process, such as the weekday pattern and holidays. A simulation study
identifies the strengths and weaknesses of our approach in several scenarios
compared to traditional methods to predict the number of incurred but not
reported claims from aggregated data (i.e. the chain ladder method). We also
illustrate our model on a European general liability insurance data set and
conclude that the granular approach compared to the chain ladder method is more
robust with respect to volatility in the occurrence process. Our framework can
be extended to other predictive problems where interest goes to events that
incurred in the past but which are subject to an observation delay (e.g. the
number of infections during an epidemic).
",0,0,0,0,0,1
903,On the Consistency of Quick Shift,"  Quick Shift is a popular mode-seeking and clustering algorithm. We present
finite sample statistical consistency guarantees for Quick Shift on mode and
cluster recovery under mild distributional assumptions. We then apply our
results to construct a consistent modal regression algorithm.
",1,0,0,1,0,0
6730,"Cross-Sectional Variation of Intraday Liquidity, Cross-Impact, and their Effect on Portfolio Execution","  The composition of natural liquidity has been changing over time. An analysis
of intraday volumes for the S&P500 constituent stocks illustrates that (i)
volume surprises, i.e., deviations from their respective forecasts, are
correlated across stocks, and (ii) this correlation increases during the last
few hours of the trading session. These observations could be attributed, in
part, to the prevalence of portfolio trading activity that is implicit in the
growth of ETF, passive and systematic investment strategies; and, to the
increased trading intensity of such strategies towards the end of the trading
session, e.g., due to execution of mutual fund inflows/outflows that are
benchmarked to the closing price on each day. In this paper, we investigate the
consequences of such portfolio liquidity on price impact and portfolio
execution. We derive a linear cross-asset market impact from a stylized model
that explicitly captures the fact that a certain fraction of natural liquidity
providers only trade portfolios of stocks whenever they choose to execute. We
find that due to cross-impact and its intraday variation, it is optimal for a
risk-neutral, cost minimizing liquidator to execute a portfolio of orders in a
coupled manner, as opposed to a separable VWAP-like execution that is often
assumed. The optimal schedule couples the execution of the various orders so as
to be able to take advantage of increased portfolio liquidity towards the end
of the day. A worst case analysis shows that the potential cost reduction from
this optimized execution schedule over the separable approach can be as high as
6% for plausible model parameters. Finally, we discuss how to estimate
cross-sectional price impact if one had a dataset of realized portfolio
transaction records that exploits the low-rank structure of its coefficient
matrix suggested by our analysis.
",0,0,0,0,0,1
694,Unveiling the internal entanglement structure of the Kondo singlet,"  We disentangle all the individual degrees of freedom in the quantum impurity
problem to deconstruct the Kondo singlet, both in real and energy space, by
studying the contribution of each individual free electron eigenstate. This is
a problem of two spins coupled to a bath, where the bath is formed by the
remaining conduction electrons. Being a mixed state, we resort to the
""concurrence"" to quantify entanglement. We identify ""projected natural
orbitals"" that allow us to individualize a single-particle electronic wave
function that is responsible of more than $90\%$ of the impurity screening. In
the weak coupling regime, the impurity is entangled to an electron at the Fermi
level, while in the strong coupling regime, the impurity counterintuitively
entangles mostly with the high energy electrons and disentangles completely
from the low-energy states carving a ""hole"" around the Fermi level. This
enables one to use concurrence as a pseudo order parameter to compute the
characteristic ""size"" of the Kondo cloud, beyond which electrons are are weakly
correlated to the impurity and are dominated by the physics of the boundary.
",0,1,0,0,0,0
799,Stochastic Ratcheting on a Funneled Energy Landscape is Necessary for Highly Efficient Contractility of Actomyosin Force Dipoles,"  Current understanding of how contractility emerges in disordered actomyosin
networks of non-muscle cells is still largely based on the intuition derived
from earlier works on muscle contractility. This view, however, largely
overlooks the free energy gain following passive cross-linker binding, which,
even in the absence of active fluctuations, provides a thermodynamic drive
towards highly overlapping filamentous states. In this work, we shed light on
this phenomenon, showing that passive cross-linkers, when considered in the
context of two anti-parallel filaments, generate noticeable contractile forces.
However, as binding free energy of cross-linkers is increased, a sharp onset of
kinetic arrest follows, greatly diminishing effectiveness of this contractility
mechanism, allowing the network to contract only with weakly resisting tensions
at its boundary. We have carried out stochastic simulations elucidating this
mechanism, followed by a mean-field treatment that predicts how contractile
forces asymptotically scale at small and large binding energies, respectively.
Furthermore, when considering an active contractile filament pair, based on
non-muscle myosin II, we found that the non-processive nature of these motors
leads to highly inefficient force generation, due to recoil slippage of the
overlap during periods when the motor is dissociated. However, we discovered
that passive cross-linkers can serve as a structural ratchet during these
unbound motor time spans, resulting in vast force amplification. Our results
shed light on the non-equilibrium effects of transiently binding proteins in
biological active matter, as observed in the non-muscle actin cytoskeleton,
showing that highly efficient contractile force dipoles result from synergy of
passive cross-linker and active motor dynamics, via a ratcheting mechanism on a
funneled energy landscape.
",0,1,0,0,0,0
5072,Enhancing Blood Glucose Prediction with Meal Absorption and Physical Exercise Information,"  Objective: Numerous glucose prediction algorithm have been proposed to
empower type 1 diabetes (T1D) management. Most of these algorithms only account
for input such as glucose, insulin and carbohydrate, which limits their
performance. Here, we present a novel glucose prediction algorithm which, in
addition to standard inputs, accounts for meal absorption and physical exercise
information to enhance prediction accuracy. Methods: a compartmental model of
glucose-insulin dynamics combined with a deconvolution technique for state
estimation is employed for glucose prediction. In silico data corresponding
from the 10 adult subjects of UVa-Padova simulator, and clinical data from 10
adults with T1D were used. Finally, a comparison against a validated glucose
prediction algorithm based on a latent variable with exogenous input (LVX)
model is provided. Results: For a prediction horizon of 60 minutes, accounting
for meal absorption and physical exercise improved glucose forecasting
accuracy. In particular, root mean square error (mg/dL) went from 26.68 to
23.89, p<0.001 (in silico data); and from 37.02 to 35.96, p<0.001 (clinical
data - only meal information). Such improvement in accuracy was translated into
significant improvements on hypoglycaemia and hyperglycaemia prediction.
Finally, the performance of the proposed algorithm is statistically superior to
that of the LVX algorithm (26.68 vs. 32.80, p<0.001 (in silico data); 37.02 vs.
49.17, p<0.01 (clinical data). Conclusion: Taking into account meal absorption
and physical exercise information improves glucose prediction accuracy.
",1,0,0,0,1,0
283,New quantum mds constacylıc codes,"  This paper is devoted to the study of the construction of new quantum MDS
codes. Based on constacyclic codes over Fq2 , we derive four new families of
quantum MDS codes, one of which is an explicit generalization of the
construction given in Theorem 7 in [22]. We also extend the result of Theorem
3:3 given in [17].
",1,0,0,0,0,0
243,Bayesian Metabolic Flux Analysis reveals intracellular flux couplings,"  Metabolic flux balance analyses are a standard tool in analysing metabolic
reaction rates compatible with measurements, steady-state and the metabolic
reaction network stoichiometry. Flux analysis methods commonly place
unrealistic assumptions on fluxes due to the convenience of formulating the
problem as a linear programming model, and most methods ignore the notable
uncertainty in flux estimates. We introduce a novel paradigm of Bayesian
metabolic flux analysis that models the reactions of the whole genome-scale
cellular system in probabilistic terms, and can infer the full flux vector
distribution of genome-scale metabolic systems based on exchange and
intracellular (e.g. 13C) flux measurements, steady-state assumptions, and
target function assumptions. The Bayesian model couples all fluxes jointly
together in a simple truncated multivariate posterior distribution, which
reveals informative flux couplings. Our model is a plug-in replacement to
conventional metabolic balance methods, such as flux balance analysis (FBA).
Our experiments indicate that we can characterise the genome-scale flux
covariances, reveal flux couplings, and determine more intracellular unobserved
fluxes in C. acetobutylicum from 13C data than flux variability analysis. The
COBRA compatible software is available at github.com/markusheinonen/bamfa
",0,0,0,1,0,0
318,DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,"  Recent advances in adversarial Deep Learning (DL) have opened up a largely
unexplored surface for malicious attacks jeopardizing the integrity of
autonomous DL systems. With the wide-spread usage of DL in critical and
time-sensitive applications, including unmanned vehicles, drones, and video
surveillance systems, online detection of malicious inputs is of utmost
importance. We propose DeepFense, the first end-to-end automated framework that
simultaneously enables efficient and safe execution of DL models. DeepFense
formalizes the goal of thwarting adversarial attacks as an optimization problem
that minimizes the rarely observed regions in the latent feature space spanned
by a DL network. To solve the aforementioned minimization problem, a set of
complementary but disjoint modular redundancies are trained to validate the
legitimacy of the input samples in parallel with the victim DL model. DeepFense
leverages hardware/software/algorithm co-design and customized acceleration to
achieve just-in-time performance in resource-constrained settings. The proposed
countermeasure is unsupervised, meaning that no adversarial sample is leveraged
to train modular redundancies. We further provide an accompanying API to reduce
the non-recurring engineering cost and ensure automated adaptation to various
platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders
of magnitude performance improvement while enabling online adversarial sample
detection.
",1,0,0,1,0,0
368,On the $μ$-ordinary locus of a Shimura variety,"  In this paper, we study the $\mu$-ordinary locus of a Shimura variety with
parahoric level structure. Under the axioms in \cite{HR}, we show that
$\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport
strata introduced in \cite{HR} and we give criteria on the density of the
$\mu$-ordinary locus.
",0,0,1,0,0,0
5637,Optimal hedging under fast-varying stochastic volatility,"  In a market with a rough or Markovian mean-reverting stochastic volatility
there is no perfect hedge. Here it is shown how various delta-type hedging
strategies perform and can be evaluated in such markets. A precise
characterization of the hedging cost, the replication cost caused by the
volatility fluctuations, is presented in an asymptotic regime of rapid mean
reversion for the volatility fluctuations. The optimal dynamic asset based
hedging strategy in the considered regime is identified as the so-called
`practitioners' delta hedging scheme. It is moreover shown that the
performances of the delta-type hedging schemes are essentially independent of
the regularity of the volatility paths in the considered regime and that the
hedging costs are related to a vega risk martingale whose magnitude is
proportional to a new market risk parameter.
",0,0,0,0,0,1
724,Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM,"  The numerical availability of statistical inference methods for a modern and
robust analysis of longitudinal- and multivariate data in factorial experiments
is an essential element in research and education. While existing approaches
that rely on specific distributional assumptions of the data (multivariate
normality and/or characteristic covariance matrices) are implemented in
statistical software packages, there is a need for user-friendly software that
can be used for the analysis of data that do not fulfill the aforementioned
assumptions and provide accurate p-value and confidence interval estimates.
Therefore, newly developed statistical methods for the analysis of repeated
measures designs and multivariate data that neither assume multivariate
normality nor specific covariance matrices have been implemented in the freely
available R-package MANOVA.RM. The package is equipped with a graphical user
interface for plausible applications in academia and other educational purpose.
Several motivating examples illustrate the application of the methods.
",0,0,0,1,0,0
2198,Self-sustained activity in balanced networks with low firing-rate,"  The brain can display self-sustained activity (SSA), which is the persistent
firing of neurons in the absence of external stimuli. This spontaneous activity
shows low neuronal firing rates and is observed in diverse in vitro and in vivo
situations. In this work, we study the influence of excitatory/inhibitory
balance, connection density, and network size on the self-sustained activity of
a neuronal network model. We build a random network of adaptive exponential
integrate-and-fire (AdEx) neuron models connected through inhibitory and
excitatory chemical synapses. The AdEx model mimics several behaviours of
biological neurons, such as spike initiation, adaptation, and bursting
patterns. In an excitation/inhibition balanced state, if the mean connection
degree (K) is fixed, the firing rate does not depend on the network size (N),
whereas for fixed N, the firing rate decreases when K increases. However, for
large K, SSA states can appear only for large N. We show the existence of SSA
states with similar behaviours to those observed in experimental recordings,
such as very low and irregular neuronal firing rates, and spike-train power
spectra with slow fluctuations, only for balanced networks of large size.
",0,0,0,0,1,0
502,Multiset Combinatorial Batch Codes,"  Batch codes, first introduced by Ishai, Kushilevitz, Ostrovsky, and Sahai,
mimic a distributed storage of a set of $n$ data items on $m$ servers, in such
a way that any batch of $k$ data items can be retrieved by reading at most some
$t$ symbols from each server. Combinatorial batch codes, are replication-based
batch codes in which each server stores a subset of the data items.
In this paper, we propose a generalization of combinatorial batch codes,
called multiset combinatorial batch codes (MCBC), in which $n$ data items are
stored in $m$ servers, such that any multiset request of $k$ items, where any
item is requested at most $r$ times, can be retrieved by reading at most $t$
items from each server. The setup of this new family of codes is motivated by
recent work on codes which enable high availability and parallel reads in
distributed storage systems. The main problem under this paradigm is to
minimize the number of items stored in the servers, given the values of
$n,m,k,r,t$, which is denoted by $N(n,k,m,t;r)$. We first give a necessary and
sufficient condition for the existence of MCBCs. Then, we present several
bounds on $N(n,k,m,t;r)$ and constructions of MCBCs. In particular, we
determine the value of $N(n,k,m,1;r)$ for any $n\geq
\left\lfloor\frac{k-1}{r}\right\rfloor{m\choose k-1}-(m-k+1)A(m,4,k-2)$, where
$A(m,4,k-2)$ is the maximum size of a binary constant weight code of length
$m$, distance four and weight $k-2$. We also determine the exact value of
$N(n,k,m,1;r)$ when $r\in\{k,k-1\}$ or $k=m$.
",1,0,1,0,0,0
13368,Static vs Adaptive Strategies for Optimal Execution with Signals,"  We consider an optimal execution problem in which a trader is looking at a
short-term price predictive signal while trading. In the case where the trader
is creating an instantaneous market impact, we show that transactions costs
resulting from the optimal adaptive strategy are substantially lower than the
corresponding costs of the optimal static strategy. Later, we investigate the
case where the trader is creating transient market impact. We show that
strategies in which the trader is observing the signal a number of times during
the trading period, can dramatically reduce the transaction costs and improve
the performance of the optimal static strategy. These results answer a question
which was raised by Brigo and Piat [6], by analyzing two cases where adaptive
strategies can improve the performance of the execution.
",0,0,0,0,0,1
239,Morphological characterization of Ge ion implanted SiO2 matrix using multifractal technique,"  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy
were implanted into SiO2 matrix with Different fluences. The implanted samples
were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of
implanted as well as annealed samples were captured by the atomic force
microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation
analysis (MFDFA) based on the partition function approach has been used to
study the surfaces of ion implanted and annealed samples. The partition
function is used to calculate generalized Hurst exponent with the segment size.
Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with
the moment, thereby exhibiting the multifractal nature. The multifractality of
surface is pronounced after annealing for the surface implanted with fluence
7.5X1016 ions/cm^2.
",0,1,0,0,0,0
8656,"The QLBS Q-Learner Goes NuQLear: Fitted Q Iteration, Inverse RL, and Option Portfolios","  The QLBS model is a discrete-time option hedging and pricing model that is
based on Dynamic Programming (DP) and Reinforcement Learning (RL). It combines
the famous Q-Learning method for RL with the Black-Scholes (-Merton) model's
idea of reducing the problem of option pricing and hedging to the problem of
optimal rebalancing of a dynamic replicating portfolio for the option, which is
made of a stock and cash. Here we expand on several NuQLear (Numerical
Q-Learning) topics with the QLBS model. First, we investigate the performance
of Fitted Q Iteration for a RL (data-driven) solution to the model, and
benchmark it versus a DP (model-based) solution, as well as versus the BSM
model. Second, we develop an Inverse Reinforcement Learning (IRL) setting for
the model, where we only observe prices and actions (re-hedges) taken by a
trader, but not rewards. Third, we outline how the QLBS model can be used for
pricing portfolios of options, rather than a single option in isolation, thus
providing its own, data-driven and model independent solution to the (in)famous
volatility smile problem of the Black-Scholes model.
",0,0,0,0,0,1
16404,Option Pricing in Illiquid Markets with Jumps,"  The classical linear Black--Scholes model for pricing derivative securities
is a popular model in financial industry. It relies on several restrictive
assumptions such as completeness, and frictionless of the market as well as the
assumption on the underlying asset price dynamics following a geometric
Brownian motion. The main purpose of this paper is to generalize the classical
Black--Scholes model for pricing derivative securities by taking into account
feedback effects due to an influence of a large trader on the underlying asset
price dynamics exhibiting random jumps. The assumption that an investor can
trade large amounts of assets without affecting the underlying asset price
itself is usually not satisfied, especially in illiquid markets. We generalize
the Frey--Stremme nonlinear option pricing model for the case the underlying
asset follows a Levy stochastic process with jumps. We derive and analyze a
fully nonlinear parabolic partial-integro differential equation for the price
of the option contract. We propose a semi-implicit numerical discretization
scheme and perform various numerical experiments showing influence of a large
trader and intensity of jumps on the option price.
",0,0,0,0,0,1
382,An enhanced method to compute the similarity between concepts of ontology,"  With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring.
",1,0,0,0,0,0
560,Empirical analysis of non-linear activation functions for Deep Neural Networks in classification tasks,"  We provide an overview of several non-linear activation functions in a neural
network architecture that have proven successful in many machine learning
applications. We conduct an empirical analysis on the effectiveness of using
these function on the MNIST classification task, with the aim of clarifying
which functions produce the best results overall. Based on this first set of
results, we examine the effects of building deeper architectures with an
increasing number of hidden layers. We also survey the impact of using, on the
same task, different initialisation schemes for the weights of our neural
network. Using these sets of experiments as a base, we conclude by providing a
optimal neural network architecture that yields impressive results in accuracy
on the MNIST classification task.
",1,0,0,1,0,0
9449,Derivatives pricing using signature payoffs,"  We introduce signature payoffs, a family of path-dependent derivatives that
are given in terms of the signature of the price path of the underlying asset.
We show that these derivatives are dense in the space of continuous payoffs, a
result that is exploited to quickly price arbitrary continuous payoffs. This
approach to pricing derivatives is then tested with European options, American
options, Asian options, lookback options and variance swaps. As we show,
signature payoffs can be used to price these derivatives with very high
accuracy.
",0,0,0,0,0,1
3897,Capital Regulation under Price Impacts and Dynamic Financial Contagion,"  We construct a continuous time model for price-mediated contagion
precipitated by a common exogenous stress to the trading book of all firms in
the financial system. In this setting, firms are constrained so as to satisfy a
risk-weight based capital ratio requirement. We use this model to find
analytical bounds on the risk-weights for an asset as a function of the market
liquidity. Under these appropriate risk-weights, we find existence and
uniqueness for the joint system of firm behavior and the asset price. We
further consider an analytical bound on the firm liquidations, which allows us
to construct exact formulas for stress testing the financial system with
deterministic or random stresses. Numerical case studies are provided to
demonstrate various implications of this model and analytical bounds.
",0,0,0,0,0,1
5102,Age-at-harvest models as monitoring and harvest management tools for Wisconsin carnivores,"  Quantifying and estimating wildlife population sizes is a foundation of
wildlife management. However, many carnivore species are cryptic, leading to
innate difficulties in estimating their populations. We evaluated the potential
for using more rigorous statistical models to estimate the populations of black
bears (Ursus americanus) in Wisconisin, and their applicability to other
furbearers such as bobcats (Lynx rufus). To estimate black bear populations, we
developed an AAH state-space model in a Bayesian framework based on Norton
(2015) that can account for variation in harvest and population demographics
over time. Our state-space model created an accurate estimate of the black bear
population in Wisconsin based on age-at-harvest data and improves on previous
models by using little demographic data, no auxiliary data, and not being
sensitive to initial population size. The increased accuracy of the AAH
state-space models should allow for better management by being able to set
accurate quotas to ensure a sustainable harvest for the black bear population
in each zone. We also evaluated the demography and annual harvest data of
bobcats in Wisconsin to determine trends in harvest, method, and hunter
participation in Wisconsin. Each trait of harvested bobcats that we tested
(mass, male:female sex ratio, and age) changed over time, and because these
were interrelated, and we inferred that harvest selection for larger size
biased harvests in favor of a) larger, b) older, and c) male bobcats by hound
hunters. We also found an increase in the proportion of bobcats that were
harvested by hound hunting compared to trapping, and that hound hunters were
more likely to create taxidermy mounts than trappers. We also found that
decreasing available bobcat tags and increasing success have occurred over
time, and correlate with substantially increasing both hunter populations and
hunter interest.
",0,0,0,0,1,0
856,Moduli Spaces of Unordered $n\ge5$ Points on the Riemann Sphere and Their Singularities,"  For $n\ge5$, it is well known that the moduli space $\mathfrak{M_{0,\:n}}$ of
unordered $n$ points on the Riemann sphere is a quotient space of the Zariski
open set $K_n$ of $\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this
$S_n$ action at certain points of this Zariski open set $K_n$ correspond to the
groups fixing the sets of $n$ points on the Riemann sphere. Let $\alpha$ be a
subset of $n$ distinct points on the Riemann sphere. We call the group of all
linear fractional transformations leaving $\alpha$ invariant the stabilizer of
$\alpha$, which is finite by observation. For each non-trivial finite subgroup
$G$ of the group ${\rm PSL}(2,{\Bbb C})$ of linear fractional transformations,
we give the necessary and sufficient condition for finite subsets of the
Riemann sphere under which the stabilizers of them are conjugate to $G$. We
also prove that there does exist some finite subset of the Riemann sphere whose
stabilizer coincides with $G$. Next we obtain the irreducible decompositions of
the representations of the stabilizers on the tangent spaces at the
singularities of $\mathfrak{M_{0,\:n}}$. At last, on $\mathfrak{M_{0,\:5}}$ and
$\mathfrak{M_{0,\:6}}$, we work out explicitly the singularities and the
representations of their stabilizers on the tangent spaces at them.
",0,0,1,0,0,0
13531,Generalised Lyapunov Functions and Functionally Generated Trading Strategies,"  This paper investigates the dependence of functional portfolio generation,
introduced by Fernholz (1999), on an extra finite variation process. The
framework of Karatzas and Ruf (2017) is used to formulate conditions on trading
strategies to be strong arbitrage relative to the market over sufficiently
large time horizons. A mollification argument and Komlos theorem yield a
general class of potential arbitrage strategies. These theoretical results are
complemented by several empirical examples using data from the S&P 500 stocks.
",0,0,0,0,0,1
391,Lefschetz duality for intersection (co)homology,"  We prove the Lefschetz duality for intersection (co)homology in the framework
of $\partial$-pesudomanifolds. We work with general perversities and without
restriction on the coefficient ring.
",0,0,1,0,0,0
959,Boundedness of $\mathbb{Q}$-Fano varieties with degrees and alpha-invariants bounded from below,"  We show that $\mathbb{Q}$-Fano varieties of fixed dimension with
anti-canonical degrees and alpha-invariants bounded from below form a bounded
family. As a corollary, K-semistable $\mathbb{Q}$-Fano varieties of fixed
dimension with anti-canonical degrees bounded from below form a bounded family.
",0,0,1,0,0,0
640,Smallest eigenvalue density for regular or fixed-trace complex Wishart-Laguerre ensemble and entanglement in coupled kicked tops,"  The statistical behaviour of the smallest eigenvalue has important
implications for systems which can be modeled using a Wishart-Laguerre
ensemble, the regular one or the fixed trace one. For example, the density of
the smallest eigenvalue of the Wishart-Laguerre ensemble plays a crucial role
in characterizing multiple channel telecommunication systems. Similarly, in the
quantum entanglement problem, the smallest eigenvalue of the fixed trace
ensemble carries information regarding the nature of entanglement.
For real Wishart-Laguerre matrices, there exists an elegant recurrence scheme
suggested by Edelman to directly obtain the exact expression for the smallest
eigenvalue density. In the case of complex Wishart-Laguerre matrices, for
finding exact and explicit expressions for the smallest eigenvalue density,
existing results based on determinants become impractical when the determinants
involve large-size matrices. In this work, we derive a recurrence scheme for
the complex case which is analogous to that of Edelman's for the real case.
This is used to obtain exact results for the smallest eigenvalue density for
both the regular, and the fixed trace complex Wishart-Laguerre ensembles. We
validate our analytical results using Monte Carlo simulations. We also study
scaled Wishart-Laguerre ensemble and investigate its efficacy in approximating
the fixed-trace ensemble. Eventually, we apply our result for the fixed-trace
ensemble to investigate the behaviour of the smallest eigenvalue in the
paradigmatic system of coupled kicked tops.
",0,1,1,1,0,0
1820,Causal Queries from Observational Data in Biological Systems via Bayesian Networks: An Empirical Study in Small Networks,"  Biological networks are a very convenient modelling and visualisation tool to
discover knowledge from modern high-throughput genomics and postgenomics data
sets. Indeed, biological entities are not isolated, but are components of
complex multi-level systems. We go one step further and advocate for the
consideration of causal representations of the interactions in living
systems.We present the causal formalism and bring it out in the context of
biological networks, when the data is observational. We also discuss its
ability to decipher the causal information flow as observed in gene expression.
We also illustrate our exploration by experiments on small simulated networks
as well as on a real biological data set.
",0,0,0,1,1,0
695,A parallel orbital-updating based plane-wave basis method for electronic structure calculations,"  Motivated by the recently proposed parallel orbital-updating approach in real
space method, we propose a parallel orbital-updating based plane-wave basis
method for electronic structure calculations, for solving the corresponding
eigenvalue problems. In addition, we propose two new modified parallel
orbital-updating methods. Compared to the traditional plane-wave methods, our
methods allow for two-level parallelization, which is particularly interesting
for large scale parallelization. Numerical experiments show that these new
methods are more reliable and efficient for large scale calculations on modern
supercomputers
",0,1,1,0,0,0
9698,Discovering Bayesian Market Views for Intelligent Asset Allocation,"  Along with the advance of opinion mining techniques, public mood has been
found to be a key element for stock market prediction. However, how market
participants' behavior is affected by public mood has been rarely discussed.
Consequently, there has been little progress in leveraging public mood for the
asset allocation problem, which is preferred in a trusted and interpretable
way. In order to address the issue of incorporating public mood analyzed from
social media, we propose to formalize public mood into market views, because
market views can be integrated into the modern portfolio theory. In our
framework, the optimal market views will maximize returns in each period with a
Bayesian asset allocation model. We train two neural models to generate the
market views, and benchmark the model performance on other popular asset
allocation strategies. Our experimental results suggest that the formalization
of market views significantly increases the profitability (5% to 10% annually)
of the simulated portfolio at a given risk level.
",0,0,0,0,0,1
6955,Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler,"  In this work, we propose a model for estimating volatility from financial
time series, extending the non-Gaussian family of space-state models with exact
marginal likelihood proposed by Gamerman, Santos and Franco (2013). On the
literature there are models focused on estimating financial assets risk,
however, most of them rely on MCMC methods based on Metropolis algorithms,
since full conditional posterior distributions are not known. We present an
alternative model capable of estimating the volatility, in an automatic way,
since all full conditional posterior distributions are known, and it is
possible to obtain an exact sample of parameters via Gibbs Sampler. The
incorporation of jumps in returns allows the model to capture speculative
movements of the data, so that their influence does not propagate to
volatility. We evaluate the performance of the algorithm using synthetic and
real data time series.
Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,
Dynamic linear models.
",0,0,0,0,0,1
460,Axiomatic quantum mechanics: Necessity and benefits for the physics studies,"  The ongoing progress in quantum theory emphasizes the crucial role of the
very basic principles of quantum theory. However, this is not properly followed
in teaching quantum mechanics on the graduate and undergraduate levels of
physics studies. The existing textbooks typically avoid the axiomatic
presentation of the theory. We emphasize usefulness of the systematic,
axiomatic approach to the basics of quantum theory as well as its importance in
the light of the modern scientific-research context.
",0,1,0,0,0,0
1008,Oracle Importance Sampling for Stochastic Simulation Models,"  We consider the problem of estimating an expected outcome from a stochastic
simulation model using importance sampling. We propose a two-stage procedure
that involves a regression stage and a sampling stage to construct our
estimator. We introduce a parametric and a nonparametric regression estimator
in the first stage and study how the allocation between the two stages affects
the performance of final estimator. We derive the oracle property for both
approaches. We analyze the empirical performances of our approaches using two
simulated data and a case study on wind turbine reliability evaluation.
",0,0,0,1,0,0
697,Clarifying the Hubble constant tension with a Bayesian hierarchical model of the local distance ladder,"  Estimates of the Hubble constant, $H_0$, from the distance ladder and the
cosmic microwave background (CMB) differ at the $\sim$3-$\sigma$ level,
indicating a potential issue with the standard $\Lambda$CDM cosmology.
Interpreting this tension correctly requires a model comparison calculation
depending on not only the traditional `$n$-$\sigma$' mismatch but also the
tails of the likelihoods. Determining the form of the tails of the local $H_0$
likelihood is impossible with the standard Gaussian least-squares
approximation, as it requires using non-Gaussian distributions to faithfully
represent anchor likelihoods and model outliers in the Cepheid and supernova
(SN) populations, and simultaneous fitting of the full distance-ladder dataset
to correctly propagate uncertainties. We have developed a Bayesian hierarchical
model that describes the full distance ladder, from nearby geometric anchors
through Cepheids to Hubble-Flow SNe. This model does not rely on any
distributions being Gaussian, allowing outliers to be modeled and obviating the
need for arbitrary data cuts. Sampling from the $\sim$3000-parameter joint
posterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\pm$ 1.67)
${\rm km\,s^{-1}\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.
(2016) data, and ($73.15 \pm 1.78$) ${\rm km\,s^{-1}\,Mpc^{-1}}$ with SN
outliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the
distance-ladder likelihood allows us to apply Bayesian model comparison to
assess the evidence for deviation from $\Lambda$CDM. We set up this comparison
to yield a lower limit on the odds of the underlying model being $\Lambda$CDM
given the distance-ladder and Planck XIII (2016) CMB data. The odds against
$\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers
are cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)
likelihood is used.
",0,1,0,0,0,0
1536,Identification of Conduit Countries and Community Structures in the Withholding Tax Networks,"  Due to economic globalization, each country's economic law, including tax
laws and tax treaties, has been forced to work as a single network. However,
each jurisdiction (country or region) has not made its economic law under the
assumption that its law functions as an element of one network, so it has
brought unexpected results. We thought that the results are exactly
international tax avoidance. To contribute to the solution of international tax
avoidance, we tried to investigate which part of the network is vulnerable.
Specifically, focusing on treaty shopping, which is one of international tax
avoidance methods, we attempt to identified which jurisdiction are likely to be
used for treaty shopping from tax liabilities and the relationship between
jurisdictions which are likely to be used for treaty shopping and others. For
that purpose, based on withholding tax rates imposed on dividends, interest,
and royalties by jurisdictions, we produced weighted multiple directed graphs,
computed the centralities and detected the communities. As a result, we
clarified the jurisdictions that are likely to be used for treaty shopping and
pointed out that there are community structures. The results of this study
suggested that fewer jurisdictions need to introduce more regulations for
prevention of treaty abuse worldwide.
",0,0,0,0,0,1
718,Information Retrieval and Recommendation System for Astronomical Observatories,"  We present a machine learning based information retrieval system for
astronomical observatories that tries to address user defined queries related
to an instrument. In the modern instrumentation scenario where heterogeneous
systems and talents are simultaneously at work, the ability to supply with the
right information helps speeding up the detector maintenance operations.
Enhancing the detector uptime leads to increased coincidence observation and
improves the likelihood for the detection of astrophysical signals. Besides,
such efforts will efficiently disseminate technical knowledge to a wider
audience and will help the ongoing efforts to build upcoming detectors like the
LIGO-India etc even at the design phase to foresee possible challenges. The
proposed method analyses existing documented efforts at the site to
intelligently group together related information to a query and to present it
on-line to the user. The user in response can further go into interesting links
and find already developed solutions or probable ways to address the present
situation optimally. A web application that incorporates the above idea has
been implemented and tested for LIGO Livingston, LIGO Hanford and Virgo
observatories.
",0,1,0,0,0,0
870,A Game of Tax Evasion: evidences from an agent-based model,"  This paper presents a simple agent-based model of an economic system,
populated by agents playing different games according to their different view
about social cohesion and tax payment. After a first set of simulations,
correctly replicating results of existing literature, a wider analysis is
presented in order to study the effects of a dynamic-adaptation rule, in which
citizens may possibly decide to modify their individual tax compliance
according to individual criteria, such as, the strength of their ethical
commitment, the satisfaction gained by consumption of the public good and the
perceived opinion of neighbors. Results show the presence of thresholds levels
in the composition of society - between taxpayers and evaders - which explain
the extent of damages deriving from tax evasion.
",0,0,0,0,0,1
768,Dynamic Watermarking for General LTI Systems,"  Detecting attacks in control systems is an important aspect of designing
secure and resilient control systems. Recently, a dynamic watermarking approach
was proposed for detecting malicious sensor attacks for SISO LTI systems with
partial state observations and MIMO LTI systems with a full rank input matrix
and full state observations; however, these previous approaches cannot be
applied to general LTI systems that are MIMO and have partial state
observations. This paper designs a dynamic watermarking approach for detecting
malicious sensor attacks for general LTI systems, and we provide a new set of
asymptotic and statistical tests. We prove these tests can detect attacks that
follow a specified attack model (more general than replay attacks), and we also
show that these tests simplify to existing tests when the system is SISO or has
full rank input matrix and full state observations. The benefit of our approach
is demonstrated with a simulation analysis of detecting sensor attacks in
autonomous vehicles. Our approach can distinguish between sensor attacks and
wind disturbance (through an internal model principle framework), whereas
improperly designed tests cannot distinguish between sensor attacks and wind
disturbance.
",1,0,1,0,0,0
564,SimProp v2r4: Monte Carlo simulation code for UHECR propagation,"  We introduce the new version of SimProp, a Monte Carlo code for simulating
the propagation of ultra-high energy cosmic rays in intergalactic space. This
version, SimProp v2r4, together with an overall improvement of the code
capabilities with a substantial reduction in the computation time, also
computes secondary cosmogenic particles such as electron-positron pairs and
gamma rays produced during the propagation of ultra-high energy cosmic rays. As
recently pointed out by several authors, the flux of this secondary radiation
and its products, within reach of the current observatories, provides useful
information about models of ultra-high energy cosmic ray sources which would be
hard to discriminate otherwise.
",0,1,0,0,0,0
522,Fully Bayesian Estimation Under Informative Sampling,"  Bayesian estimation is increasingly popular for performing model based
inference to support policymaking. These data are often collected from surveys
under informative sampling designs where subject inclusion probabilities are
designed to be correlated with the response variable of interest. Sampling
weights constructed from marginal inclusion probabilities are typically used to
form an exponentiated pseudo likelihood that adjusts the population likelihood
for estimation on the sample due to ease-of-estimation. We propose an
alternative adjustment based on a Bayes rule construction that simultaneously
performs weight smoothing and estimates the population model parameters in a
fully Bayesian construction. We formulate conditions on known marginal and
pairwise inclusion probabilities that define a class of sampling designs where
$L_{1}$ consistency of the joint posterior is guaranteed. We compare
performances between the two approaches on synthetic data, which reveals that
our fully Bayesian approach better estimates posterior uncertainty without a
requirement to calibrate the normalization of the sampling weights. We
demonstrate our method on an application concerning the National Health and
Nutrition Examination Survey exploring the relationship between caffeine
consumption and systolic blood pressure.
",0,0,1,1,0,0
636,Eigendecompositions of Transfer Operators in Reproducing Kernel Hilbert Spaces,"  Transfer operators such as the Perron--Frobenius or Koopman operator play an
important role in the global analysis of complex dynamical systems. The
eigenfunctions of these operators can be used to detect metastable sets, to
project the dynamics onto the dominant slow processes, or to separate
superimposed signals. We extend transfer operator theory to reproducing kernel
Hilbert spaces and show that these operators are related to Hilbert space
representations of conditional distributions, known as conditional mean
embeddings in the machine learning community. Moreover, numerical methods to
compute empirical estimates of these embeddings are akin to data-driven methods
for the approximation of transfer operators such as extended dynamic mode
decomposition and its variants. One main benefit of the presented kernel-based
approaches is that these methods can be applied to any domain where a
similarity measure given by a kernel is available. We illustrate the results
with the aid of guiding examples and highlight potential applications in
molecular dynamics as well as video and text data analysis.
",1,0,0,1,0,0
208,Weak Form of Stokes-Dirac Structures and Geometric Discretization of Port-Hamiltonian Systems,"  We present the mixed Galerkin discretization of distributed parameter
port-Hamiltonian systems. On the prototypical example of hyperbolic systems of
two conservation laws in arbitrary spatial dimension, we derive the main
contributions: (i) A weak formulation of the underlying geometric
(Stokes-Dirac) structure with a segmented boundary according to the causality
of the boundary ports. (ii) The geometric approximation of the Stokes-Dirac
structure by a finite-dimensional Dirac structure is realized using a mixed
Galerkin approach and power-preserving linear maps, which define minimal
discrete power variables. (iii) With a consistent approximation of the
Hamiltonian, we obtain finite-dimensional port-Hamiltonian state space models.
By the degrees of freedom in the power-preserving maps, the resulting family of
structure-preserving schemes allows for trade-offs between centered
approximations and upwinding. We illustrate the method on the example of
Whitney finite elements on a 2D simplicial triangulation and compare the
eigenvalue approximation in 1D with a related approach.
",1,0,0,0,0,0
3076,Heterogeneous inputs to central pattern generators can shape insect gaits,"  In our previous work, we studied an interconnected bursting neuron model for
insect locomotion, and its corresponding phase oscillator model, which at high
speed can generate stable tripod gaits with three legs off the ground
simultaneously in swing, and at low speed can generate stable tetrapod gaits
with two legs off the ground simultaneously in swing. However, at low speed
several other stable locomotion patterns, that are not typically observed as
insect gaits, may coexist. In the present paper, by adding heterogeneous
external input to each oscillator, we modify the bursting neuron model so that
its corresponding phase oscillator model produces only one stable gait at each
speed, specifically: a unique stable tetrapod gait at low speed, a unique
stable tripod gait at high speed, and a unique branch of stable transition
gaits connecting them. This suggests that control signals originating in the
brain and central nervous system can modify gait patterns.
",0,0,0,0,1,0
17246,Portfolio diversification and model uncertainty: a robust dynamic mean-variance approach,"  This paper is concerned with a multi-asset mean-variance portfolio selection
problem under model uncertainty. We develop a continuous time framework for
taking into account ambiguity aversion about both expected return rates and
correlation matrix of the assets, and for studying the effects on portfolio
diversification. We prove a separation principle for the associated robust
control problem, which allows to reduce the determination of the optimal
dynamic strategy to the parametric computation of the minimal risk premium
function. Our results provide a justification for under-diversification, as
documented in empirical studies. We explicitly quantify the degree of
under-diversification in terms of correlation and Sharpe ratio ambiguity. In
particular, we show that an investor with a poor confidence in the expected
return estimation does not hold any risky asset, and on the other hand, trades
only one risky asset when the level of ambiguity on correlation matrix is
large. This extends to the continuous-time setting the results obtained by
Garlappi, Uppal and Wang [13], and Liu and Zeng [24] in a one-period model. JEL
Classification: G11, C61 MSC Classification: 91G10, 91G80, 60H30
",0,0,0,0,0,1
554,Anomalous transport properties in Nb/Bi1.95Sb0.05Se3 hybrid structure,"  We report the proximity induced anomalous transport behavior in a Nb
Bi1.95Sb0.05Se3 heterostructure. Mechanically Exfoliated single crystal of
Bi1.95Sb0.05Se3 topological insulator (TI) is partially covered with a 100 nm
thick Niobium superconductor using DC magnetron sputtering by shadow masking
technique. The magnetotransport (MR) measurements have been performed
simultaneously on the TI sample with and without Nb top layer in the
temperature,T, range of 3 to 8 K, and a magnetic field B up to 15 T. MR on TI
region shows Subnikov de Haas oscillation at fields greater than 5 T. Anomalous
linear change in resistance is observed in the field range of negative 4T to
positive 4T at which Nb is superconducting. At 0 T field, the temperature
dependence of resistance on the Nb covered region revealed a superconducting
transition (TC) at 8.2 K, whereas TI area showed similar TC with the absence of
zero resistance states due to the additional resistance from superconductor
(SC) TI interface. Interestingly below the TC the R vs T measured on TI showed
an enhancement in resistance for positive field and prominent fall in
resistance for negative field direction. This indicates the directional
dependent scattering of the Cooper pairs on the surface of the TI due to the
superposition of spin singlet and triplet states in the superconductor and TI
respectively.
",0,1,0,0,0,0
316,"Merge decompositions, two-sided Krohn-Rhodes, and aperiodic pointlikes","  This paper provides short proofs of two fundamental theorems of finite
semigroup theory whose previous proofs were significantly longer, namely the
two-sided Krohn-Rhodes decomposition theorem and Henckell's aperiodic pointlike
theorem, using a new algebraic technique that we call the merge decomposition.
A prototypical application of this technique decomposes a semigroup $T$ into a
two-sided semidirect product whose components are built from two subsemigroups
$T_1,T_2$, which together generate $T$, and the subsemigroup generated by their
setwise product $T_1T_2$. In this sense we decompose $T$ by merging the
subsemigroups $T_1$ and $T_2$. More generally, our technique merges semigroup
homomorphisms from free semigroups.
",1,0,1,0,0,0
615,"Tropical formulae for summation over a part of SL(2, Z)","  Let $f(a,b,c,d)=\sqrt{a^2+b^2}+\sqrt{c^2+d^2}-\sqrt{(a+c)^2+(b+d)^2}$, let
$(a,b,c,d)$ stand for $a,b,c,d\in\mathbb Z_{\geq 0}$ such that $ad-bc=1$.
Define \begin{equation} \label{eq_main} F(s) = \sum_{(a,b,c,d)} f(a,b,c,d)^s.
\end{equation} In other words, we consider the sum of the powers of the
triangle inequality defects for the lattice parallelograms (in the first
quadrant) of area one.
We prove that $F(s)$ converges when $s>1/2$ and diverges at $s=1/2$. We also
prove $$\sum\limits_{\substack{(a,b,c,d),\\ 1\leq a\leq b, 1\leq c\leq d}}
\frac{1}{(a+b)^2(c+d)^2(a+b+c+d)^2} = 1/24,$$ and show a general method to
obtain such formulae. The method comes from the consideration of the tropical
analogue of the caustic curves, whose moduli give a complete set of continuous
invariants on the space of convex domains.
",0,0,1,0,0,0
396,"Random Forests, Decision Trees, and Categorical Predictors: The ""Absent Levels"" Problem","  One advantage of decision tree based methods like random forests is their
ability to natively handle categorical predictors without having to first
transform them (e.g., by using feature engineering techniques). However, in
this paper, we show how this capability can lead to an inherent ""absent levels""
problem for decision tree based methods that has never been thoroughly
discussed, and whose consequences have never been carefully explored. This
problem occurs whenever there is an indeterminacy over how to handle an
observation that has reached a categorical split which was determined when the
observation in question's level was absent during training. Although these
incidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's
random forests FORTRAN code and the randomForest R package (Liaw and Wiener,
2002) as motivating case studies, we examine how overlooking the absent levels
problem can systematically bias a model. Furthermore, by using three real data
examples, we illustrate how absent levels can dramatically alter a model's
performance in practice, and we empirically demonstrate how some simple
heuristics can be used to help mitigate the effects of the absent levels
problem until a more robust theoretical solution is found.
",1,0,0,1,0,0
806,A Generalization of Permanent Inequalities and Applications in Counting and Optimization,"  A polynomial $p\in\mathbb{R}[z_1,\dots,z_n]$ is real stable if it has no
roots in the upper-half complex plane. Gurvits's permanent inequality gives a
lower bound on the coefficient of the $z_1z_2\dots z_n$ monomial of a real
stable polynomial $p$ with nonnegative coefficients. This fundamental
inequality has been used to attack several counting and optimization problems.
Here, we study a more general question: Given a stable multilinear polynomial
$p$ with nonnegative coefficients and a set of monomials $S$, we show that if
the polynomial obtained by summing up all monomials in $S$ is real stable, then
we can lowerbound the sum of coefficients of monomials of $p$ that are in $S$.
We also prove generalizations of this theorem to (real stable) polynomials that
are not multilinear. We use our theorem to give a new proof of Schrijver's
inequality on the number of perfect matchings of a regular bipartite graph,
generalize a recent result of Nikolov and Singh, and give deterministic
polynomial time approximation algorithms for several counting problems.
",1,0,1,0,0,0
6606,Investor Reaction to Financial Disclosures Across Topics: An Application of Latent Dirichlet Allocation,"  This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations.
",0,0,0,0,0,1
595,Nudging the particle filter,"  We investigate a new sampling scheme aimed at improving the performance of
particle filters whenever (a) there is a significant mismatch between the
assumed model dynamics and the actual system, or (b) the posterior probability
tends to concentrate in relatively small regions of the state space. The
proposed scheme pushes some particles towards specific regions where the
likelihood is expected to be high, an operation known as nudging in the
geophysics literature. We re-interpret nudging in a form applicable to any
particle filtering scheme, as it does not involve any changes in the rest of
the algorithm. Since the particles are modified, but the importance weights do
not account for this modification, the use of nudging leads to additional bias
in the resulting estimators. However, we prove analytically that nudged
particle filters can still attain asymptotic convergence with the same error
rates as conventional particle methods. Simple analysis also yields an
alternative interpretation of the nudging operation that explains its
robustness to model errors. Finally, we show numerical results that illustrate
the improvements that can be attained using the proposed scheme. In particular,
we present nonlinear tracking examples with synthetic data and a model
inference example using real-world financial data.
",0,0,0,1,0,0
2210,Extended Reduced-Form Framework for Non-Life Insurance,"  In this paper we propose a general framework for modeling an insurance
claims' information flow in continuous time, by generalizing the reduced-form
framework for credit risk and life insurance. In particular, we assume a
nontrivial dependence structure between the reference filtration and the
insurance internal filtration. We apply these results for pricing non-life
insurance liabilities in hybrid financial and insurance markets, while taking
into account the role of inflation under the benchmark approach. This framework
offers at the same time a general and flexible structure, and explicit and
treatable pricing formula.
",0,0,0,0,0,1
514,A Framework for Relating the Structures and Recovery Statistics in Pressure Time-Series Surveys for Dust Devils,"  Dust devils are likely the dominant source of dust for the martian
atmosphere, but the amount and frequency of dust-lifting depend on the
statistical distribution of dust devil parameters. Dust devils exhibit pressure
perturbations and, if they pass near a barometric sensor, they may register as
a discernible dip in a pressure time-series. Leveraging this fact, several
surveys using barometric sensors on landed spacecraft have revealed dust devil
structures and occurrence rates. However powerful they are, though, such
surveys suffer from non-trivial biases that skew the inferred dust devil
properties. For example, such surveys are most sensitive to dust devils with
the widest and deepest pressure profiles, but the recovered profiles will be
distorted, broader and shallow than the actual profiles. In addition, such
surveys often do not provide wind speed measurements alongside the pressure
time series, and so the durations of the dust devil signals in the time series
cannot be directly converted to profile widths. Fortunately, simple statistical
and geometric considerations can de-bias these surveys, allowing conversion of
the duration of dust devil signals into physical widths, given only a
distribution of likely translation velocities, and the recovery of the
underlying distributions of physical parameters. In this study, we develop a
scheme for de-biasing such surveys. Applying our model to an in-situ survey
using data from the Phoenix lander suggests a larger dust flux and a dust devil
occurrence rate about ten times larger than previously inferred. Comparing our
results to dust devil track surveys suggests only about one in five
low-pressure cells lifts sufficient dust to leave a visible track.
",0,1,0,0,0,0
726,Local Marchenko-Pastur Law for Random Bipartite Graphs,"  This paper is the first chapter of three of the author's undergraduate
thesis. We study the random matrix ensemble of covariance matrices arising from
random $(d_b, d_w)$-regular bipartite graphs on a set of $M$ black vertices and
$N$ white vertices, for $d_b \gg \log^4 N$. We simultaneously prove that the
Green's functions of these covariance matrices and the adjacency matrices of
the underlying graphs agree with the corresponding limiting law (e.g.
Marchenko-Pastur law for covariance matrices) down to the optimal scale. This
is an improvement from the previously known mesoscopic results. We obtain
eigenvector delocalization for the covariance matrix ensemble as consequence,
as well as a weak rigidity estimate.
",0,0,1,1,0,0
5259,Markov chain aggregation and its application to rule-based modelling,"  Rule-based modelling allows to represent molecular interactions in a compact
and natural way. The underlying molecular dynamics, by the laws of stochastic
chemical kinetics, behaves as a continuous-time Markov chain. However, this
Markov chain enumerates all possible reaction mixtures, rendering the analysis
of the chain computationally demanding and often prohibitive in practice. We
here describe how it is possible to efficiently find a smaller, aggregate
chain, which preserves certain properties of the original one. Formal methods
and lumpability notions are used to define algorithms for automated and
efficient construction of such smaller chains (without ever constructing the
original ones). We here illustrate the method on an example and we discuss the
applicability of the method in the context of modelling large signalling
pathways.
",1,0,0,0,1,0
5019,Network flow of mobile agents enhances the evolution of cooperation,"  We study the effect of contingent movement on the persistence of cooperation
on complex networks with empty nodes. Each agent plays Prisoner's Dilemma game
with its neighbors and then it either updates the strategy depending on the
payoff difference with neighbors or it moves to another empty node if not
satisfied with its own payoff. If no neighboring node is empty, each agent
stays at the same site. By extensive evolutionary simulations, we show that the
medium density of agents enhances cooperation where the network flow of mobile
agents is also medium. Moreover, if the movements of agents are more frequent
than the strategy updating, cooperation is further promoted. In scale-free
networks, the optimal density for cooperation is lower than other networks
because agents get stuck at hubs. Our study suggests that keeping a smooth
network flow is significant for the persistence of cooperation in ever-changing
societies.
",0,0,0,0,1,0
331,Contraction and uniform convergence of isotonic regression,"  We consider the problem of isotonic regression, where the underlying signal
$x$ is assumed to satisfy a monotonicity constraint, that is, $x$ lies in the
cone $\{ x\in\mathbb{R}^n : x_1 \leq \dots \leq x_n\}$. We study the isotonic
projection operator (projection to this cone), and find a necessary and
sufficient condition characterizing all norms with respect to which this
projection is contractive. This enables a simple and non-asymptotic analysis of
the convergence properties of isotonic regression, yielding uniform confidence
bands that adapt to the local Lipschitz properties of the signal.
",0,0,1,1,0,0
13150,"Contemporary facets of business successes among leading companies, operating in Bulgaria","  The current article unveils and analyzes some important factors, influencing
diversity in strategic decision-making approaches in local companies.
Researcher's attention is oriented to survey important characteristics of the
strategic moves, undertaken by leading companies in Bulgaria.
",0,0,0,0,0,1
5406,The self-referring DNA and protein: a remark on physical and geometrical aspects,"  All known life forms are based upon a hierarchy of interwoven feedback loops,
operating over a cascade of space, time and energy scales. Among the most basic
loops are those connecting DNA and proteins. For example, in genetic networks,
DNA genes are expressed as proteins, which may bind near the same genes and
thereby control their own expression. In this molecular type of self-reference,
information is mapped from the DNA sequence to the protein and back to DNA.
There is a variety of dynamic DNA-protein self-reference loops, and the purpose
of this remark is to discuss certain geometrical and physical aspects related
to the back and forth mapping between DNA and proteins. The discussion raises
basic questions regarding the nature of DNA and proteins as self-referring
matter, which are examined in a simple toy model.
",0,0,0,0,1,0
518,Topology optimization for transient response of structures subjected to dynamic loads,"  This paper presents a topology optimization framework for structural problems
subjected to transient loading. The mechanical model assumes a linear elastic
isotropic material, infinitesimal strains, and a dynamic response. The
optimization problem is solved using the gradient-based optimizer Method of
Moving Asymptotes (MMA) with time-dependent sensitivities provided via the
adjoint method. The stiffness of materials is interpolated using the Solid
Isotropic Material with Penalization (SIMP) method and the Heaviside Projection
Method (HPM) is used to stabilize the problem numerically and improve the
manufacturability of the topology-optimized designs. Both static and dynamic
optimization examples are considered here. The resulting optimized designs
demonstrate the ability of topology optimization to tailor the transient
response of structures.
",0,1,1,0,0,0
427,Why Condorcet Consistency is Essential,"  In a single winner election with several candidates and ranked choice or
rating scale ballots, a Condorcet winner is one who wins all their two way
races by majority rule or MR. A voting system has Condorcet consistency or CC
if it names any Condorcet winner the winner. Many voting systems lack CC, but a
three step line of reasoning is used here to show why it is necessary. In step
1 we show that we can dismiss all the electoral criteria which conflict with
CC. In step 2 we point out that CC follows almost automatically if we can agree
that MR is the only acceptable system for elections with two candidates. In
step 3 we make that argument for MR. This argument itself has three parts.
First, in races with two candidates, the only well known alternatives to MR can
sometimes name as winner a candidate who is preferred over their opponent by
only one voter, with all others preferring the opponent. That is unacceptable.
Second, those same systems are also extremely susceptible to strategic
insincere voting. Third, in simulation studies using spatial models with two
candidates, the best known alternative to MR picks the best or most centrist
candidate significantly less often than MR does.
",0,0,0,1,0,0
451,Convergence Analysis of the Dynamics of a Special Kind of Two-Layered Neural Networks with $\ell_1$ and $\ell_2$ Regularization,"  In this paper, we made an extension to the convergence analysis of the
dynamics of two-layered bias-free networks with one $ReLU$ output. We took into
consideration two popular regularization terms: the $\ell_1$ and $\ell_2$ norm
of the parameter vector $w$, and added it to the square loss function with
coefficient $\lambda/2$. We proved that when $\lambda$ is small, the weight
vector $w$ converges to the optimal solution $\hat{w}$ (with respect to the new
loss function) with probability $\geq (1-\varepsilon)(1-A_d)/2$ under random
initiations in a sphere centered at the origin, where $\varepsilon$ is a small
value and $A_d$ is a constant. Numerical experiments including phase diagrams
and repeated simulations verified our theory.
",1,0,0,1,0,0
637,Asymptotics of the bound state induced by $δ$-interaction supported on a weakly deformed plane,"  In this paper we consider the three-dimensional Schrödinger operator with
a $\delta$-interaction of strength $\alpha > 0$ supported on an unbounded
surface parametrized by the mapping $\mathbb{R}^2\ni x\mapsto (x,\beta f(x))$,
where $\beta \in [0,\infty)$ and $f\colon \mathbb{R}^2\rightarrow\mathbb{R}$,
$f\not\equiv 0$, is a $C^2$-smooth, compactly supported function. The surface
supporting the interaction can be viewed as a local deformation of the plane.
It is known that the essential spectrum of this Schrödinger operator
coincides with $[-\frac14\alpha^2,+\infty)$. We prove that for all sufficiently
small $\beta > 0$ its discrete spectrum is non-empty and consists of a unique
simple eigenvalue. Moreover, we obtain an asymptotic expansion of this
eigenvalue in the limit $\beta \rightarrow 0+$. In particular, this eigenvalue
tends to $-\frac14\alpha^2$ exponentially fast as $\beta\rightarrow 0+$.
",0,0,1,0,0,0
12448,A sparse grid approach to balance sheet risk measurement,"  In this work, we present a numerical method based on a sparse grid
approximation to compute the loss distribution of the balance sheet of a
financial or an insurance company. We first describe, in a stylised way, the
assets and liabilities dynamics that are used for the numerical estimation of
the balance sheet distribution. For the pricing and hedging model, we chose a
classical Black & Scholes model with a stochastic interest rate following a
Hull & White model. The risk management model describing the evolution of the
parameters of the pricing and hedging model is a Gaussian model. The new
numerical method is compared with the traditional nested simulation approach.
We review the convergence of both methods to estimate the risk indicators under
consideration. Finally, we provide numerical results showing that the sparse
grid approach is extremely competitive for models with moderate dimension.
",0,0,0,0,0,1
4873,Endogeneous Dynamics of Intraday Liquidity,"  In this paper we investigate the endogenous information contained in four
liquidity variables at a five minutes time scale on equity markets around the
world: the traded volume, the bid-ask spread, the volatility and the volume at
first limits of the orderbook. In the spirit of Granger causality, we measure
the level of information by the level of accuracy of linear autoregressive
models. This empirical study is carried out on a dataset of more than 300
stocks from four different markets (US, UK, Japan and Hong Kong) from a period
of over five years. We discuss the obtained performances of autoregressive (AR)
models on stationarized versions of the variables, focusing on explaining the
observed differences between stocks.
Since empirical studies are often conducted at this time scale, we believe it
is of paramount importance to document endogenous dynamics in a simple
framework with no addition of supplemental information. Our study can hence be
used as a benchmark to identify exogenous effects. On the other hand, most
optimal trading frameworks (like the celebrated Almgren and Chriss one), focus
on computing an optimal trading speed at a frequency close to the one we
consider. Such frameworks very often take i.i.d. assumptions on liquidity
variables; this paper document the auto-correlations emerging from real data,
opening the door to new developments in optimal trading.
",0,0,0,0,0,1
330,Playing Atari with Six Neurons,"  Deep reinforcement learning on Atari games maps pixel directly to actions;
internally, the deep neural network bears the responsibility of both extracting
useful information and making decisions based on it. Aiming at devoting entire
deep networks to decision making alone, we propose a new method for learning
policies and compact state representations separately but simultaneously for
policy approximation in reinforcement learning. State representations are
generated by a novel algorithm based on Vector Quantization and Sparse Coding,
trained online along with the network, and capable of growing its dictionary
size over time. We also introduce new techniques allowing both the neural
network and the evolution strategy to cope with varying dimensions. This
enables networks of only 6 to 18 neurons to learn to play a selection of Atari
games with performance comparable---and occasionally superior---to
state-of-the-art techniques using evolution strategies on deep networks two
orders of magnitude larger.
",0,0,0,1,0,0
674,Robust and Efficient Boosting Method using the Conditional Risk,"  Well-known for its simplicity and effectiveness in classification, AdaBoost,
however, suffers from overfitting when class-conditional distributions have
significant overlap. Moreover, it is very sensitive to noise that appears in
the labels. This article tackles the above limitations simultaneously via
optimizing a modified loss function (i.e., the conditional risk). The proposed
approach has the following two advantages. (1) It is able to directly take into
account label uncertainty with an associated label confidence. (2) It
introduces a ""trustworthiness"" measure on training samples via the Bayesian
risk rule, and hence the resulting classifier tends to have finite sample
performance that is superior to that of the original AdaBoost when there is a
large overlap between class conditional distributions. Theoretical properties
of the proposed method are investigated. Extensive experimental results using
synthetic data and real-world data sets from UCI machine learning repository
are provided. The empirical study shows the high competitiveness of the
proposed method in predication accuracy and robustness when compared with the
original AdaBoost and several existing robust AdaBoost algorithms.
",0,0,0,1,0,0
386,Topology of Large-Scale Structures of Galaxies in Two Dimensions - Systematic Effects,"  We study the two-dimensional topology of the galactic distribution when
projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
simulation data, we construct the genus of the two-dimensional field and
consider how this statistic is affected by late-time nonlinear effects --
principally gravitational collapse and redshift space distortion (RSD). We also
consider systematic and numerical artifacts such as shot noise, galaxy bias,
and finite pixel effects. We model the systematics using a Hermite polynomial
expansion and perform a comprehensive analysis of known effects on the
two-dimensional genus, with a view toward using the statistic for cosmological
parameter estimation. We find that the finite pixel effect is dominated by an
amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
$1/3$ of the angular smoothing length. Nonlinear gravitational evolution
introduces time-dependent coefficients of the zeroth, first, and second Hermite
polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
measured up to third order in the Hermite polynomial expansion when studying
RSD. Differences in shapes of the genus curves in real and redshift space are
small when we adopt thick redshift shells, but the amplitude change remains a
significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
biasing and shot noise produce systematic effects up to the second Hermite
polynomial. It is shown that, when sampling, the use of galaxy mass cuts
significantly reduces the effect of shot noise relative to random sampling.
",0,1,0,0,0,0
667,Ultra-high strain in epitaxial silicon carbide nanostructures utilizing residual stress amplification,"  Strain engineering has attracted great attention, particularly for epitaxial
films grown on a different substrate. Residual strains of SiC have been widely
employed to form ultra-high frequency and high Q factor resonators. However, to
date the highest residual strain of SiC was reported to be limited to
approximately 0.6%. Large strains induced into SiC could lead to several
interesting physical phenomena, as well as significant improvement of resonant
frequencies. We report an unprecedented nano strain-amplifier structure with an
ultra-high residual strain up to 8% utilizing the natural residual stress
between epitaxial 3C SiC and Si. In addition, the applied strain can be tuned
by changing the dimensions of the amplifier structure. The possibility of
introducing such a controllable and ultra-high strain will open the door to
investigating the physics of SiC in large strain regimes, and the development
of ultra sensitive mechanical sensors.
",0,1,0,0,0,0
597,ExSIS: Extended Sure Independence Screening for Ultrahigh-dimensional Linear Models,"  Statistical inference can be computationally prohibitive in
ultrahigh-dimensional linear models. Correlation-based variable screening, in
which one leverages marginal correlations for removal of irrelevant variables
from the model prior to statistical inference, can be used to overcome this
challenge. Prior works on correlation-based variable screening either impose
strong statistical priors on the linear model or assume specific post-screening
inference methods. This paper first extends the analysis of correlation-based
variable screening to arbitrary linear models and post-screening inference
techniques. In particular, ($i$) it shows that a condition---termed the
screening condition---is sufficient for successful correlation-based screening
of linear models, and ($ii$) it provides insights into the dependence of
marginal correlation-based screening on different problem parameters. Numerical
experiments confirm that these insights are not mere artifacts of analysis;
rather, they are reflective of the challenges associated with marginal
correlation-based variable screening. Second, the paper explicitly derives the
screening condition for two families of linear models, namely, sub-Gaussian
linear models and arbitrary (random or deterministic) linear models. In the
process, it establishes that---under appropriate conditions---it is possible to
reduce the dimension of an ultrahigh-dimensional, arbitrary linear model to
almost the sample size even when the number of active variables scales almost
linearly with the sample size.
",0,0,1,1,0,0
2910,"Risk-neutral valuation under differential funding costs, defaults and collateralization","  We develop a unified valuation theory that incorporates credit risk
(defaults), collateralization and funding costs, by expanding the replication
approach to a generality that has not yet been studied previously and reaching
valuation when replication is not assumed. This unifying theoretical framework
clarifies the relationship between the two valuation approaches: the adjusted
cash flows approach pioneered for example by Brigo, Pallavicini and co-authors
([12, 13, 34]) and the classic replication approach illustrated for example by
Bielecki and Rutkowski and co-authors ([3, 8]). In particular, results of this
work cover most previous papers where the authors studied specific replication
models.
",0,0,0,0,0,1
614,DSBGK Method to Incorporate the CLL Reflection Model and to Simulate Gas Mixtures,"  Molecular reflections on usual wall surfaces can be statistically described
by the Maxwell diffuse reflection model, which has been successfully applied in
the DSBGK simulations. We develop the DSBGK algorithm to implement the
Cercignani-Lampis-Lord (CLL) reflection model, which is widely applied to
polished surfaces and used particularly in modeling space shuttles to predict
the heat and force loads exerted by the high-speed flows around the surfaces.
We also extend the DSBGK method to simulate gas mixtures and high contrast of
number densities of different components can be handled at a cost of memory
usage much lower than that needed by the DSMC simulations because the average
numbers of simulated molecules of different components per cell can be equal in
the DSBGK simulations.
",0,1,0,0,0,0
431,Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector Machine Classifiers,"  This work proposes a new algorithm for training a re-weighted L2 Support
Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Candès
et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In
particular, the margin required for each training vector is set independently,
defining a new weighted SVM model. These weights are selected to be binary, and
they are automatically adapted during the training of the model, resulting in a
variation of the Frank-Wolfe optimization algorithm with essentially the same
computational complexity as the original algorithm. As shown experimentally,
this algorithm is computationally cheaper to apply since it requires less
iterations to converge, and it produces models with a sparser representation in
terms of support vectors and which are more stable with respect to the
selection of the regularization hyper-parameter.
",1,0,0,1,0,0
466,Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering,"  In this work, we present a methodology that enables an agent to make
efficient use of its exploratory actions by autonomously identifying possible
objectives in its environment and learning them in parallel. The identification
of objectives is achieved using an online and unsupervised adaptive clustering
algorithm. The identified objectives are learned (at least partially) in
parallel using Q-learning. Using a simulated agent and environment, it is shown
that the converged or partially converged value function weights resulting from
off-policy learning can be used to accumulate knowledge about multiple
objectives without any additional exploration. We claim that the proposed
approach could be useful in scenarios where the objectives are initially
unknown or in real world scenarios where exploration is typically a time and
energy intensive process. The implications and possible extensions of this work
are also briefly discussed.
",1,0,0,0,0,0
429,On generalizations of $p$-sets and their applications,"  The $p$-set, which is in a simple analytic form, is well distributed in unit
cubes. The well-known Weil's exponential sum theorem presents an upper bound of
the exponential sum over the $p$-set. Based on the result, one shows that the
$p$-set performs well in numerical integration, in compressed sensing as well
as in UQ. However, $p$-set is somewhat rigid since the cardinality of the
$p$-set is a prime $p$ and the set only depends on the prime number $p$. The
purpose of this paper is to present generalizations of $p$-sets, say
$\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$, which is more flexible.
Particularly, when a prime number $p$ is given, we have many different choices
of the new $p$-sets. Under the assumption that Goldbach conjecture holds, for
any even number $m$, we present a point set, say ${\mathcal L}_{p,q}$, with
cardinality $m-1$ by combining two different new $p$-sets, which overcomes a
major bottleneck of the $p$-set. We also present the upper bounds of the
exponential sums over $\mathcal{P}_{d,p}^{{\mathbf a},\epsilon}$ and ${\mathcal
L}_{p,q}$, which imply these sets have many potential applications.
",1,0,1,0,0,0
830,A Comprehensive Study of Ly$α$ Emission in the High-redshift Galaxy Population,"  We present an exhaustive census of Lyman alpha (Ly$\alpha$) emission in the
general galaxy population at $3<z<4.6$. We use the Michigan/Magellan Fiber
System (M2FS) spectrograph to study a stellar mass (M$_*$) selected sample of
625 galaxies homogeneously distributed in the range
$7.6<\log{\mbox{M$_*$/M$_{\odot}$}}<10.6$. Our sample is selected from the
3D-HST/CANDELS survey, which provides the complementary data to estimate
Ly$\alpha$ equivalent widths ($W_{Ly\alpha}$) and escape fractions ($f_{esc}$)
for our galaxies. We find both quantities to anti-correlate with M$_*$,
star-formation rate (SFR), UV luminosity, and UV slope ($\beta$). We then model
the $W_{Ly\alpha}$ distribution as a function of M$_{UV}$ and $\beta$ using a
Bayesian approach. Based on our model and matching the properties of typical
Lyman break galaxy (LBG) selections, we conclude that the $W_{Ly\alpha}$
distribution in such samples is heavily dependent on the limiting M$_{UV}$ of
the survey. Regarding narrowband surveys, we find their $W_{Ly\alpha}$
selections to bias samples toward low M$_*$, while their line-flux limitations
preferentially leave out low-SFR galaxies. We can also use our model to predict
the fraction of Ly$\alpha$-emitting LBGs at $4\leqslant z\leqslant 7$. We show
that reported drops in the Ly$\alpha$ fraction at $z\geqslant6$, usually
attributed to the rapidly increasing neutral gas fraction of the universe, can
also be explained by survey M$_{UV}$ incompleteness. This result does not
dismiss reionization occurring at $z\sim7$, but highlights that current data is
not inconsistent with this process taking place at $z>7$.
",0,1,0,0,0,0
222,"SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs","  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we
present new measurements of activity and magnetic field proxies of 442 low-mass
K5-M7 dwarfs. The objects were analysed as potential targets to search for
planetary-mass companions with the new spectropolarimeter and high-precision
velocimeter, SPIRou. We have analysed their high-resolution spectra in an
homogeneous way: circular polarisation, chromospheric features, and Zeeman
broadening of the FeH infrared line. The complex relationship between these
activity indicators is analysed: while no strong connection is found between
the large-scale and small-scale magnetic fields, the latter relates with the
non-thermal flux originating in the chromosphere.
We then examine the relationship between various activity diagnostics and the
optical radial-velocity jitter available in the literature, especially for
planet host stars. We use this to derive for all stars an activity merit
function (higher for quieter stars) with the goal of identifying the most
favorable stars where the radial-velocity jitter is low enough for planet
searches. We find that the main contributors to the RV jitter are the
large-scale magnetic field and the chromospheric non-thermal emission.
In addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed
along their rotation using the spectropolarimetric mode, and we derive their
magnetic topology. These very slow rotators are good representatives of future
SPIRou targets. They are compared to other stars where the magnetic topology is
also known. The poloidal component of the magnetic field is predominent in all
three stars.
",0,1,0,0,0,0
663,Multiscale Modeling of Shock Wave Localization in Porous Energetic Material,"  Shock wave interactions with defects, such as pores, are known to play a key
role in the chemical initiation of energetic materials. The shock response of
hexanitrostilbene is studied through a combination of large scale reactive
molecular dynamics and mesoscale hydrodynamic simulations. In order to extend
our simulation capability at the mesoscale to include weak shock conditions (<
6 GPa), atomistic simulations of pore collapse are used to define a strain rate
dependent strength model. Comparing these simulation methods allows us to
impose physically-reasonable constraints on the mesoscale model parameters. In
doing so, we have been able to study shock waves interacting with pores as a
function of this viscoplastic material response. We find that the pore collapse
behavior of weak shocks is characteristically different to that of strong
shocks.
",0,1,0,0,0,0
14554,Spatial risk measures induced by powers of max-stable random fields,"  A meticulous assessment of the risk of extreme environmental events is of
great necessity for populations, civil authorities as well as the
insurance/reinsurance industry. Koch (2017, 2018) introduced a concept of
spatial risk measure and a related set of axioms which are well-suited to
analyse and quantify the risk due to events having a spatial extent, precisely
such as natural disasters. In this paper, we first carry out a detailed study
of the correlation (and covariance) structure of powers of the Smith and
Brown-Resnick max-stable random fields. Then, using the latter results, we
thoroughly investigate spatial risk measures associated with variance and
induced by powers of max-stable random fields. In addition, we show that
spatial risk measures associated with several classical risk measures and
induced by such cost fields satisfy (at least) part of the previously mentioned
axioms under appropriate conditions on the max-stable fields. Considering such
cost fields is particularly relevant when studying the impact of extreme wind
speeds on buildings and infrastructure.
",0,0,0,0,0,1
