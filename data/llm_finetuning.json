[
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Recently, optional stopping has been a subject of debate in the Bayesian psychology community. Rouder (2014) argues that optional stopping is no problem for Bayesians, and even recommends the use of optional stopping in practice, as do Wagenmakers et al. (2012). This article addresses the question whether optional stopping is problematic for Bayesian methods, and specifies under which circumstances and in which sense it is and is not. By slightly varying and extending Rouder's (2014) experiment, we illustrate that, as soon as the parameters of interest are equipped with default or pragmatic priors - which means, in most practical applications of Bayes Factor hypothesis testing - resilience to optional stopping can break down. We distinguish between four types of default priors, each having their own specific issues with optional stopping, ranging from no-problem-at-all (Type 0 priors) to quite severe (Type II and III priors).",
        "output": "0"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "The sum of Log-normal variates is encountered in many challenging applications such as in performance analysis of wireless communication systems and in financial engineering. Several approximation methods have been developed in the literature, the accuracy of which is not ensured in the tail regions. These regions are of primordial interest wherein small probability values have to be evaluated with high precision. Variance reduction techniques are known to yield accurate, yet efficient, estimates of small probability values. Most of the existing approaches, however, have considered the problem of estimating the right-tail of the sum of Log-normal random variables (RVS). In the present work, we consider instead the estimation of the left-tail of the sum of correlated Log-normal variates with Gaussian copula under a mild assumption on the covariance matrix. We propose an estimator combining an existing mean-shifting importance sampling approach with a control variate technique. The main result is that the proposed estimator has an asymptotically vanishing relative error which represents a major finding in the context of the left-tail simulation of the sum of Log-normal RVs. Finally, we assess by various simulation results the performances of the proposed estimator compared to existing estimators.",
        "output": "0"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "We present a new approach for identifying situations and behaviours, which we call \"moves\", from soccer games in the 2D simulation league. Being able to identify key situations and behaviours are useful capabilities for analysing soccer matches, anticipating opponent behaviours to aid selection of appropriate tactics, and also as a prerequisite for automatic learning of behaviours and policies. To support a wide set of strategies, our goal is to identify situations from data, in an unsupervised way without making use of pre-defined soccer specific concepts such as \"pass\" or \"dribble\". The recurrent neural networks we use in our approach act as a high-dimensional projection of the recent history of a situation on the field. Similar situations, i.e., with similar histories, are found by clustering of network states. The same networks are also used to learn so-called conceptors, that are lower-dimensional manifolds that describe trajectories through a high-dimensional state space that enable situation-specific predictions from the same neural network. With the proposed approach, we can segment games into sequences of situations that are learnt in an unsupervised way, and learn conceptors that are useful for the prediction of the near future of the respective situation.",
        "output": "1"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Polycrystalline diamond coatings have been grown on cemented carbide substrates with different aspect ratios by a microwave plasma CVD in methane-hydrogen gas mixtures. To protect the edges of the substrates from non-uniform heating due to the plasma edge effect, a special plateholder with pockets for group growth has been used. The difference in heights of the substrates and plateholder, and its influence on the diamond film mean grain size, growth rate, phase composition and stress was investigated. The substrate temperature range, within which uniform diamond films are produced with good adhesion, is determined. The diamond-coated cutting inserts produced at optimized process exhibited a reduction of cutting force and wear resistance by a factor of two, and cutting efficiency increase by 4.3 times upon turning A390 Al-Si alloy as compared to performance of uncoated tools.",
        "output": "0"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Machine learning is finding increasingly broad application in the physical sciences. This most often involves building a model relationship between a dependent, measurable output and an associated set of controllable, but complicated, independent inputs. We present a tutorial on current techniques in machine learning -- a jumping-off point for interested researchers to advance their work. We focus on deep neural networks with an emphasis on demystifying deep learning. We begin with background ideas in machine learning and some example applications from current research in plasma physics. We discuss supervised learning techniques for modeling complicated functions, beginning with familiar regression schemes, then advancing to more sophisticated deep learning methods. We also address unsupervised learning and techniques for reducing the dimensionality of input spaces. Along the way, we describe methods for practitioners to help ensure that their models generalize from their training data to as-yet-unseen test data. We describe classes of tasks -- predicting scalars, handling images, fitting time-series -- and prepare the reader to choose an appropriate technique. We finally point out some limitations to modern machine learning and speculate on some ways that practitioners from the physical sciences may be particularly suited to help.",
        "output": "1"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Large inter-datacenter transfers are crucial for cloud service efficiency and are increasingly used by organizations that have dedicated wide area networks between datacenters. A recent work uses multicast forwarding trees to reduce the bandwidth needs and improve completion times of point-to-multipoint transfers. Using a single forwarding tree per transfer, however, leads to poor performance because the slowest receiver dictates the completion time for all receivers. Using multiple forwarding trees per transfer alleviates this concern--the average receiver could finish early; however, if done naively, bandwidth usage would also increase and it is apriori unclear how best to partition receivers, how to construct the multiple trees and how to determine the rate and schedule of flows on these trees. This paper presents QuickCast, a first solution to these problems. Using simulations on real-world network topologies, we see that QuickCast can speed up the average receiver's completion time by as much as $10\times$ while only using $1.04\times$ more bandwidth; further, the completion time for all receivers also improves by as much as $1.6\times$ faster at high loads.",
        "output": "1"
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "An aggregate data meta-analysis is a statistical method that pools the summary statistics of several selected studies to estimate the outcome of interest. When considering a continuous outcome, typically each study must report the same measure of the outcome variable and its spread (e.g., the sample mean and its standard error). However, some studies may instead report the median along with various measures of spread. Recently, the task of incorporating medians in meta-analysis has been achieved by estimating the sample mean and its standard error from each study that reports a median in order to meta-analyze the means. In this paper, we propose two alternative approaches to meta-analyze data that instead rely on medians. We systematically compare these approaches via simulation study to each other and to methods that transform the study-specific medians and spread into sample means and their standard errors. We demonstrate that the proposed median-based approaches perform better than the transformation-based approaches, especially when applied to skewed data and data with high inter-study variance. In addition, when meta-analyzing data that consists of medians, we show that the median-based approaches perform considerably better than or comparably to the best-case scenario for a transformation approach: conducting a meta-analysis using the actual sample mean and standard error of the mean of each study. Finally, we illustrate these approaches in a meta-analysis of patient delay in tuberculosis diagnosis.",
        "output": "0"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "We study the U.S. Operations Research/Industrial-Systems Engineering (ORIE) faculty hiring network, consisting of 1,179 faculty origin and destination data together with attribute data from 83 ORIE departments. A social network analysis of faculty hires can reveal important patterns in an academic field, such as the existence of a hierarchy or sociological aspects such as the presence of communities of departments. We first statistically test for the existence of a linear hierarchy in the network and for its steepness. We find a near linear hierarchical order of the departments, proposing a new index for hiring networks, which we contrast with other indicators of hierarchy, including published rankings. A single index is not capable to capture the full structure of a complex network, however, so we next fit a latent exponential random graph model (ERGM) to the network, which is able to reproduce its main observed characteristics: high incidence of self-hiring, skewed out-degree distribution, low density and clustering. Finally, we use the latent variables in the ERGM to simplify the network to one where faculty hires take place among three groups of departments. We contrast our findings with those reported for other related disciplines, Computer Science and Business.",
        "output": "1"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Inference amortization methods share information across multiple posterior-inference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties: (a) they do not encode any independence assertions that are absent from the model and; (b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches.",
        "output": "1"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Features and applications of quasi-spherical settling accretion onto rotating magnetized neutron stars in high-mass X-ray binaries are discussed. The settling accretion occurs in wind-fed HMXBs when the plasma cooling time is longer than the free-fall time from the gravitational capture radius, which can take place in low-luminosity HMXBs with $L_x\\lesssim 4\times 10^{36}$ erg/s. We briefly review the implications of the settling accretion, focusing on the SFXT phenomenon, which can be related to instability of the quasi-spherical convective shell above the neutron star magnetosphere due to magnetic reconnection from fast temporarily magnetized winds from OB-supergiant. If a young neutron star in a wind-fed HMXB is rapidly rotating, the propeller regime in a quasi-spherical hot shell occurs. We show that X-ray spectral and temporal properties of enigmatic $\\gamma$ Cas Be-stars are consistent with failed settling accretion regime onto a propelling neutron star. The subsequent evolutionary stage of $\\gamma$ Cas and its analogs should be the X Per-type binaries comprising low-luminosity slowly rotating X-ray pulsars.",
        "output": "0"      
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "We present numerical evidence that most two-dimensional surface states of a bulk topological superconductor (TSC) sit at an integer quantum Hall plateau transition. We study TSC surface states in class CI with quenched disorder. Low-energy (finite-energy) surface states were expected to be critically delocalized (Anderson localized). We confirm the low-energy picture, but find instead that finite-energy states are also delocalized, with universal statistics that are independent of the TSC winding number, and consistent with the spin quantum Hall plateau transition (percolation).",
        "output": "0"
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "In this paper, we propose a new algorithm based on radial symmetry center method to track colloidal particles close to contact, where the optical images of the particles start to overlap in digital video microscopy. This overlapping effect is important to observe the pair interaction potential in colloidal studies and it appears as additional interaction in the measurement of the interaction with conventional tracking analysis. The proposed algorithm in this work is simple, fast and applicable for not only two particles but also three and more particles without any modification. The algorithm uses gradient vectors of the particle intensity distribution, which allows us to use a part of the symmetric intensity distribution in the calculation of the actual particle position. In this study, simulations are performed to see the performance of the proposed algorithm for two and three particles, where the simulation images are generated by using fitted curve to experimental particle image for different sized particles. As a result, the algorithm yields the maximum error smaller than 2 nm for 5.53 {\\mu}m silica particles in contact condition.",
        "output": "0"
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Queueing networks are systems of theoretical interest that give rise to complex families of stochastic processes, and find widespread use in the performance evaluation of interconnected resources. Yet, despite their importance within applications, and in comparison to their counterpart stochastic models in genetics or mathematical biology, there exist few relevant approaches for transient inference and uncertainty quantification tasks in these systems. This is a consequence of strong computational impediments and distinctive properties of the Markov jump processes induced by queueing networks. In this paper, we offer a comprehensive overview of the inferential challenge and its comparison to analogue tasks within related mathematical domains. We then discuss a model augmentation over an approximating network system, and present a flexible and scalable variational Bayesian framework, which is targeted at general-form open and closed queueing systems, with varied service disciplines and priorities. The inferential procedure is finally validated in a couple of uncertainty quantification tasks for network service rates.",
        "output": "1"
    },    
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "We construct Hall algebra of elliptic curve over $\\mathbb{F}_1$ using the theory of monoidal scheme due to Deitmar and the theory of Hall algebra for monoidal representations due to Szczesny. The resulting algebra is shown to be a specialization of elliptic Hall algebra studied by Burban and Schiffmann. Thus our algebra is isomorphic to the skein algebra for torus by the recent work of Morton and Samuelson.",
        "output": "0"
    },
    {
        "instruction": "The following is an abstract from a research article. Classify this abstract; respond \"1\" if the article is a computer science article, and \"0\" if it is not, or of you are unsure.",
        "input": "Ambiguities in the definition of stored energy within distributed or radiating electromagnetic systems motivate the discussion of the well-defined concept of recoverable energy. This concept is commonly overlooked by the community and the purpose of this communication is to recall its existence and to discuss its relationship to fractional bandwidth. Using a rational function approximation of a system's input impedance, the recoverable energy of lumped and radiating systems is calculated in closed form and is related to stored energy and fractional bandwidth. Lumped circuits are also used to demonstrate the relationship between recoverable energy and the energy stored within equivalent circuits produced by the minimum phase-shift Darlington's synthesis procedure.",
        "output": "0"
    }
]
